Summary,Issue key,Component/s,Component/s,Component/s,Component/s,Description
HTTPFS proxy server needs pluggable-auth support,HDFS-7983,,,,,"Now that WebHDFS has been fixed to support pluggable auth, the httpfs proxy server also needs support."
libhdfs should use doxygen plugin to generate mvn site output,HDFS-9031,,,,,"Rather than point people to the hdfs.h file, we should take advantage of the doxyfile and actually generate for mvn site so it shows up on the website."
Documentation needs to be exposed,HDFS-9464,hdfs-client,,,,"From the few builds I've done, there doesn't appear to be any user-facing documentation that is actually exposed when mvn site is built.  HDFS-8745 allegedly added doxygen support, but even those docs aren't tied into the docs and/or site build. "
No header files in mvn package,HDFS-9465,hdfs-client,,,,The current build appears to only include the shared library and no header files to actually use the library in the final maven binary build.
Update NN/DN min software version to 3.0.0-beta1,HDFS-10398,,,,,"Before we release the first 3.0.0 beta, we need to update the min software version to exclude the alpha releases since we will not support alpha -> beta compatibility. beta->GA compatibility will work though."
"libwebhdfs lacks headers, documentation; not part of mvn package",HDFS-9030,,,,,This library is useless without header files to include and documentation on how to use it.  Both appear to be missing from the mvn package and site documentation.
Test : Hadoop-HdfsTrunk test cases after HDFS-4937,HDFS-9352,,,,,"{noformat}
Stack Trace:
org.apache.hadoop.ipc.RemoteException: File /TestAbandonBlock_test_quota1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1736)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:299)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2457)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:796)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:976)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2305)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2301)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1669)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2301)
{noformat}

 https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/558/"
Replica recovery doesn't distinguish between flushed-but-corrupted last chunk and unflushed last chunk,HDFS-1103,datanode,,,,"When the DN creates a replica under recovery, it calls validateIntegrity, which truncates the last checksum chunk off of a replica if it is found to be invalid. Then when the block recovery process happens, this shortened block wins over a longer replica from another node where there was no corruption. Thus, if just one of the DNs has an invalid last checksum chunk, data that has been sync()ed to other datanodes can be lost."
Secondary NameNode failed to rollback from 2.4.1 to 2.2.0,HDFS-7114,namenode,,,,"Can upgrade from 2.2.0 to 2.4.1, but failed to rollback the secondary namenode with following issue.

2014-09-22 10:41:28,358 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /var/hadoop/tmp/hdfs/dfs/namesecondary. Reported: -56. Expecting = -47.
        at org.apache.hadoop.hdfs.server.common.Storage.setLayoutVersion(Storage.java:1082)
        at org.apache.hadoop.hdfs.server.common.Storage.setFieldsFromProperties(Storage.java:890)
        at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:585)
        at org.apache.hadoop.hdfs.server.common.Storage.readProperties(Storage.java:921)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:913)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:249)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:199)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:652)
2014-09-22 10:41:28,360 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2014-09-22 10:41:28,363 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG:"
Failed to rollback hdfs version from 2.4.1 to 2.2.0,HDFS-7053,ha,namenode,,,"I can successfully upgrade from 2.2.0 to 2.4.1 with QJM HA enabled and with downtime, but failed to rollback from 2.4.1 to 2.2.0. The error message:
 2014-09-10 16:50:29,599 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
 org.apache.hadoop.HadoopIllegalArgumentException: Invalid startup option. Cannot perform DFS upgrade with HA enabled.
              at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1207)
               at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)
 2014-09-10 16:50:29,601 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
"
Failed to rolling upgrade hdfs from 2.2.0 to 2.4.1,HDFS-7002,journal-node,namenode,qjm,,
Unable to reset password,HDFS-6241,,,,,"I tried emailing root@apache.org - as indicated in INFRA-5241 - about these difficulties but it seems to be bouncing. Here is the email that I sent:

Greetings -

I am trying to reset my password and have encountered the following problems:

1. it seems that the public key associated with my account is erroneously that of the lead of our project (Knox). His must have set up my account in the beginning and provided his key maybe? Anyway, this means that he has to decrypt my email.

2. Once he does decrypt it and I follow the link to reset it - I get a No Such Token error message and am unable to reset my password.

The email below indicated that I should email root with problems.

Please let me know if I should file and Infra jira. I did find a similar one there that told them to email root. So, that is where I am starting.

We are in the process of trying to get a release out - so I would greatly appreciate the help here.

thanks!

--larry


Hi Larry McCay,

96.235.186.40 has asked Apache ID <https://id.apache.org>

to initiate a password reset for your apache.org account 'lmccay'.

If you requested this password reset, please use the following link to

reset your Apache LDAP password:

<deleted-url>

If you did not request this password reset, please email root@apache.org --- but

delete the above URL from the text of the reply email before sending it.

This link will expire at 2014-04-14 14:31:25 +0000, and can only be used from 96.235.186.40.

--

Best Regards,

Apache Infrastructure"
CLONE - Unable to reset password,HDFS-6242,,,,,"I tried emailing root@apache.org - as indicated in INFRA-5241 - about these difficulties but it seems to be bouncing. Here is the email that I sent:

Greetings -

I am trying to reset my password and have encountered the following problems:

1. it seems that the public key associated with my account is erroneously that of the lead of our project (Knox). His must have set up my account in the beginning and provided his key maybe? Anyway, this means that he has to decrypt my email.

2. Once he does decrypt it and I follow the link to reset it - I get a No Such Token error message and am unable to reset my password.

The email below indicated that I should email root with problems.

Please let me know if I should file and Infra jira. I did find a similar one there that told them to email root. So, that is where I am starting.

We are in the process of trying to get a release out - so I would greatly appreciate the help here.

thanks!

--larry


Hi Larry McCay,

96.235.186.40 has asked Apache ID <https://id.apache.org>

to initiate a password reset for your apache.org account 'lmccay'.

If you requested this password reset, please use the following link to

reset your Apache LDAP password:

<deleted-url>

If you did not request this password reset, please email root@apache.org --- but

delete the above URL from the text of the reply email before sending it.

This link will expire at 2014-04-14 14:31:25 +0000, and can only be used from 96.235.186.40.

--

Best Regards,

Apache Infrastructure"
namenode OOMs under Bigtop's TestCLI,HDFS-4940,,,,,"Bigtop's TestCLI when executed against Hadoop 2.1.0 seems to make it OOM quite reliably regardless of the heap size settings. I'm attaching a heap dump URL. Alliteratively anybody can just take Bigtop's tests, compiled them against Hadoop 2.1.0 bits and try to reproduce it.  "
Hftp should support both SPNEGO and KSSL,HDFS-3983,security,,,,"Hftp currently doesn't work against a secure cluster unless you configure {{dfs.https.port}} to be the http port, otherwise the client can't fetch tokens:

{noformat}
$ hadoop fs -ls hftp://c1225.hal.cloudera.com:50070/
12/09/26 18:02:00 INFO fs.FileSystem: Couldn't get a delegation token from http://c1225.hal.cloudera.com:50470 using http.
ls: Security enabled but user not authenticated by filter
{noformat}

This is due to Hftp still using the https port. Post HDFS-2617 it should use the regular http port. Hsftp should still use the secure port, however now that we have HADOOP-8581 it's worth considering removing Hsftp entirely. I'll start a separate thread about that.  

"
Enable retries for create and append operations.,HDFS-4849,namenode,,,,"create, append and delete operations can be made retriable. This will reduce chances for a job or other app failures when NN fails over."
want to integrate oracle nosql to hadoop 1.0.3,HDFS-4907,datanode,,,,"I had been asked by my customer to integrate oracle nosql with hadoop 1.0.3.
How to go about"
HA: Transition to active can cause NN deadlock,HDFS-2823,ha,namenode,,,"On transition to active, we have to take the FSNS write lock. In {{EditLogTailer#stop}}, we interrupt the edit log tailer thread and then join on that thread. When tailing edits, the edit log tailer thread acquires the FSNS write lock interruptibly, precisely so that we avoid deadlocks on transition to active. However, the edit log tailer thread now also triggers edit log rolls. Several places in {{ipc.Client}} catch and ignore {{InterruptedException}}, and in so doing may cause the {{Thread#interrupt}} call to be missed by the edit log tailer thread."
Datanode is going OOM due to small files in hdfs,HDFS-4630,datanode,namenode,,,"Hi, 

We have very small files(size ranging 10KB-1MB) in our hdfs and no of files are in tens of millions. Due to this namenode and datanode both going out of memory very frequently. When we analyse the head dump of datanode most of the memory was used by ReplicaMap. 

Can we use EhCache or other to not to store all the data in memory? 

Thanks
Ankush"
TestBlockTokenWithDFS fails on trunk,HDFS-1469,namenode,,,,"TestBlockTokenWithDFS is failing on trunk:

Testcase: testAppend took 31.569 sec
  FAILED
null
junit.framework.AssertionFailedError: null
  at org.apache.hadoop.hdfs.server.namenode.TestBlockTokenWithDFS.testAppend(TestBlockTokenWithDFS.java:223)"
Hadoop mapreduce job to process S3 logs gets hung at INFO mapred.JobClient:  map 0% reduce 0%.,HDFS-2583,,,,,"I am trying to run a mapreduce job to process the Amazon S3 logs. However, the code hangs at INFO mapred.JobClient:  map 0% reduce 0% and does not even attempt to launch the tasks. The sample code for the job setup is given below:
public int run(CommandLine cl) throws Exception 
{
       Configuration conf = getConf();
       String inputPath = """";
       String outputPath = """";
       try
       {
           Job job = new Job(conf, ""Dummy"");
           job.setNumReduceTasks(0);
           job.setMapperClass(Mapper.class);
           inputPath = cl.getOptionValue(""input""); //input is an s3n path
           outputPath = cl.getOptionValue(""output"");
           FileInputFormat.setInputPaths(job, inputPath);
           FileOutputFormat.setOutputPath(job, new Path(outputPath));
           _log.info(""Input path set as "" + inputPath);
           _log.info(""Output path set as "" + outputPath);
           job.waitForCompletion(true);
           return 0;
       } catch (Exception ex)
       {
           _log.error(ex);
           return 1;
       }
}
The above code works on the staging machine. However, it fails on the production machine which is same as the staging machine with more capacity.

Does anyone know what could be the possible reason for the error? 

Thanks in advance!

Nitika"
bin/hdfs conflicts with common user shortcut,HDFS-1253,,,,,The 'hdfs' command introduced in 0.21 (unreleased at this time) conflicts with a common user alias and wrapper script.  This change should either be reverted or moved from $HADOOP_HOME/bin to somewhere else in $HADOOP_HOME (perhaps sbin?) so that users do not accidentally hit it.
JMX values for RPC Activity is always zero,HDFS-2244,,,,,jconsole is showing that the RPC metrics gathered for the datanode is always zero.  Other metrics for the DN appears fine.
TestAuthorizationFilter is failing,HDFS-1666,contrib/hdfsproxy,,,,two test cases were failing for a number of builds (see attached logs)
HDFS build is broken: hadoop-core snapshot can't be resolved.,HDFS-749,,,,,"{noformat}
ivy-resolve-common:
[ivy:resolve] :: resolving dependencies :: org.apache.hadoop#Hadoop-Hdfs;0.22.0-SNAPSHOT
[ivy:resolve]   confs: [common]
[ivy:resolve]   found commons-logging#commons-logging;1.0.4 in maven2
[ivy:resolve]   found log4j#log4j;1.2.15 in maven2
[ivy:resolve]   found org.aspectj#aspectjrt;1.6.4 in maven2
[ivy:resolve]   found org.aspectj#aspectjtools;1.6.4 in maven2
[ivy:resolve] :: resolution report :: resolve 3088ms :: artifacts dl 30ms
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      common      |   5   |   0   |   0   |   0   ||   4   |   0   |
        ---------------------------------------------------------------------
[ivy:resolve] 
[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: WARNINGS
[ivy:resolve]           module not found: org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT
[ivy:resolve]   ==== apache-snapshot: tried
[ivy:resolve]     https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.pom
[ivy:resolve]     -- artifact org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT!hadoop-core.jar:
[ivy:resolve]     https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.jar
[ivy:resolve]   ==== maven2: tried
[ivy:resolve]     http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.pom
[ivy:resolve]     -- artifact org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT!hadoop-core.jar:
[ivy:resolve]     http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.jar
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           :: org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT: not found
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

BUILD FAILED
/homes/cos/work/Hdfs.trunk/build.xml:1337: impossible to resolve dependencies:
{noformat}"
TestFileCreation times out,HDFS-1376,,,,,"I see TestFileCreation periodically time out on a host when ant test runs but w/o anything thing else running. 

{noformat}
Testsuite: org.apache.hadoop.hdfs.TestFileCreation
Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec

Testcase: testFileCreationNonRecursive took 0.002 sec
        Caused an ERROR
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
{noformat}"
TestDiskError.testShutdown fails with port out of range: -1 error,HDFS-834,,,,,"The current build is broken on the TestDiskError.testShutdown unit test with the following error:
port out of range:-1

http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Hdfs-trunk/171/"
TestDiskError.testReplicationError fails with locked storage error ,HDFS-833,,,,,"The current build is failing with on TestDiskError.testReplication with the following error:
Cannot lock storage /grid/0/hudson/hudson-slave/workspace/Hadoop-Hdfs-trunk/trunk/build/test/data/dfs/name1. The directory is already locked.

 http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Hdfs-trunk/171/ "
file size is fluctuating although file is closed,HDFS-743,namenode,,,,"I am seeing that the length of a file sometimes becomes zero after a namenode restart. These files have only one block. All the three replicas of that block on the datanode(s) has non-zero size. Increasing the replication factor of the file causes the file to show its correct non-zero length.

I am marking this as a blocker because it is still to be investigated which releases it affects. I am seeing this on 0.17.x very frequently. I might have seen this on 0.20.x but do not have a reproducible case yet."
start-all.sh / stop-all.sh does not seem to work with HDFS,HDFS-1288,scripts,,,,"The start-all.sh / stop-all.sh script shipping with the ""combined"" hadoop-0.21.0-rc1 does not start/stop the DFS daemons unless $HADOOP_HDFS_HOME is explicitly set."
"Secondary Name Node crash, NPE in edit log replay",HDFS-1002,,,,,"An NPE in SNN, the core of the message looks like yay so:

2010-02-25 11:54:05,834 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1152)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1164)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addNode(FSDirectory.java:1067)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(FSDirectory.java:213)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadEditRecords(FSEditLog.java:511)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:401)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:368)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1172)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:594)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:476)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:353)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:317)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:219)
        at java.lang.Thread.run(Thread.java:619)

This happens even if I restart SNN over and over again."
TestFsck times out on branch 0.20.1,HDFS-784,,,,,"Two tests are currently failing on Hudson:

chttp://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-20-Build/lastBuild/testReport

>>> org.apache.hadoop.hdfs.TestDatanodeBlockScanner.testBlockCorruptionPolicy 	0.0020	9
>>> org.apache.hadoop.hdfs.server.namenode.TestFsck.testCorruptBlock 

HDFS-734 has already been filed for TestDatanodeBlockScanner.
"
rollingupgrade needs some guard rails,HDFS-7231,,,,,See first comment.
Hadoop compilation fails on gcc 7.3,HDFS-13737,,,,,"As per document we have specified  *GCC 4.8.1 or later*

Compilation fails on 7.3

{code}
root@bibinpc:~# /usr/bin/gcc-7 --version
gcc-7 (Ubuntu 7.3.0-16ubuntu3) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

{code}

{code}
   [exec] [ 28%] Building CXX object main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/ioservice_impl.cc.o
     [exec] In file included from /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h:22:0,
     [exec]                  from /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:19:
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:109:30: error: 鈥榮td::function鈥?has not been declared
     [exec]    virtual void PostTask(std::function<void(void)> asyncTask) = 0;
     [exec]                               ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:109:38: error: expected 鈥?鈥?or 鈥?..鈥?before 鈥?鈥?token
     [exec]    virtual void PostTask(std::function<void(void)> asyncTask) = 0;
     [exec]                                       ^
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h: In member function 鈥榲oid hdfs::IoService::PostLambda(LambdaInstance&&)鈥?
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:117:10: error: 鈥榝unction鈥?is not a member of 鈥榮td鈥?
     [exec]      std::function<void(void)> typeEraser = func;
     [exec]           ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:117:10: note: suggested alternative: 鈥榠s_function鈥?
     [exec]      std::function<void(void)> typeEraser = func;
     [exec]           ^~~~~~~~
     [exec]           is_function
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:117:19: error: expected primary-expression before 鈥榲oid鈥?
     [exec]      std::function<void(void)> typeEraser = func;
     [exec]                    ^~~~
     [exec] In file included from /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:19:0:
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h: At global scope:
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h:45:22: error: 鈥榮td::function鈥?has not been declared
     [exec]    void PostTask(std::function<void(void)> asyncTask) override;
     [exec]                       ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h:45:30: error: expected 鈥?鈥?or 鈥?..鈥?before 鈥?鈥?token
     [exec]    void PostTask(std::function<void(void)> asyncTask) override;
     [exec]                               ^
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:35: error: variable or field 鈥楶ostTask鈥?declared void
     [exec]  void IoServiceImpl::PostTask(std::function<void(void)> asyncTask) {
     [exec]                                    ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:35: error: 鈥榝unction鈥?is not a member of 鈥榮td鈥?
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:35: note: suggested alternative: 鈥榠s_function鈥?
     [exec]  void IoServiceImpl::PostTask(std::function<void(void)> asyncTask) {
     [exec]                                    ^~~~~~~~
     [exec]                                    is_function
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:44: error: expected primary-expression before 鈥榲oid鈥?
     [exec]  void IoServiceImpl::PostTask(std::function<void(void)> asyncTask) {
     [exec]                                             ^~~~
     [exec] main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/build.make:88: recipe for target 'main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/ioservice_impl.cc.o' failed
     [exec] CMakeFiles/Makefile2:1966: recipe for target 'main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/all' failed
     [exec] Makefile:140: recipe for target 'all' failed
     [exec] make[2]: *** [main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/ioservice_impl.cc.o] Error 1
     [exec] make[1]: *** [main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/all] Error 2
     [exec] make: *** [all] Error 2

{code}
"
HADOOP_HDFS_LOG_DIR should be HDFS_LOG_DIR in deprecations,HDFS-7913,,,,,"The wrong variable is deprecated in hdfs-config.sh.  It should be HDFS_LOG_DIR, not HADOOP_HDFS_LOG_DIR.  This is breaking backward compatibility.

It might be worthwhile to doublecheck the other dep's to make sure they are correct as well.

Also, release notes for the deprecation jira should be updated to reflect this change."
HdfsFileStatus#getPath returning null.,HDFS-12970,hdfs,,,,"After HDFS-12681, HdfsFileStatus#getPath() returns null.
I don't think this is expected.

Both the implementation of {{HdfsFileStatus}} sets the path to null.
{code:title=HdfsNamedFileStatus.java|borderStyle=solid}
  HdfsNamedFileStatus(long length, boolean isdir, int replication,
                      long blocksize, long mtime, long atime,
                      FsPermission permission, Set<Flags> flags,
                      String owner, String group,
                      byte[] symlink, byte[] path, long fileId,
                      int childrenNum, FileEncryptionInfo feInfo,
                      byte storagePolicy, ErasureCodingPolicy ecPolicy) {
    super(length, isdir, replication, blocksize, mtime, atime,
        HdfsFileStatus.convert(isdir, symlink != null, permission, flags),
        owner, group, null, null,           ------ The last null is for path.
        HdfsFileStatus.convert(flags));
{code}


{code:title=HdfsLocatedFileStatus.java|borderStyle=solid}
  HdfsLocatedFileStatus(long length, boolean isdir, int replication,
                        long blocksize, long mtime, long atime,
                        FsPermission permission, EnumSet<Flags> flags,
                        String owner, String group,
                        byte[] symlink, byte[] path, long fileId,
                        int childrenNum, FileEncryptionInfo feInfo,
                        byte storagePolicy, ErasureCodingPolicy ecPolicy,
                        LocatedBlocks hdfsloc) {
    super(length, isdir, replication, blocksize, mtime, atime,
        HdfsFileStatus.convert(isdir, symlink != null, permission, flags),
        owner, group, null, null, HdfsFileStatus.convert(flags),  -- The last null on this line is for path.
        null);
{code}
"
DFS.concat should throw exception if files have different EC policies. ,HDFS-12923,erasure-coding,,,,"{{DFS#concat}} appends blocks from different files to a single file. However, if these files have different EC policies, or mixed with replicated and EC files, the resulted file would be problematic to read, because the EC codec is defined in INode instead of in a block. 

"
NameNode fails to start after upgrade - Missing state in ECPolicy Proto ,HDFS-12918,hdfs,,,,"According to documentation and code comments, the default setting for erasure coding policy is disabled:

/** Policy is disabled. It's policy default state. */
 DISABLED(1),

However, HDFS-12258 appears to have incorrectly set the policy state in the protobuf to enabled:

{code:java}
 message ErasureCodingPolicyProto {
    ooptional string name = 1;
    optional ECSchemaProto schema = 2;
    optional uint32 cellSize = 3;
    required uint32 id = 4; // Actually a byte - only 8 bits used
 + optional ErasureCodingPolicyState state = 5 [default = ENABLED];
  }
{code}

This means the parameter can't actually be optional, it must always be included, and existing serialized data without this optional field will be incorrectly interpreted as having erasure coding enabled.

This unnecessarily breaks compatibility and will require existing HDFS installations that store metadata in protobufs to require reformatting.

It looks like a simple mistake that was overlooked in code review."
"I didn't find the ""-find"" command on hadoop2.6",HDFS-12715,auto-failover,,,,"When I looked for files on HDFS, I found no ""find"" command. I didn't find the ""find"" command by using ""Hadoop FS -help"".
"
WebHdfsFileSystem#getFileBlockLocations will always return BlockLocation#corrupt as false,HDFS-12442,webhdfs,,,,"Was going through {{JsonUtilClient#toBlockLocation}} code.
Below is the relevant code snippet.
{code:title=JsonUtilClient.java|borderStyle=solid}
 /** Convert a Json map to BlockLocation. **/
  static BlockLocation toBlockLocation(Map<?, ?> m)
      throws IOException{
    ...
    ...  
    boolean corrupt = Boolean.
        getBoolean(m.get(""corrupt"").toString());
    ...
    ...
  }
{code}
According to java docs for {{Boolean#getBoolean}}
{noformat}
Returns true if and only if the system property named by the argument exists and is equal to the string ""true"". 
{noformat}
I assume, the map value for key {{corrupt}} will be populated with either {{true}} or {{false}}.
On the client side, {{Boolean#getBoolean}} will look for system property for true or false.
So it will always return false unless the system property is set for true or false."
namenode crash in fsimage download/transfer,HDFS-9126,namenode,,,,"In our product Hadoop cluster,when active namenode begin download/transfer 
fsimage from standby namenode.some times zkfc monitor health of NameNode socket timeout,zkfs judge active namenode status SERVICE_NOT_RESPONDING ,happen hadoop namenode ha failover,fence old active namenode.

zkfc logs:
2015-09-24 11:44:44,739 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at hostname1/192.168.10.11:8020: Call From hostname1/192.168.10.11 to hostname1:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 45000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.10.11:22614 remote=hostname1/192.168.10.11:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at hostname1/192.168.10.11:8020 entered state: SERVICE_NOT_RESPONDING
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at hostname1/192.168.10.11:8020 and marking that fencing is necessary
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-09-24 11:44:44,761 INFO org.apache.zookeeper.ZooKeeper: Session: 0x54d81348fe503e3 closed
2015-09-24 11:44:44,761 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x54d81348fe503e3
2015-09-24 11:44:44,764 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down

namenode logs:
2015-09-24 11:43:34,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.10.12
2015-09-24 11:43:34,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-09-24 11:43:34,075 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2317430129
2015-09-24 11:43:34,253 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 272988 Total time for transactions(ms): 5502 Number of transactions batched in Syncs: 146274 Number of syncs: 32375 SyncTimes(ms): 274465 319599
2015-09-24 11:43:46,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-09-24 11:44:21,054 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: PendingReplicationMonitor timed out blk_1185804191_112164210
2015-09-24 11:44:36,076 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /software/data/hadoop-data/hdfs/namenode/current/edits_inprogress_0000000002317430129 -> /software/data/hadoop-data/hdfs/namenode/current/edits_0000000002317430129-0000000002317703116
2015-09-24 11:44:36,077 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2317703117
2015-09-24 11:45:38,008 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 61585
2015-09-24 11:45:38,009 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 222.88s at 63510.29 KB/s
2015-09-24 11:45:38,009 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000002317430128 size 14495092105 bytes.
2015-09-24 11:45:38,416 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Remote journal 192.168.10.13:8485 failed to write txns 2317703117-2317703117. Will try to write to this JN again after the next log roll.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): IPC's epoch 44 is less than the last promised epoch 45
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:414)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkWriteRequest(Journal.java:442)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.journal(Journal.java:342)
        at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.journal(JournalNodeRpcServer.java:148)
        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.journal(QJournalProtocolServerSideTranslatorPB.java:158)
        at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25421)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
        at org.apache.hadoop.ipc.Client.call(Client.java:1468)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy9.journal(Unknown Source)
        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB.journal(QJournalProtocolTranslatorPB.java:167)
        at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:385)
        at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:378)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
		
#Similar log like above 

2015-09-24 11:45:38,418 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Aborting QuorumOutputStream starting at txid 2317703117
2015-09-24 11:45:38,505 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-09-24 11:45:38,549 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hostname1/192.168.10.11
"
Ozone: Datanode is unable to register with scm if scm starts later,HDFS-12098,datanode,ozone,scm,,"Reproducing steps
1. Start namenode

{{./bin/hdfs --daemon start namenode}}

2. Start datanode

{{./bin/hdfs datanode}}

will see following connection issues

{noformat}
17/07/13 21:16:48 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
17/07/13 21:16:49 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
17/07/13 21:16:50 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
17/07/13 21:16:51 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
{noformat}

this is expected because scm is not started yet

3. Start scm

{{./bin/hdfs scm}}

expecting datanode can register to this scm, expecting the log in scm

{noformat}
17/07/13 21:22:30 INFO node.SCMNodeManager: Data node with ID: af22862d-aafa-4941-9073-53224ae43e2c Registered.
{noformat}

but did *NOT* see this log. (_I debugged into the code and found the datanode state was transited SHUTDOWN unexpectedly because the thread leaks, each of those threads counted to set to next state and they all set to SHUTDOWN state_)

4. Create a container from scm CLI

{{./bin/hdfs scm -container -create -c 20170714c0}}

this fails with following exception

{noformat}
Creating container : 20170714c0.
Error executing command:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.scm.exceptions.SCMException): Unable to create container while in chill mode
	at org.apache.hadoop.ozone.scm.container.ContainerMapping.allocateContainer(ContainerMapping.java:241)
	at org.apache.hadoop.ozone.scm.StorageContainerManager.allocateContainer(StorageContainerManager.java:392)
	at org.apache.hadoop.ozone.protocolPB.StorageContainerLocationProtocolServerSideTranslatorPB.allocateContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:73)
{noformat}

datanode was not registered to scm, thus it's still in chill mode.

*Note*, if we start scm first, there is no such issue, I can create container from CLI without any problem.

"
chooseRemoteRack() semantics broken in trunk,HDFS-12272,block placement,,,,"The {{chooseRemoteRack()}} method in the default block placement policy was designed to pick from maximum number of racks. E.g. If asked to pick 2 and there are 2 or more racks available, the two will be on different racks. It wasn't implicit or accidental semantics. There was a specific logic in {{chooseRandom()}} that makes it happen.

This behavior is broken after HDFS-11530 as this logic was removed from {{chooseRandom()}}. Now the result is unpredictable. Sometimes the replicas end up in the same rack."
Backport HDFS-7964 to branch-2.7: add support for async edit logging,HDFS-11737,ha,namenode,,,
DatanodeHttpServer is not setting Endpoint based on configured policy and not loading ssl configuration.,HDFS-9045,,,,,"Always DN is starting in http mode.
{code}
    HttpServer2.Builder builder = new HttpServer2.Builder()
        .setName(""datanode"")
        .setConf(confForInfoServer)
        .setACL(new AccessControlList(conf.get(DFS_ADMIN, "" "")))
        .hostName(getHostnameForSpnegoPrincipal(confForInfoServer))
        .addEndpoint(URI.create(""http://localhost:0""))
        .setFindPort(true);

{code}
Should be based on configured policy"
"For HA, a logical name is visible in URIs - add an explicit logical name",HDFS-3153,,,,,Please see this [comment|https://issues.apache.org/jira/browse/HDFS-2839?focusedCommentId=13227729&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13227729] for a discussion of logical names.
Backport HDFS-3553 (hftp proxy tokens) to branch-1,HDFS-4368,,,,,"Proxy tokens are broken for hftp.  The impact is systems using proxy tokens, such as oozie jobs, cannot use hftp."
data node sudden killed ,HDFS-10340,datanode,,,,"I tried to setup a new data node using ubuntu 16 
and get it join to an existed Hadoop Hdfs cluster ( there are 10 nodes in this cluster and they all run on centos Os 6 ) 
But when i try to boostrap this node , after about 10 or 20 minutes i get this strange errors : 

2016-04-26 20:12:09,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.3.24.65:55323, dest: /10.3.24.197:50010, bytes: 79902, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1379996362_1, offset: 0, srvID: 225f5b43-1dd3-4ac6-88d2-1e8d27dba55b, blockid: BP-352432948-10.3.24.65-1433821675295:blk_1074038505_789832, duration: 15331628
2016-04-26 20:12:09,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-352432948-10.3.24.65-1433821675295:blk_1074038505_789832, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2016-04-26 20:12:25,410 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-352432948-10.3.24.65-1433821675295:blk_1074038502_789829
2016-04-26 20:12:25,411 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-352432948-10.3.24.65-1433821675295:blk_1074038505_789832
2016-04-26 20:13:18,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1074038502_789829 file /data/hadoop_data/backup/data/current/BP-352432948-10.3.24.65-1433821675295/current/finalized/subdir4/subdir134/blk_1074038502 for deletion
2016-04-26 20:13:18,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-352432948-10.3.24.65-1433821675295 blk_1074038502_789829 file /data/hadoop_data/backup/data/current/BP-352432948-10.3.24.65-1433821675295/current/finalized/subdir4/subdir134/blk_1074038502
2016-04-26 20:15:46,481 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2016-04-26 20:15:46,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at bigdata-dw-24-197/10.3.24.197
************************************************************/
"
Block reports could be silently dropped by NN,HDFS-10563,namenode,,,,"Reading through the block reporting code I think I've spotted a case when block reports can silently be dropped and leave thread waiting indefinitely on a FutureTask that never will be executed.

The BlockReportProcessingThread.enqueue method doesn't return any status on if the enqueuing of the task was successful and does not handle the case when the queue is full and offer return false.

Going back through the call stack to BlockManager.runBlockOp, which indirectly calls enqueue with a FutureTask and then proceeds to do get() om the task.

So if the internal queue in the BlockReportingProcessingThread is full, the BR would never be handled and the thread queuing the task would wait indefinitely on the FutureTask that will never be executed."
Secret manager saves expired items,HDFS-4539,security,,,,Expired tokens and secrets should not be written out when the secret manager is serialized into the fs image.
DFSClient should ignore dfs.client.retry.policy.enabled for HA proxies,HDFS-8708,,,,,"DFSClient should ignore dfs.client.retry.policy.enabled for HA proxies to ensure fast failover. Otherwise, dfsclient retries the NN which is no longer active and delays the failover."
Replicas awaiting recovery should return a full visible length,HDFS-2288,datanode,,,,"Currently, if the client calls getReplicaVisibleLength for a RWR, it returns a visible length of 0. This causes one of HBase's tests to fail, and I believe it's incorrect behavior."
"using NFS As a shared storage for NameNode HA , how to ensure that only one write",HDFS-3847,ha,,,,
TestFileAppend4 fails intermittently,HDFS-2433,datanode,namenode,test,,"A Jenkins build we have running failed twice in a row with issues form TestFileAppend4.testAppendSyncReplication1 in an attempt to reproduce the error I ran TestFileAppend4 in a loop over night saving the results away.  (No clean was done in between test runs)

When TestFileAppend4 is run in a loop the testAppendSyncReplication[012] tests fail about 10% of the time (14 times out of 130 tries)  They all fail with something like the following.  Often it is only one of the tests that fail, but I have seen as many as two fail in one run.

{noformat}
Testcase: testAppendSyncReplication2 took 32.198 sec
        FAILED
Should have 2 replicas for that block, not 1
junit.framework.AssertionFailedError: Should have 2 replicas for that block, not 1
        at org.apache.hadoop.hdfs.TestFileAppend4.replicationTest(TestFileAppend4.java:477)
        at org.apache.hadoop.hdfs.TestFileAppend4.testAppendSyncReplication2(TestFileAppend4.java:425)
{noformat}

I also saw several other tests that are a part of TestFileApped4 fail during this experiment.  They may all be related to one another so I am filing them in the same JIRA.  If it turns out that they are not related then they can be split up later.

testAppendSyncBlockPlusBbw failed 6 out of the 130 times or about 5% of the time

{noformat}
Testcase: testAppendSyncBlockPlusBbw took 1.633 sec
        FAILED
unexpected file size! received=0 , expected=1024
junit.framework.AssertionFailedError: unexpected file size! received=0 , expected=1024
        at org.apache.hadoop.hdfs.TestFileAppend4.assertFileSize(TestFileAppend4.java:136)
        at org.apache.hadoop.hdfs.TestFileAppend4.testAppendSyncBlockPlusBbw(TestFileAppend4.java:401)
{noformat}

testAppendSyncChecksum[012] failed 2 out of the 130 times or about 1.5% of the time

{noformat}
Testcase: testAppendSyncChecksum1 took 32.385 sec
        FAILED
Should have 1 replica for that block, not 2
junit.framework.AssertionFailedError: Should have 1 replica for that block, not 2
        at org.apache.hadoop.hdfs.TestFileAppend4.checksumTest(TestFileAppend4.java:556)
        at org.apache.hadoop.hdfs.TestFileAppend4.testAppendSyncChecksum1(TestFileAppend4.java:500)
{noformat}

I will attach logs for all of the failures.  Be aware that I did change some of the logging messages in this test so I could better see when testAppendSyncReplication started and ended.  Other then that the code is stock 0.20.205 RC2"
Impala compilation breaks with libhdfs in 2.7 as getJNIEnv is not visible,HDFS-8474,build,libhdfs,,,"Impala in CDH 5.2.0 is not compiling with libhdfs.so in 2.7.0 on RedHat 6.4.
This is because getJNIEnv is not visible in the so file.

Compilation fails with below error message :
../../build/release/exec/libExec.a(hbase-table-scanner.cc.o): In function `impala::HBaseTableScanner::Init()':
/usr1/code/Impala/code/current/impala/be/src/exec/hbase-table-scanner.cc:113: undefined reference to `getJNIEnv'
../../build/release/exprs/libExprs.a(hive-udf-call.cc.o):/usr1/code/Impala/code/current/impala/be/src/exprs/hive-udf-call.cc:227: more undefined references to `getJNIEnv' follow
collect2: ld returned 1 exit status
make[3]: *** [be/build/release/service/impalad] Error 1
make[2]: *** [be/src/service/CMakeFiles/impalad.dir/all] Error 2
make[1]: *** [be/src/service/CMakeFiles/impalad.dir/rule] Error 2
make: *** [impalad] Error 2
Compiler Impala Failed, exit


libhdfs.so.0.0.0 returns nothing when following command is run.
""nm -D libhdfs.so.0.0.0  | grep getJNIEnv""

The change in HDFS-7879 breaks the backward compatibility of libhdfs although it can be argued that Impala shouldn't be using above API."
Datanode does not log reads,HDFS-8315,,,,,"HDFS-6836 made datanode read request logging DEBUG.  There is a good reason why it was at INFO for so many years.  This is very useful in debugging load issues. This jira will revert HDFS-6836.

We haven't seen it being a bottleneck on busy hbase clusters, but if someone thinks it is a serious overhead, please make it configurable in a separate jira."
Failed pipeline creation during append leaves lease hanging on NN,HDFS-1262,hdfs-client,namenode,,,"Ryan Rawson came upon this nasty bug in HBase cluster testing. What happened was the following:
1) File's original writer died
2) Recovery client tried to open file for append - looped for a minute or so until soft lease expired, then append call initiated recovery
3) Recovery completed successfully
4) Recovery client calls append again, which succeeds on the NN
5) For some reason, the block recovery that happens at the start of append pipeline creation failed on all datanodes 6 times, causing the append() call to throw an exception back to HBase master. HBase assumed the file wasn't open and put it back on a queue to try later
6) Some time later, it tried append again, but the lease was still assigned to the same DFS client, so it wasn't able to recover.

The recovery failure in step 5 is a separate issue, but the problem for this JIRA is that the NN can think it failed to open a file for append when the NN thinks the writer holds a lease. Since the writer keeps renewing its lease, recovery never happens, and no one can open or recover the file until the DFS client shuts down."
loss of VERSION file on datanode when trying to startup with full disk,HDFS-60,datanode,,,,"datanode working ok previously. subsequent bringup of datanode fails:

/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoop003.sf2p.facebook.com/10.16.159.103
STARTUP_MSG:   args = []
************************************************************/
2008-01-08 08:23:38,400 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processN
ame=DataNode, sessionId=null
2008-01-08 08:23:48,491 INFO org.apache.hadoop.ipc.RPC: Problem connecting to server: hadoop001.sf2p.facebook
.com/10.16.159.101:9000
2008-01-08 08:23:59,495 INFO org.apache.hadoop.ipc.RPC: Problem connecting to server: hadoop001.sf2p.facebook
.com/10.16.159.101:9000
2008-01-08 08:24:01,597 ERROR org.apache.hadoop.dfs.DataNode: java.io.IOException: No space left on device
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(StreamEncoder.java:336)
        at sun.nio.cs.StreamEncoder$CharsetSE.implFlushBuffer(StreamEncoder.java:404)
        at sun.nio.cs.StreamEncoder$CharsetSE.implFlush(StreamEncoder.java:408)
        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:152)
        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:213)
        at java.io.BufferedWriter.flush(BufferedWriter.java:236)
        at java.util.Properties.store(Properties.java:666)
        at org.apache.hadoop.dfs.Storage$StorageDirectory.write(Storage.java:176)
        at org.apache.hadoop.dfs.Storage$StorageDirectory.write(Storage.java:164)
        at org.apache.hadoop.dfs.Storage.writeAll(Storage.java:510)
        at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:146)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:243)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:206)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1391)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1335)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:1356)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1525)

2008-01-08 08:24:01,597 INFO org.apache.hadoop.dfs.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop003.sf2p.facebook.com/10.16.159.103
"
WebHdfsFilesystem does not work within a proxyuser doAs call in secure mode,HDFS-3509,,,,,"It does not find kerberos credentials in the context (the UGI is logged in from a keytab) and it fails with the following trace:

{code}
java.lang.IllegalStateException: unknown char '<'(60) in org.mortbay.util.ajax.JSON$ReaderSource@23245e75
	at org.mortbay.util.ajax.JSON.handleUnknown(JSON.java:788)
	at org.mortbay.util.ajax.JSON.parse(JSON.java:777)
	at org.mortbay.util.ajax.JSON.parse(JSON.java:603)
	at org.mortbay.util.ajax.JSON.parse(JSON.java:183)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:259)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:268)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:427)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:722)
{code}"
DNS Issues during TrashEmptier initialization can silently leave it non-functional,HDFS-5850,,,,,"[~knoguchi] recently noticed that the trash directories of a restarted cluster were not cleaned up. It turned out that it was caused by a transient DNS problem during initialization.

TrashEmptier thread in namenode is actually a FileSystem client running in a loop, which makes RPC calls to itself in order  to list, rename and delete trash files.  In a secure setup, the client needs to create the right service principal name for the namenode for making a RPC connection. If there is a DNS issue at that moment, the SPN ends up with the IP address, not the fqdn.

Since KDC does not recognize this SPN, TrashEmptier does not work from that point on. I verified that the SPN with the IP address was what the TrashEmptier thread asked KDC for a service ticket for."
S-live: Rate operation count for delete is worse than 0.20.204 by 28.8%,HDFS-2984,benchmarks,,,,Rate operation count for delete is worse than 0.20.204.xx by 28.8%
"Namenode can't restart due to corrupt edit logs, timing issue with shutdown and edit log rolling",HDFS-3771,namenode,,,,"Our 0.23.3 nightly HDFS regression suite encountered a particularly nasty issue recently, which resulted in the cluster's default Namenode being unable to restart, this was on a 20 node Federated cluster with security. The cause appears to be that the NN was just starting to roll its edit log when a shutdown occurred, the shutdown was intentional to restart the cluster as part of an automated test.

The tests that were running do not appear to be the issue in themselves, the cluster was just wrapping up an adminReport subset and this failure case has not reproduce so far, nor was it failing previously. It looks like a chance occurrence of sending the shutdown just as the edit log roll was begun.

From the NN log, the following sequence is noted:

1. an InvalidateBlocks operation had completed
2. FSNamesystem: Roll Edit Log from [Secondary Namenode IPaddr]
3. FSEditLog: Ending log segment 23963
4. FSEditLog: Starting log segment at 23967
4. NameNode: SHUTDOWN_MSG
=> the NN shuts down and then is restarted...
5. FSImageTransactionalStorageInspector: Logs beginning at txid 23967 were are all in-progress
6. FSImageTransactionalStorageInspector: Marking log at /grid/[PATH]/edits_inprogress_0000000000000023967 as corrupt since it has no transactions in it.
7. NameNode: Exception in namenode join [main]java.lang.IllegalStateException: No non-corrupt logs for txid 23967
=> NN start attempts continue to cycle trying to restart but can't, failing on the same exception due to lack of non-corrupt edit logs

If observations are correct and issue is from shutdown happening as edit logs are rolling, does the NN have an equivalent to the conventional fs 'sync' blocking action that should be called, or perhaps has a timing hole?"
WebHDFS obtains/sets delegation token service hostname using wrong config leading to issues when NN is configured with 0.0.0.0 RPC IP,HDFS-4457,webhdfs,,,,"If the NameNode RPC address is configured with an wildcard IP 0.0.0.0, then delegationotkens are configured with 0.0.0.0 as service and this breaks clients trying to use those tokens.

Looking at NamenodeWebHdfsMethods#generateDelegationToken() the problem is SecurityUtil.setTokenService(t, namenode.getHttpAddress());, tracing back what is being used to resolve getHttpAddress() the NameNodeHttpServer is resolving the httpAddress doing a httpAddress = new InetSocketAddress(bindAddress.getAddress(), httpServer.getPort());
, and if using ""0.0.0.0"" in the configuration, you get 0.0.0.0 from bindAddress.getAddress().

Normally (non webhdfs) this is not an issue because it is the responsibility of the client, but in the case of WebHDFS, WebHDFS does it before returning the string version of the token (it must be this way because the client may not be a java client at all and cannot manipulate the DelegationToken as such).

The solution (thanks to Eric Sammer for helping figure this out) is for WebHDFS to use the exacty hostname that came in the HTTP request as the service to set in the delegation tokens."
NameNode low on available disk space,HDFS-4425,namenode,,,,"Hi,

Namenode switches into safemode when it has low disk space on the root fs / i have to manually run a command to leave it. Below are log messages for low space on root / fs. Is there any parameter so that i can reduce reserved amount.


2013-01-21 01:22:52,217 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume '/dev/mapper/vg_lv_root' is 10653696, which is below the configured reserved amount 104857600
2013-01-21 01:22:52,218 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: NameNode low on available disk space. Entering safe mode.
2013-01-21 01:22:52,218 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is ON.


"
hdfs script does not work out of the box.,HDFS-2674,,,,,"As the title says, hadoop-config.sh doesn't add the hadoop-common jars, which makes the hdfs script fail.

To repro, follow the instructions from http://wiki.apache.org/hadoop/HowToSetupYourDevelopmentEnvironment
{code}
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:24 [0 jobs] [hist 1889] 
$ export HADOOP_COMMON_HOME=$(pwd)/$(ls -d hadoop-common-project/hadoop-common/target/hadoop-common-*-SNAPSHOT)
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:29 [0 jobs] [hist 1890] 
$ export HADOOP_HDFS_HOME=$(pwd)/$(ls -d hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-*-SNAPSHOT)
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:36 [0 jobs] [hist 1891] 
$ export PATH=$HADOOP_COMMON_HOME/bin:$HADOOP_HDFS_HOME/bin:$PATH
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:42 [0 jobs] [hist 1892] 
$ cat > $HADOOP_COMMON_HOME/etc/hadoop/core-site.xml  << EOF
> <?xml version=""1.0""?><!-- core-site.xml -->
> <configuration>
>   <property>
>     <name>fs.default.name</name>
>     <value>hdfs://localhost/</value>
>   </property>
> </configuration>
> EOF
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:51 [0 jobs] [hist 1893] 
$ hdfs namenode -format
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/HadoopIllegalArgumentException
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.HadoopIllegalArgumentException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)

{code}"
dfs.du.reserved not honored in 0.15/16 (regression from 0.14+patch for 2549),HDFS-74,,,,,"changes for https://issues.apache.org/jira/browse/HADOOP-1463

have caused a regression. earlier:

- we could set dfs.du.reserve to 1G and be *sure* that 1G would not be used.

now this is no longer true. I am quoting Pete Wyckoff's example:

<example>
Let's look at an example. 100 GB disk and /usr using 45 GB and dfs using 50 GBs now

Df -kh shows:

Capacity = 100 GB
Available = 1 GB (remember ~4 GB chopped out for metadata and stuff)
Used = 95 GBs   

remaining = 100 GB - 50 GB - 1GB = 49 GB 

Min(remaining, available) = 1 GB

98% of which is usable for DFS apparently - 

So, we're at the limit, but are free to use 98% of the remaining 1GB.
</example>

this is broke. based on the discussion on 1463 - it seems like the notion of 'capacity' as being the first field of 'df' is problematic. For example - here's what our df output looks like:

Filesystem            Size  Used Avail Use% Mounted on
/dev/sda3             130G  123G   49M 100% /


as u can see - 'Size' is a misnomer - that much space is not available. Rather the actual usable space is 123G+49M ~ 123G. (not entirely sure what the discrepancy is due to - but have heard this may be due to space reserved for file system metadata). Because of this discrepancy - we end up in a situation where file system is out of space.
"
Datanode stops cleaning disk space,HDFS-63,,,,,"Here is the situation - DFS cluster running Hadoop version 0.19.0. The cluster is running on multiple servers with practically identical hardware. 
Everything works perfectly well, except for one thing - from time to time one of the data nodes (every time it's a different node) starts to consume more and more disk space. The node keeps going and if we don't do anything - it runs out of space completely (ignoring 20GB reserved space settings). 
Once restarted - it cleans disk rapidly and goes back to approximately the same utilization as the rest of data nodes in the cluster.
"
MapReduce Streaming job hang when all replications of the input file has corrupted!,HDFS-183,,,,,"On some special cases, all replications of a given file has truncated to zero  but the namenode still hold the original size (we don't know why),  the mapreduce streaming job will hang if we don't specified mapred.task.timeout when the input files contain this corrupted file, even the dfs shell ""cat"" will hang when fetch data from this corrupted file.

We found that job hang at DFSInputStream.blockSeekTo() when chosing a datanode.  The following test will show:
1)	Copy a small file to hdfs. 
2)	Get the file blocks and login to these datanodes, and truncate these blocks to zero.
3)	Cat this file through dfs shell ""cat""
4)	Cat command will enter dead loop.
"
java.net.SocketTimeoutException: timed out waiting for rpc response ,HDFS-191,,,,,"Following exception happens when users run Pig queries over large data. Brought up this with Hadoop team and this is follow-up JIRA. 

java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:484)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at $Proxy1.getJobStatus(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy1.getJobStatus(Unknown Source)
        at org.apache.hadoop.mapred.JobClient$NetworkedJob.ensureFreshStatus(JobClient.java:182)
        at org.apache.hadoop.mapred.JobClient$NetworkedJob.isComplete(JobClient.java:237)
        at org.apache.pig.impl.mapreduceExec.MapReduceLauncher.launchPig(MapReduceLauncher.java:189)
        at org.apache.pig.impl.physicalLayer.POMapreduce.open(POMapreduce.java:136)
        at org.apache.pig.impl.physicalLayer.PhysicalPlan.exec(PhysicalPlan.java:39)
        at org.apache.pig.impl.physicalLayer.IntermedResult.exec(IntermedResult.java:122)
        at org.apache.pig.PigServer.store(PigServer.java:445)
        at org.apache.pig.PigServer.store(PigServer.java:413)
        at org.apache.pig.tools.grunt.GruntParser.processStore(GruntParser.java:135)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:327)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:64)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:48)
        at org.apache.pig.Main.main(Main.java:246)
timed out waiting for rpc response

Is this a bug or we have  a time-out setting directly related to this in Hadoop?"
separate space reservation for hdfs blocks and intermediate storage,HDFS-329,,,,,"both dfs client buffering (and i imagine map-reduce intermediate data) and datanode try to honor the same space reservation (dfs.du.reserved). But this is problematic because once hdfs/data-node fill up a node - there's no space left for map-reduce computations.

ideally - hdfs should be allowed to consume upto some watermark (say 60%) and then dfs buffering/intermediate storage should be allowed to consume space upto some higher watermark (say 90%). this way the node will always remain usable.

we are hitting this problem in a cluster where a few nodes have lower amount of space. while the cluster overall has space left, these nodes are hitting their space limits. but now tasks scheduled on these nodes fail because dfs client does not find space to buffer to. there's no workaround really i can think of.

another option would be to globally allocate hdfs blocks based on space availability (keep all nodes at the same space utilization % approx.)."
"DataNode fails to deliver blocks, holds thousands of open socket connections",HDFS-162,,,,,"9/27 update: uploaded the logs, with hopefully all the bits that should be examined. If other things are needed, just let me know. Note that all the paths refer to 0.18.1. This is still an 18.0 installation using the 18.0 core jar, just installed to a non-standard location.

9/26 update: we have successfully reproduced this using Hadoop 0.18 as well. The problem happens on both our own network infrastructure as well as on an Amazon EC2 cluster running CentOS5 images. I'll be attaching the logs Raghu asked for shortly.

A job that used to run correctly on our grid (in 0.15.0) now fails. The failure occurs after the map phase is complete, and about 2/3rds of the way through the reduce phase.   This job is processing a modest amount of input data (approximately 220G)

When the error occurs the nodes hosting DataNodes have literally thousands of open socket connections on them.  The DataNode instances are holding large amounts of memory.  Sometimes the DataNodes crash or exit, other times they continue to run.

The error which gets kicked out from the application perspective is:

08/05/27 11:30:08 INFO mapred.JobClient: map 100% reduce 89%
08/05/27 11:30:41 INFO mapred.JobClient: map 100% reduce 90%
08/05/27 11:32:45 INFO mapred.JobClient: map 100% reduce 86%
08/05/27 11:32:45 INFO mapred.JobClient: Task Id :
 task_200805271056_0001_r_000007_0, Status : FAILED
java.io.IOException: Could not get block locations. Aborting...
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanode
 Error(DFSClient.java:1832)
at
 org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1487)
at
 org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1579)

I then discovered that 1 or more DataNode instances on the slave nodes
 are down (we run 1 DataNode instance per machine). The cause for at
 least some of the DataNode failures is a JVM internal error that gets
 raised due to a complete out-of-memory scenario (on a 4G, 4-way machine). 

Watching the DataNodes run, I can see them consuming more and more
 memory. For those failures for which there is a JVM traceback, I see (in
 part...NOTE 0.16.4 TRACEBACK):
#
# java.lang.OutOfMemoryError: requested 16 bytes for CHeapObj-new. Out
 of swap space?
#
# Internal Error (414C4C4F434154494F4E0E494E4C494E450E4850500017),
 pid=4246, tid=2283883408
#
# Java VM: Java HotSpot(TM) Server VM (1.6.0_02-b05 mixed mode)
# If you would like to submit a bug report, please visit:
# http://java.sun.com/webapps/bugreport/crash.jsp
#
--------------- T H R E A D ---------------
Current thread (0x8a942000): JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@3f4f44"" daemon [_thread_in_Java, id=15064]
Stack: [0x881c4000,0x88215000), sp=0x882139e0, free space=318k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code,
 C=native code)
V [libjvm.so+0x53b707]
V [libjvm.so+0x225fe1]
V [libjvm.so+0x16fdc5]
V [libjvm.so+0x22aef3]
Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
v blob 0xf4f235a7
J java.io.DataInputStream.readInt()I
j
 org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(Ljava/io/DataOutputStream;Ljava/io/DataInputStream;Ljava/io/DataOutputStream;Ljava/lang/String;Lorg/a
pache/hadoop/dfs/DataNode$Throttler;I)V+126
j
 org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(Ljava/io/DataInputStream;)V+746
j org.apache.hadoop.dfs.DataNode$DataXceiver.run()V+174
j java.lang.Thread.run()V+11
v ~StubRoutines::call_stub
--------------- P R O C E S S ---------------
Java Threads: ( => current thread )
0x0ae3f400 JavaThread ""process reaper"" daemon [_thread_blocked,
 id=26870]
0x852e6000 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@e5dce1"" daemon [_thread_in_vm, id=26869]
0x08a1cc00 JavaThread ""PacketResponder 0 for Block
 blk_-6186975972786687394"" daemon [_thread_blocked, id=26769]
0x852e5000 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@c40bf8"" daemon [_thread_in_native, id=26768]
0x0956e000 JavaThread ""PacketResponder 0 for Block
 blk_-2322514873363546651"" daemon [_thread_blocked, id=26767]
0x852e4400 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@1ca61f9"" daemon [_thread_in_native, id=26766]
0x09d3a400 JavaThread ""PacketResponder 0 for Block
 blk_8926941945313450801"" daemon [_thread_blocked, id=26764]
0x852e3c00 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@1e186d9"" daemon [_thread_in_native, id=26763]
0x0953d000 JavaThread ""PacketResponder 0 for Block
 blk_4785883052769066976"" daemon [_thread_blocked, id=26762]
0xb13a5c00 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@13d62aa"" daemon [_thread_in_native, id=26761]

The interesting part here is that if I count the number of JavaThreads
 running org.apache.hadoop.dfs.DataNode I see 4,538 (!) in the
 traceback. The number of threads was surprising.

Other DataNodes just exit without panicking the JVM. In either failure
 mode, the last few lines of the DataNode log file is apparently
 innocuous:

2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2
 got response for connect ack from downstream datanode with
 firstbadlink as
2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2
 forwarding connect ack to upstream firstbadlink is
2008-05-27 11:31:48,268 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_-2241766430103062484 src: /10.2.14.10:33626 dest:
 /10.2.14.10:50010
2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_313239508245918539 src: /10.2.14.24:37836 dest:
 /10.2.14.24:50010
2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
 forwarding connect ack to upstream firstbadlink is
2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_1684581399908730353 src: /10.2.14.16:51605 dest:
 /10.2.14.16:50010
2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
 forwarding connect ack to upstream firstbadlink is
2008-05-27 11:31:49,509 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_2493969670086107736 src: /10.2.14.18:47557 dest:
 /10.2.14.18:50010
2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1
 got response for connect ack from downstream datanode with
 firstbadlink as
2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1
 forwarding connect ack to upstream firstbadlink is

Finally, the task-level output (in userlogs) doesn't reveal much
 either:

2008-05-27 11:38:30,724 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Need 34 map output(s)
2008-05-27 11:38:30,753 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 done copying
 task_200805271056_0001_m_001976_0 output from worker9.
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1: Got 0 new map-outputs & 0 obsolete
 map-outputs from tasktracker and 0 map-outputs from previous failures
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Got 33 known map output location(s);
 scheduling...
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Scheduled 1 of 33 known outputs (0 slow
 hosts and 32 dup hosts)
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Copying task_200805271056_0001_m_001248_0
 output from worker8.
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Need 33 map output(s)
2008-05-27 11:38:31,752 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 done copying
 task_200805271056_0001_m_001248_0 output from worker8.
"
regarding HDFS-RAID,HDFS-2600,contrib/raid,,,,"I've tried to do HDFS-RAID over hadoop-0.21.0 ...
 
But, It's failed to raid when I've tried to start raidnode (start-raidnode.sh)
 
What do I have to correct it?
 
This is the information. Would you tell me how to correct it?
 
Namenode : xxx.23.129.40:9006
Datanode: xxx.23.129.39, 38
Raidnode : xxx.23.129.40 ( I want to seperate raidnoe from namenode, how can I do?)
 
* hdfs-site.xml
  <name>hdfs.raid.locations</name>
    <value>hdfs://xxx.23.129.40:9006/raid</value>   
=>  (What's the meaning of this, which address do I have to write down?)
    <description>The location for parity files. If this is
      is not defined, then defaults to /raid.
    </description>
  </property>
 
* raid.xml
<configuration>
    <srcPath prefix=""hdfs://xxx.23.129.40:9006/TEST"">  
=>  (What's the meaning of this, which address do I have to write down?)
      <policy name = ""TEST"">          =>  (What's this? )
        <property>
          <name>srcReplication</name>
          <value>3</value>
          <description> pick files for RAID only if their replication factor is
                        greater than or equal to this value.
          </description>
        </property>
        <property>
          <name>targetReplication</name>
          <value>2</value>
          <description> after RAIDing, decrease the replication factor of a file to
                        this value.
          </description>
        </property>
        <property>
          <name>metaReplication</name>
          <value>2</value>
          <description> the replication factor of the RAID meta file
          </description>
        </property>
        <property>
          <name>modTimePeriod</name>
          <value>100</value>
          <description> time (milliseconds) after a file is modified to make it a
                        candidate for RAIDing
          </description>
        </property>
 
 
*log:  when start-raidnode.sh

/************************************************************
SHUTDOWN_MSG: Shutting down RaidNode at test40/xxx.23.129.40
************************************************************/
2011-11-25 23:59:13,417 INFO org.apache.hadoop.raid.RaidNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting RaidNode
STARTUP_MSG:   host = test40/xxx.23.129.40
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.21.0
************************************************************/
2011-11-25 23:59:13,518 ERROR org.apache.hadoop.raid.ConfigManager: Reloading config 

file /KTBHOME/hadoop/hadoop/conf/raid.xml
2011-11-25 23:59:13,611 INFO org.apache.hadoop.security.Groups: Group mapping 

impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000
2011-11-25 23:59:13,647 WARN org.apache.hadoop.conf.Configuration: mapred.task.id is 

deprecated. Instead, use mapreduce.task.attempt.id
2011-11-25 23:59:13,714 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.srcReplication = 3
2011-11-25 23:59:13,714 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.targetReplication = 2
2011-11-25 23:59:13,715 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.metaReplication = 2
2011-11-25 23:59:13,715 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.modTimePeriod = 100
2011-11-25 23:59:13,716 INFO org.apache.hadoop.raid.ConfigManager: 

k_table1.targetReplication = 1
2011-11-25 23:59:13,716 INFO org.apache.hadoop.raid.ConfigManager: 

k_table1.metaReplication = 2
2011-11-25 23:59:13,716 INFO org.apache.hadoop.raid.ConfigManager: 

k_table1.modTimePeriod = 100
2011-11-25 23:59:13,728 INFO org.apache.hadoop.ipc.Server: Starting SocketReader
2011-11-25 23:59:13,751 INFO org.apache.hadoop.ipc.metrics.RpcMetrics: Initializing 

RPC Metrics with hostName=RaidNode, port=60000
2011-11-25 23:59:13,810 INFO org.apache.hadoop.ipc.metrics.RpcDetailedMetrics: 

Initializing RPC Metrics with hostName=RaidNode, port=60000
2011-11-25 23:59:13,812 INFO org.apache.hadoop.raid.RaidNode: RaidNode up at: 

/127.0.0.1:60000
2011-11-25 23:59:13,813 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: 

starting
2011-11-25 23:59:13,813 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 

60000: starting
2011-11-25 23:59:13,814 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 

60000: starting
2011-11-25 23:59:13,814 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 

60000: starting
2011-11-25 23:59:13,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 

60000: starting
2011-11-25 23:59:13,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 

60000: starting
2011-11-25 23:59:13,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 

60000: starting
2011-11-25 23:59:13,818 INFO org.apache.hadoop.raid.RaidNode: Triggering Policy 

Filter TEST hdfs://xxx.23.129.40:9006/TEST
2011-11-25 23:59:13,819 INFO org.apache.hadoop.raid.RaidNode: Started archive scan
2011-11-25 23:59:13,828 INFO org.apache.hadoop.raid.RaidNode: No filtered paths for 

policy TEST
2011-11-25 23:59:13,828 INFO org.apache.hadoop.raid.RaidNode: Triggering Policy 
Filter k_table1 hdfs://xxx.23.129.40:9006
2011-11-25 23:59:13,829 INFO org.apache.hadoop.raid.RaidNode: No filtered paths for 
policy k_table1"
hftp token renewal uses wrong port,HDFS-2326,,,,,"MAPREDUCE-2764 introduced a new method for token renewal.  The change appears to have a problem renewing hftp tokens.  The {{setDelegationToken}} method will reset the token's service to contain the remote rpc port.  However, the renewer expects it to be the remote https port."
"when i put data in hadoop cluster,there is a severe problem ",HDFS-2262,datanode,,,,"the datanode always give the message
Receiving one packet for block blk_******* of length 65536 seqno 204625 offsetInBlock 5917184 lastPacketInBlock false
PacketResponder 0 seqno=-2 for block blk_******** waiting for local datanode to finish write
Receiving one packet for block blk_******* of length 65536 seqno 204626 offsetInBlock 5982208 lastPacketInBlock false
PacketResponder 0 seqno=-2 for block blk_******** waiting for local datanode to finish write
...........
......
IOException in BlockReceiver.run()
java.io.IOException:No temprary file /mnt/data5/dfs/data/blocksBeingWritten/blk_****  for block_****_1461
    at org.apache.hadoop.hdfs.server.datanode.FSDateset.finalizeBlockInternal(FSdataset.java 1393)
    at org.apache.hadoop.hdfs.server.datanode.FSDateset.finalizeBlock(FSdataset.java 1370)
"
HDFS Contrib project ivy dependencies are not included in binary target,HDFS-768,build,,,,"As in HADOOP-6370, only Hadoop's own library dependencies are promoted to ${build.dir}/lib; any libraries required by contribs are not redistributed."
TestFileConcurrentReader test case is still timing out / failing,HDFS-1401,hdfs-client,,,,"The unit test case, TestFileConcurrentReader after its most recent fix in HDFS-1310 still times out when using java 1.6.0_07.  When using java 1.6.0_07, the test case simply hangs.  On apache Hudson build ( which possibly is using a higher sub-version of java) this test case has presented an inconsistent test result that it sometimes passes, some times fails. For example, between the most recent build 423, 424 and build 425, there is no effective change, however, the test case failed on build 424 and passed on build 425

build 424 test failed
https://hudson.apache.org/hudson/job/Hadoop-Hdfs-trunk/424/testReport/org.apache.hadoop.hdfs/TestFileConcurrentReader/

build 425 test passed
https://hudson.apache.org/hudson/job/Hadoop-Hdfs-trunk/425/testReport/org.apache.hadoop.hdfs/TestFileConcurrentReader/"
Unit test org.apache.hadoop.dfs.TestBalancer fails on Solaris with a timeout,HDFS-40,,,,,"Unit test org.apache.hadoop.dfs.TestBalancer fails on Solaris with a timeout

The console output is located at:
http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/lastBuild/console

Seems like the test was able to get the cluster balanced and then failed:

    [junit] The cluster is balanced. Exiting...
    [junit] 2008-02-05 17:18:22,215 INFO  dfs.StateChange (FSNamesystem.java:allocateBlock(1288)) - BLOCK* NameSystem.allocateBlock: /system/balancer.id. blk_3509566446573704077
    [junit] 2008-02-05 17:18:22,216 INFO  dfs.DataNode (DataNode.java:writeBlock(1048)) - Receiving block blk_3509566446573704077 src: /127.0.0.1:34105 dest: /127.0.0.1:34083
    [junit] 2008-02-05 17:18:22,217 INFO  dfs.DataNode (DataNode.java:writeBlock(1141)) - Datanode 0 forwarding connect ack to upstream firstbadlink is 
    [junit] 2008-02-05 17:18:22,218 INFO  dfs.DataNode (DataNode.java:receivePacket(2175)) - Receiving empty packet for block blk_3509566446573704077
    [junit] 2008-02-05 17:18:22,218 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1775)) - Received block blk_3509566446573704077 of size 3 from /127.0.0.1
    [junit] 2008-02-05 17:18:22,218 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1793)) - PacketResponder 0 for block blk_3509566446573704077 terminating
    [junit] 2008-02-05 17:18:22,219 INFO  dfs.StateChange (FSNamesystem.java:addStoredBlock(2485)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:34083 is added to blk_3509566446573704077 size 3
    [junit] 2008-02-05 17:18:22,254 INFO  dfs.StateChange (FSNamesystem.java:deleteInternal(1473)) - BLOCK* NameSystem.delete: blk_5642678384995114695 is added to invalidSet of 127.0.0.1:34083
    [junit] 2008-02-05 17:18:22,254 INFO  dfs.StateChange (FSNamesystem.java:deleteInternal(1473)) - BLOCK* NameSystem.delete: blk_3982250783996683858 is added to invalidSet of 127.0.0.1:34083
    [junit] 2008-02-05 17:18:22,255 INFO  dfs.StateChange (FSNamesystem.java:deleteInternal(1473)) - BLOCK* NameSystem.delete: blk_3509566446573704077 is added to invalidSet of 127.0.0.1:34083
    [junit] Balancing took 1.773 seconds
    [junit] 2008-02-05 17:18:23,601 INFO  dfs.StateChange (FSNamesystem.java:blocksToInvalidate(3013)) - BLOCK* NameSystem.blockToInvalidate: ask 127.0.0.1:34066 to delete  blk_-7040479509416829015 blk_7804526723654242046 blk_-7913732279285380423 blk_-4517202454091217528 blk_-7200213733738623986 blk_3308335641891931015 blk_-7617910363506885261 blk_-6988117892488064923 blk_-8927068573243769130
    [junit] 2008-02-05 17:18:23,601 INFO  dfs.StateChange (FSNamesystem.java:blocksToInvalidate(3013)) - BLOCK* NameSystem.blockToInvalidate: ask 127.0.0.1:34083 to delete  blk_5642678384995114695 blk_3982250783996683858 blk_3509566446573704077
    [junit] Running org.apache.hadoop.dfs.TestBalancer
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestBalancer FAILED (timeout)
"
Backporting HDFS-12882 to branch-3.0: Support full open(PathHandle) contract in HDFS,HDFS-14159,hdfs,,,,"This task aims to backport聽HDFS-12882 and some connecting commits to branch-3.0 without introducing API incompatibilities.

In order to be able to cleanly backport, first HDFS-7878, then HDFS-12877 should be backported to that branch as well (both can be executed cleanly, and with build success).

Also, this patch would also introduce API backward incompatibilities in hadoop-hdfs-client, and we should聽modify it to a compat change (similar as in HDFS-13830 fought with this problem).

聽"
how to ask a question about hdfs锛?I don't find the page,HDFS-14173,,,,,
hdfsread crash when reading data reaches to 128M,HDFS-10369,fs,,,,"see code below, it would crash after   printf(""hdfsGetDefaultBlockSize2:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  
hdfsFile read_file = hdfsOpenFile(fs, ""/testpath"", O_RDONLY, 0, 0, 1); 
  int total = hdfsAvailable(fs, read_file);
  printf(""Total:%d\n"", total);
  char* buffer = (char*)malloc(sizeof(size+1) * sizeof(char));
  int ret = -1; 
  int len = 0;
  ret = hdfsSeek(fs, read_file, 134152192);
  printf(""hdfsGetDefaultBlockSize1:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  ret = hdfsRead(fs, read_file, (void*)buffer, size);
  printf(""hdfsGetDefaultBlockSize2:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  ret = hdfsRead(fs, read_file, (void*)buffer, size);
  printf(""hdfsGetDefaultBlockSize3:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  return 0;"
TestConfigurationFieldsBase.testCompareConfigurationClassAgainstXml fails due to missing dfs.image.string-tables.expanded from hdfs-defaults.xml,HDFS-14100,,,,,After聽HDFS-13882聽TestConfigurationFieldsBase.testCompareConfigurationClassAgainstXml聽requires hdfs-defaults.xml to have dfs.image.string-tables.expanded added and populated with a default value.
[SPS]: Fix the branch review comments,HDFS-13084,,,,,"Fix the review comments provided by [~daryn]

聽"
Add docs for NameNode initializeSharedEdits and bootstrapStandby commands,HDFS-3455,documentation,,,,"We've made the HA setup easier by adding new flags to the namenode to automatically set up the standby. But, we didn't document them yet. We should amend the HDFSHighAvailability.apt.vm docs to include this."
TestHttpFSWithKerberos failed,HDFS-11255,httpfs,,,,"{noformat}
$ mvn test -P\!shelltest -Dtest=TestHttpFSWithKerberos
...
Running org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos
Tests run: 6, Failures: 1, Errors: 5, Skipped: 0, Time elapsed: 7.356 sec <<< FAILURE! - in org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos
testDelegationTokenWithWebhdfsFileSystem(org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos)  Time elapsed: 4.73 sec  <<< ERROR!
org.apache.hadoop.security.KerberosAuthException: Login failure for user: client from keytab /Users/tucu/tucu.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:897)
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760)
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1092)
        at org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos.testDelegationTokenWithinDoAs(TestHttpFSWithKerberos.java:239)
        at org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos.testDelegationTokenWithWebhdfsFileSystem(TestHttpFSWithKerberos.java:270)

testInvalidadHttpFSAccess(org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos)  Time elapsed: 1.581 sec  <<< FAILURE!
java.lang.AssertionError: expected:<503> but was:<401>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.junit.Assert.assertEquals(Assert.java:542)
        at org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos.testInvalidadHttpFSAccess(TestHttpFSWithKerberos.java:144)
...
Failed tests: 
  TestHttpFSWithKerberos.testInvalidadHttpFSAccess:144 expected:<503> but was:<401>

Tests in error: 
  TestHttpFSWithKerberos.testDelegationTokenWithWebhdfsFileSystem:270->testDelegationTokenWithinDoAs:239 禄 KerberosAuth
  TestHttpFSWithKerberos.testValidHttpFSAccess:120 禄 Login Unable to obtain pass...
  TestHttpFSWithKerberos.testDelegationTokenWithHttpFSFileSystem:262->testDelegationTokenWithinDoAs:239 禄 KerberosAuth
  TestHttpFSWithKerberos.testDelegationTokenWithHttpFSFileSystemProxyUser:279->testDelegationTokenWithinDoAs:239 禄 KerberosAuth
  TestHttpFSWithKerberos.testDelegationTokenHttpFSAccess:155 禄 Login Unable to o...
{noformat}"
HDFS FILE CREATE APPEND LEASE,HDFS-14031,block placement,,,,"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /mnt/data1/datadir/hive_cluster1/warehouse/singhand_ntcstore.db/singhand_ntc_ip_tmp1/singhand_ntc_ip_log2073 for DFSClient_NONMAPREDUCE_818722237_1 on 172.16.0.166 because DFSClient_NONMAPREDUCE_818722237_1 is already the current lease holder.
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2883)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2683)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2982)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2950)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:654)
聽聽 聽at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:421)
聽聽 聽at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
聽聽 聽at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
聽聽 聽at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
聽聽 聽at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
聽聽 聽at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
聽聽 聽at java.security.AccessController.doPrivileged(Native Method)
聽聽 聽at javax.security.auth.Subject.doAs(Subject.java:422)
聽聽 聽at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
聽聽 聽at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)

聽聽 聽at org.apache.hadoop.ipc.Client.call(Client.java:1475)
聽聽 聽at org.apache.hadoop.ipc.Client.call(Client.java:1412)
聽聽 聽at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
聽聽 聽at com.sun.proxy.$Proxy8.append(Unknown Source)
聽聽 聽at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:328)
聽聽 聽at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
聽聽 聽at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
聽聽 聽at java.lang.reflect.Method.invoke(Method.java:498)
聽聽 聽at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
聽聽 聽at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
聽聽 聽at com.sun.proxy.$Proxy9.append(Unknown Source)
聽聽 聽at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1808)
聽聽 聽at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1877)
聽聽 聽at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1847)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)
聽聽 聽at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:348)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:318)
聽聽 聽at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1164)
聽聽 聽at com.singhand.hdfs.utils.HDFSWriter.appendWriter(HDFSWriter.java:90)
聽聽 聽at com.singhand.ntc.mock.ReadTopic2HDFS.loadData(ReadTopic2HDFS.java:149)
聽聽 聽at com.singhand.ntc.mock.ReadTopic2HDFS.kafka2HDFS(ReadTopic2HDFS.java:74)
聽聽 聽at com.singhand.ntc.mock.ReadTopic2HDFS.main(ReadTopic2HDFS.java:255)"
RBF: Adding trace support,HDFS-13365,,,,,We should support HTrace and add spans.
Need 'force close',HDFS-7307,,,,,"Until HDFS-4882 and HDFS-7306 get real fixes, operations teams need a way to force close files.  DNs are essentially held hostage by broken clients that never close.  This situation will get worse as longer/permanently running jobs start increasing."
Consolidate the HA NN documentation down to one,HDFS-7777,,,,,These are nearly the same document now.  Let's consolidate.
distribute-excludes and refresh-namenodes update to new shell framework,HDFS-7850,,,,,These need to get updated to use new shell framework.
NFS hard codes ShellBasedIdMapping,HDFS-7904,nfs,,,,The current NFS doesn't allow one to configure an alternative to the shell-based id mapping provider.  
Move the synthetic load generator into its own package,HDFS-8251,,,,,"It doesn't really make sense for the HDFS load generator to be a part of the (extremely large) mapreduce jobclient package. It should be pulled out and put its own package, probably in hadoop-tools."
WebHDFS REST v2,HDFS-9055,webhdfs,,,,There's starting to be enough changes to fix and add missing functionality to webhdfs that we should probably update to REST v2.  This also gives us an opportunity to deal with some incompatible issues.
add set/remove quota capability to webhdfs,HDFS-9056,webhdfs,,,,It would be nice to be able to set and remove quotas via WebHDFS.
enable find via WebHDFS,HDFS-9058,webhdfs,,,,It'd be useful to implement find over webhdfs rather than forcing the client to grab a lot of data.
hdfs groups should be exposed via WebHDFS,HDFS-9061,webhdfs,,,,It would be extremely useful from a REST perspective to expose which groups the NN says the user belongs to.
Add liberasurecode support,HDFS-9778,erasure-coding,,,,It would be beneficial to use liberasurecode as either supplemental or in lieu of ISA-L in order to provide the widest possible hardware/OS platform and OOB support.  Major software platforms appear to be converging on this library and we should too.
httpfs generates docs in bin tarball ,HDFS-10509,build,documentation,,,"When building a release, httpfs generates a share/doc/hadoop/httpfs dir with content when it shouldn't."
clearCorruptLazyPersistFiles could crash NameNode,HDFS-13672,,,,,"I started a NameNode on a pretty large fsimage. Since the NameNode is started without any DataNodes, all blocks (100 million) are ""corrupt"".

Afterwards I observed FSNamesystem#clearCorruptLazyPersistFiles() held write lock for a long time:

{noformat}
18/06/12 12:37:03 INFO namenode.FSNamesystem: FSNamesystem write lock held for 46024 ms via
java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:945)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:198)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock(FSNamesystem.java:1689)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.clearCorruptLazyPersistFiles(FSNamesystem.java:5532)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.run(FSNamesystem.java:5543)
java.lang.Thread.run(Thread.java:748)
        Number of suppressed write-lock reports: 0
        Longest write-lock held interval: 46024
{noformat}

Here's the relevant code:

{code}
      writeLock();

      try {
        final Iterator<BlockInfo> it =
            blockManager.getCorruptReplicaBlockIterator();

        while (it.hasNext()) {
          Block b = it.next();
          BlockInfo blockInfo = blockManager.getStoredBlock(b);
          if (blockInfo.getBlockCollection().getStoragePolicyID() == lpPolicy.getId()) {
            filesToDelete.add(blockInfo.getBlockCollection());
          }
        }

        for (BlockCollection bc : filesToDelete) {
          LOG.warn(""Removing lazyPersist file "" + bc.getName() + "" with no replicas."");
          changed |= deleteInternal(bc.getName(), false, false, false);
        }
      } finally {
        writeUnlock();
      }
{code}
In essence, the iteration over corrupt replica list should be broken down into smaller iterations to avoid a single long wait.

Since this operation holds NameNode write lock for more than 45 seconds, the default ZKFC connection timeout, it implies an extreme case like this (100 million corrupt blocks) could lead to NameNode failover."
To detect fsimage corruption on the spot,HDFS-13031,hdfs,,,,"Since we fixed HDFS-9406, there are new cases reported from the field that similar fsimage corruption happens. We need good fsimage + editlogs to replay to reproduce the corruption. However, usually when the corruption is detected (at later NN restart), the good fsimage is already deleted.

We need to have a way to detect fsimage corruption on the spot. Currently what I think we could do is:
 # after SNN creates a new fsimage, it spawn a new modified NN process (NN with some new command line args) to just load the fsimage and do nothing else.聽
 # If the process failed, the currently running SNN will do either a) backup the fsimage + editlogs or b) no longer do checkpointing. And it need to somehow raise a flag to user that the fsimage is corrupt.

In step 2, if we do a, we need to introduce new NN->JN API to backup editlogs; if we do b, it changes SNN's behavior, and kind of not compatible.聽
"
"RBF: Remove FSCK from Router Web UI, because fsck is not supported currently",HDFS-13803,,,,,"When i click FSCK on Router Web UI Utilities, i got errors
{quote}
HTTP ERROR 404
Problem accessing /fsck. Reason:

    NOT_FOUND
Powered by Jetty://
{quote}
I deep into the source code and find that fsck is not supported currently, So i think we should remove FSCK from Router Web UI"
[DOC] update flag is not necessary to avoid verifying checksums,HDFS-13764,documentation,,,,"We mentioned to use ""-update"" option to avoid checksum in the following doc:

[https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#Copying_between_encrypted_and_unencrypted_locations]
{code:java}
Copying between encrypted and unencrypted locations
By default, distcp compares checksums provided by the filesystem to verify that the data was successfully copied to the destination. When copying between an unencrypted and encrypted location, the filesystem checksums will not match since the underlying block data is different. In this case, specify the聽-skipcrccheck聽and聽-update聽distcp flags to avoid verifying checksums.
{code}
聽

But actually, ""-update"" option is not necessary, only ""-skipcrccheck"" is needed. Can we change it to:

聽
{code:java}
Copying between encrypted and unencrypted locations
By default, distcp compares checksums provided by the filesystem to verify that the data was successfully copied to the destination. When copying between an unencrypted and encrypted location, the filesystem checksums will not match since the underlying block data is different. In this case, specify the聽-skipcrccheck聽flags to avoid verifying checksums.
{code}
聽"
"""hdfs dfs -du -h /*"" shows local dir instead of HDFS dir",HDFS-13751,hdfs,,,,"Hi,

""hdfs dfs -du聽 -h / ""command shows below result.

0聽 聽 聽 聽 /app-logs

95.2 G 聽 /apps

0聽 聽 聽 聽 /ats

249.8 M聽 /hdp

0聽 聽 聽 聽 /mapred

0聽 聽 聽 聽 /mr-history

0聽 聽 聽 聽 /ngs

18.2 M 聽 /tmp

431.5 M聽 /user

聽

Where as adding a * as ""hdfs dfs -du聽 -h /* "" shows below result.

du: `/bin': No such file or directory

du: `/boot': No such file or directory

du: `/cgroup': No such file or directory

du: `/cgroups_test': No such file or directory

du: `/dev': No such file or directory

du: `/etc': No such file or directory

du: `/hadoop': No such file or directory

du: `/home': No such file or directory

du: `/lib': No such file or directory

du: `/lib64': No such file or directory

du: `/lost+found': No such file or directory

du: `/media': No such file or directory

du: `/mnt': No such file or directory

du: `/ngs1': No such file or directory

du: `/ngs2': No such file or directory

du: `/ngs3': No such file or directory

du: `/ngs4': No such file or directory

du: `/ngs5': No such file or directory

du: `/ngs6': No such file or directory

du: `/ngs7': No such file or directory

du: `/ngs8': No such file or directory

du: `/opt': No such file or directory

du: `/proc': No such file or directory

du: `/root': No such file or directory

du: `/sbin': No such file or directory

du: `/selinux': No such file or directory

du: `/srv': No such file or directory

du: `/sys': No such file or directory

聽

0聽 聽 聽 聽 /tmp/demo

0聽 聽 聽 聽 /tmp/demo1

0聽 聽 聽 聽 /tmp/entity-file-history

390.8 K聽 /tmp/examples

64.8 K 聽 /tmp/examples.tar.gz

3.3 K聽 聽 /tmp/ide20a8b61_date472418

3.3 K聽 聽 /tmp/ide20a9661_date432418

3.3 K聽 聽 /tmp/ide20a9961_date151318

3.3 K聽 聽 /tmp/ide20aa061_date082418

17.7 M 聽 /tmp/lib_20180406111044

聽

聽

Why does adding ""*"" re-directs to local dir instead of HDFS dir.

聽

Thanks"
Ozone: Replace Jersey container with Netty Container,HDFS-10357,ozone,,,,"In the ozone branch, we have implemented Web Interface calls using JAX-RS. This was very useful when the REST interfaces where in flux. This JIRA proposes to replace Jersey based code with pure netty and remove any dependency that Ozone has on Jersey. This will create both faster and simpler code in Ozone web interface."
Ozone: SCM: Handle duplicate Datanode ID ,HDFS-11139,ozone,,,,"The Datanode ID is used when a data node registers. It is assumed that datanodes are unique across the cluster. 
However due to operator error or other cases we might encounter duplicate datanode ID. SCM should be able to recognize this and handle in correctly. Here is a sub-set  of datanode scenarios it needs to handle.

1. Normal Datanode
2.  Copy of a Datanode metadata by operator to another node
3. A Datanode being renamed - hostname change
4. Container Reports -- 2 machines with same datanode ID. SCM thinks they are same node.
5. Decommission --  we decommission both nodes if IDs are same.
6. Commands will be send to both nodes.

So it is necessary that SCM identity when a datanode is reusing a datanode ID that is already used by another node.  

"
Edit tailing period configuration should accept time units,HDFS-13595,ha,namenode,,,"The {{dfs.ha.tail-edits.period}} config should accept time units to be able to more easily specified across a wide range, and in particular for HDFS-13150 it is useful to have a period shorter than 1 second which is not currently possible."
[um,HDFS-13565,,,,,
Hftp should support namenode logical service names in URI,HDFS-5123,ha,,,,"For example if the dfs.nameservices is set to arpit

{code}
hdfs dfs -ls hftp://arpit:50070/tmp

or 

hdfs dfs -ls hftp://arpit/tmp
{code}
does not work

You have to provide the exact active namenode hostname. On an HA cluster using dfs client one should not need to provide the active nn hostname"
Fix TestEncryptionZonesWithKMS failure due to HADOOP-14445,HDFS-13430,,,,,"Unfortunately HADOOP-14445 had an HDFS test failure that's not caught in the hadoop-common precommit runs.

This is caught聽by our internal pre-commit using dist-test, and appears to be the only failure."
TestDistributedFileSystem.testAllWithNoXmlDefaults failed intermittently,HDFS-6589,hdfs-client,,,,"https://builds.apache.org/job/PreCommit-HDFS-Build/7207 is clean
https://builds.apache.org/job/PreCommit-HDFS-Build/7208 has the following failure. The code is essentially the same.

Running the same test locally doesn't reproduce. A flaky test there.

{code}
Stacktrace

java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClient(TestDistributedFileSystem.java:263)
	at org.apache.hadoop.hdfs.TestDistributedFileSystem.testAllWithNoXmlDefaults(TestDistributedFileSystem.java:651)
{code}
"
Secure Datanode stop/start from cli does not throw a valid error if HDFS_DATANODE_SECURE_USER is not set,HDFS-13501,,,,,Secure Datanode start/stop from cli does not throw a valid error if HADOOP_SECURE_DN_USER/HDFS_DATANODE_SECURE_USER is not set. If HDFS_DATANODE_SECURE_USER and JSVC_HOME is not set start/stop is expected to fail (when privilege ports are used) but it should show some valid message.
#NAME?,HDFS-11515,namenode,shell,,,"HDFS-10797 fixed a disk summary (-du) bug, but it introduced a new bug.

The bug can be reproduced running the following commands:
{noformat}
bash-4.1$ hdfs dfs -mkdir /tmp/d0
bash-4.1$ hdfs dfsadmin -allowSnapshot /tmp/d0
Allowing snaphot on /tmp/d0 succeeded
bash-4.1$ hdfs dfs -touchz /tmp/d0/f4
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1
bash-4.1$ hdfs dfs -createSnapshot /tmp/d0 s1
Created snapshot /tmp/d0/.snapshot/s1
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d2
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d3
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d2/d4
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d3/d5
bash-4.1$ hdfs dfs -createSnapshot /tmp/d0 s2
Created snapshot /tmp/d0/.snapshot/s2
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d2/d4
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d2
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d3/d5
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d3
bash-4.1$ hdfs dfs -du -h /tmp/d0
du: java.util.ConcurrentModificationException
0 0 /tmp/d0/f4
{noformat}

A ConcurrentModificationException forced du to terminate abruptly.

Correspondingly, NameNode log has the following error:
{noformat}
2017-03-08 14:32:17,673 WARN org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSumma
ry from 10.0.0.198:49957 Call#2 Retry#0
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
        at java.util.HashMap$KeyIterator.next(HashMap.java:956)
        at org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext.tallyDeletedSnapshottedINodes(ContentSummaryComputationContext.java:209)
        at org.apache.hadoop.hdfs.server.namenode.INode.computeAndConvertContentSummary(INode.java:507)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(FSDirectory.java:2302)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:4535)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1087)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getContentSummary(AuthorizationProviderProxyClientProtocol.java:5
63)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.jav
a:873)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)
{noformat}

The bug is due to a improper use of HashSet, not concurrent operations. Basically, a HashSet can not be updated while an iterator is traversing it."
Add config for min percentage of data nodes to come out of chill mode in SCM,HDFS-13354,,,,,SCM will come out of ChillMode if one datanode reports in now. We need to support percentage of known datanodes before SCM comes out of Chill Mode.
1.x: Add a retention period for purged edit logs,HDFS-3653,namenode,,,,"Occasionally we have a bug which causes something to go wrong with edits files. Even more occasionally the bug is such that the namenode mistakenly deletes an {{edits}} file without merging it into {{fsimage}} properly -- e.g if the bug mistakenly writes an OP_INVALID at the top of the log.

In trunk/2.0 we retain many edit log segments going back in time to be more robust to this kind of error. I'd like to implement something similar (but much simpler) in 1.x, which would be used only by HDFS developers in root-causing or repairing from these rare scenarios: the NN should never directly delete an edit log file. Instead, it should rename the file into some kind of ""trash"" directory inside the name dir, and associate it with a timestamp. Then, periodically a separate thread should scan the trash dirs and delete any logs older than a configurable time."
"If an edits file has more edits in it than expected by its name, should trigger an error",HDFS-3069,namenode,,,,"In testing what happens in HA split brain scenarios, I ended up with an edits log that was named edits_47-47 but actually had two edits in it (#47 and #48). The edits loading process should detect this situation and barf. Otherwise, the problem shows up later during loading or even on the next restart, and is tough to fix."
QJM should validate startLogSegment() more strictly,HDFS-5058,qjm,,,,"We've seen a small handful of times a case where one of the NNs in an HA cluster ends up with an fsimage checkpoint that falls in the middle of an edit segment. We're not sure yet how this happens, but one issue can happen as a result:
- Node has fsimage_500. Cluster has edits_1-1000, edits_1001_inprogress
- Node restarts, loads fsimage_500
- Node wants to become active. It calls selectInputStreams(500). Currently, this API logs a WARN that 500 falls in the middle of the 1-1000 segment, but continues and returns no results.
- Node calls startLogSegment(501).

Currently, the QJM will accept this (incorrectly). The node then crashes when it first tries to journal a real transaction, but it ends up leaving the edits_501_inprogress lying around, potentially causing more issues later."
Ozone: start-all script is missing ozone start,HDFS-12707,,,,,start-all script is missing ozone start
libhdfs++: Integrate logging with the C API,HDFS-10205,hdfs-client,,,,"Logging was added in HDFS-9118.  The C API has it's own API based on setting errno that is independent from the rest of the logs.  These should be tied together so C API events get logged as well when it is used.

It might also be useful to log the event handlers added in HDFS-9616 as part of this work."
"start-dfs.sh and hdfs --daemon start datanode say ""ERROR: Cannot set priority of datanode process XXXX""",HDFS-13397,hdfs,,,,"When executing
{code:java}
$HADOOP_HOME/bin/hdfs --daemon start datanode
{code}
as a regular user (e.g. ""hdfs"") you achieve fail saying
{code:java}
ERROR: Cannot set priority of datanode process XXXX
{code}
where XXXX is some PID.

It turned out that this is because at least on Gentoo Linux (and I think this is pretty well universal), by default a regular user process can't increase the priority of itself or any of the user's other processes. To fix this, I added these lines to /etc/security/limits.conf [NOTE: the users hdfs, yarn, and mapred are in the group called hadoop on this system]:
{code:java}
@hadoop聽聽 聽聽聽聽 hard聽聽聽 nice聽聽聽聽聽聽聽聽聽聽聽 -15
@hadoop聽聽聽聽聽聽聽 hard聽聽聽 priority聽聽聽聽聽聽聽 -15
{code}
This change will need to be made on all datanodes.

The need to enable [at minimum] the hdfs user to raise its processes' priority needs to be added to the documentation. This is not a problem I observed under 3.0.0."
Ozone: ObjectStoreRestPlugin initialization depend on HdslDatanodeService,HDFS-13367,ozone,,,,
TestFileCreation#testOverwriteOpenForWrite hangs,HDFS-7304,,,,,The test case times out. It has been observed in multiple pre-commit builds.
DistributedFileSystem#getStatus returns a misleading FsStatus,HDFS-10388,fs,,,,"The method {{DistributedFileSystem#getStatus}} returns the dfs's disk status.
{code}
   public FsStatus getStatus(Path p) throws IOException {
     statistics.incrementReadOps(1);
    return dfs.getDiskStatus();
   }
{code}
So the param path is no meaning here. And the object returned will mislead for users to use this method. I looked into the code, only when the file system has multiple partitions, the use and capacity of the partition pointed to by the specified path will be reflected. For example, in the subclass {{RawLocalFileSystem}}, it will be return correctly.

We should return a new meaningless FsStatus here (like new FsStatus(0, 0, 0)) and indicate that the invoked method isn't available in {{DistributedFileSystem}}."
TestDistributedFileSystem#testGetFileBlockStorageLocationsError is flaky,HDFS-6308,,,,,"Found this on pre-commit build of HDFS-6261
{code}
java.lang.AssertionError: Expected one valid and one invalid volume
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.hdfs.TestDistributedFileSystem.testGetFileBlockStorageLocationsError(TestDistributedFileSystem.java:837)
{code}"
StreamCapabilities.StreamCapability should be public.,HDFS-12260,,,,,"Client should use {{StreamCapability}} enum instead of raw string to query the capability of an OutputStream, for better type safety / IDE supports and etc."
Shaded jar for hadoop-hdfs-client,HDFS-12928,,,,,"This jira is to have a shaded client for hadoop-hdfs-client.

Similar to hadoop-client, which has hadoop-client-api and hadoop-client-runtime.

Similarly for hadoop-hdfs-client, we can have hadoop-hdfs-clienta-api and hadoop-hdfs-client-runtime.

This will help, when I am working on hdfs client programs, and want to use only hdfs client and does not require mapreduce-client and yarn-client, I can use this new jar, instead of hadoop-client-api jar which includes everything.

And also when I am breaking Hadoop in to modules like hdfs-client, yarn-client and mapred-client, and upgrade only one component this helps.

"
TestBlockToken fails on JDK 7,HDFS-6132,,,,,"Currently, UserGroupInformation.setConfiguration(conf) does not reset loginUser. This is causing this test case to fail.

For now, the work around solution is to use UserGroupInformation.reset() before UserGroupInformation.setConfiguration(conf). However, in general, we should do this reset whenever we call UserGroupInformation.setConfiguration(conf)"
Ambari UI deploy fails during startup of Ambari Metrics,HDFS-13169,,,,,"{noformat}
HDP version:    HDP-3.0.0.0-702
Ambari version: 2.99.99.0-77
{noformat}

/var/lib/ambari-agent/data/errors-52.txt:
{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py"", line 90, in <module>
    AmsCollector().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 371, in execute
    self.execute_prefix_function(self.command_name, 'post', env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 392, in execute_prefix_function
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 434, in post_start
    raise Fail(""Pid file {0} doesn't exist after starting of the component."".format(pid_file))
resource_management.core.exceptions.Fail: Pid file /var/run/ambari-metrics-collector//hbase-ams-master.pid doesn't exist after starting of the component.
{noformat}

/var/lib/ambari-agent/data/output-52.txt:
{noformat}
2018-01-11 13:03:40,753 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-702 -> 3.0.0.0-702
2018-01-11 13:03:40,755 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:40,884 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-702 -> 3.0.0.0-702
2018-01-11 13:03:40,885 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:40,886 - Group['hdfs'] {}
2018-01-11 13:03:40,887 - Group['hadoop'] {}
2018-01-11 13:03:40,887 - Group['users'] {}
2018-01-11 13:03:40,887 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,890 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,891 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,892 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,893 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,893 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
2018-01-11 13:03:40,894 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,894 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
2018-01-11 13:03:40,895 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
2018-01-11 13:03:40,895 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,896 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,897 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,897 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,898 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
2018-01-11 13:03:40,903 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
2018-01-11 13:03:40,903 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:40,904 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,905 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,906 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
2018-01-11 13:03:40,913 - call returned (0, '1002')
2018-01-11 13:03:40,914 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1002'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
2018-01-11 13:03:40,917 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1002'] due to not_if
2018-01-11 13:03:40,918 - Group['hdfs'] {}
2018-01-11 13:03:40,918 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', 'hdfs']}
2018-01-11 13:03:40,919 - FS Type: 
2018-01-11 13:03:40,919 - Directory['/etc/hadoop'] {'mode': 0755}
2018-01-11 13:03:40,932 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,933 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
2018-01-11 13:03:40,947 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}
2018-01-11 13:03:40,962 - Directory['/var/log/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:40,964 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}
2018-01-11 13:03:40,964 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'create_parents': True, 'cd_access': 'a'}
2018-01-11 13:03:40,967 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}
2018-01-11 13:03:40,969 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'hdfs'}
2018-01-11 13:03:40,973 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:40,982 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/hadoop-metrics2.properties'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,983 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
2018-01-11 13:03:40,983 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,987 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:40,991 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
2018-01-11 13:03:40,996 - Skipping stack-select on AMBARI_METRICS because it does not exist in the stack-select package structure.
2018-01-11 13:03:41,142 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:41,144 - checked_call['hostid'] {}
2018-01-11 13:03:41,147 - checked_call returned (0, '007f0100')
2018-01-11 13:03:41,150 - Directory['/etc/ams-hbase/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,151 - Changing owner for /etc/ams-hbase/conf from 0 to ams
2018-01-11 13:03:41,151 - Changing group for /etc/ams-hbase/conf from 0 to hadoop
2018-01-11 13:03:41,151 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,159 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] since it doesn't exist.
2018-01-11 13:03:41,159 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp from 0 to ams
2018-01-11 13:03:41,160 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:41,160 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] since it doesn't exist.
2018-01-11 13:03:41,160 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 0 to ams
2018-01-11 13:03:41,160 - Changing group for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 0 to hadoop
2018-01-11 13:03:41,160 - Changing permission for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 755 to 775
2018-01-11 13:03:41,161 - File['/etc/ams-hbase/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,161 - File['/etc/ams-hbase/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,161 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,170 - Generating config: /etc/ams-hbase/conf/hbase-site.xml
2018-01-11 13:03:41,170 - File['/etc/ams-hbase/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,198 - Writing File['/etc/ams-hbase/conf/hbase-site.xml'] because contents don't match
2018-01-11 13:03:41,198 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,198 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool'] since it doesn't exist.
2018-01-11 13:03:41,199 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool from 0 to ams
2018-01-11 13:03:41,199 - Changing group for /var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool from 0 to hadoop
2018-01-11 13:03:41,199 - XmlConfig['hbase-policy.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {}, 'configurations': {'security.masterregion.protocol.acl': '*', 'security.admin.protocol.acl': '*', 'security.client.protocol.acl': '*'}}
2018-01-11 13:03:41,205 - Generating config: /etc/ams-hbase/conf/hbase-policy.xml
2018-01-11 13:03:41,205 - File['/etc/ams-hbase/conf/hbase-policy.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,207 - Writing File['/etc/ams-hbase/conf/hbase-policy.xml'] because contents don't match
2018-01-11 13:03:41,213 - File['/etc/ams-hbase/conf/hbase-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,215 - Writing File['/etc/ams-hbase/conf/hbase-env.sh'] because contents don't match
2018-01-11 13:03:41,219 - File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties.j2'), 'owner': 'ams', 'group': 'hadoop'}
2018-01-11 13:03:41,220 - Writing File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] because contents don't match
2018-01-11 13:03:41,220 - TemplateConfig['/etc/ams-hbase/conf/regionservers'] {'owner': 'ams', 'template_tag': None}
2018-01-11 13:03:41,222 - File['/etc/ams-hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'ams', 'group': None, 'mode': None}
2018-01-11 13:03:41,223 - Writing File['/etc/ams-hbase/conf/regionservers'] because it doesn't exist
2018-01-11 13:03:41,223 - Changing owner for /etc/ams-hbase/conf/regionservers from 0 to ams
2018-01-11 13:03:41,223 - Directory['/var/run/ambari-metrics-collector/'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,224 - Creating directory Directory['/var/run/ambari-metrics-collector/'] since it doesn't exist.
2018-01-11 13:03:41,224 - Changing owner for /var/run/ambari-metrics-collector/ from 0 to ams
2018-01-11 13:03:41,225 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,225 - Creating directory Directory['/var/log/ambari-metrics-collector'] since it doesn't exist.
2018-01-11 13:03:41,225 - Changing owner for /var/log/ambari-metrics-collector from 0 to ams
2018-01-11 13:03:41,225 - Directory['/var/lib/ambari-metrics-collector/hbase'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,225 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase'] since it doesn't exist.
2018-01-11 13:03:41,226 - Changing owner for /var/lib/ambari-metrics-collector/hbase from 0 to ams
2018-01-11 13:03:41,226 - File['/var/run/ambari-metrics-collector//distributed_mode'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,228 - File['/etc/ams-hbase/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,229 - Writing File['/etc/ams-hbase/conf/log4j.properties'] because contents don't match
2018-01-11 13:03:41,230 - Directory['/etc/ams-hbase/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,230 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,230 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:41,231 - File['/etc/ams-hbase/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,231 - File['/etc/ams-hbase/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,231 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,237 - Generating config: /etc/ams-hbase/conf/hbase-site.xml
2018-01-11 13:03:41,237 - File['/etc/ams-hbase/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,264 - XmlConfig['hbase-policy.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {}, 'configurations': {'security.masterregion.protocol.acl': '*', 'security.admin.protocol.acl': '*', 'security.client.protocol.acl': '*'}}
2018-01-11 13:03:41,270 - Generating config: /etc/ams-hbase/conf/hbase-policy.xml
2018-01-11 13:03:41,270 - File['/etc/ams-hbase/conf/hbase-policy.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,280 - File['/etc/ams-hbase/conf/hbase-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,283 - File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties.j2'), 'owner': 'ams', 'group': 'hadoop'}
2018-01-11 13:03:41,283 - TemplateConfig['/etc/ams-hbase/conf/regionservers'] {'owner': 'ams', 'template_tag': None}
2018-01-11 13:03:41,284 - File['/etc/ams-hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'ams', 'group': None, 'mode': None}
2018-01-11 13:03:41,285 - Directory['/var/run/ambari-metrics-collector/'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,285 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,287 - File['/etc/ams-hbase/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,288 - Directory['/etc/ambari-metrics-collector/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,288 - Changing owner for /etc/ambari-metrics-collector/conf from 0 to ams
2018-01-11 13:03:41,288 - Changing group for /etc/ambari-metrics-collector/conf from 0 to hadoop
2018-01-11 13:03:41,289 - Directory['/var/lib/ambari-metrics-collector/checkpoint'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,289 - Creating directory Directory['/var/lib/ambari-metrics-collector/checkpoint'] since it doesn't exist.
2018-01-11 13:03:41,289 - Changing owner for /var/lib/ambari-metrics-collector/checkpoint from 0 to ams
2018-01-11 13:03:41,289 - Changing group for /var/lib/ambari-metrics-collector/checkpoint from 0 to hadoop
2018-01-11 13:03:41,289 - XmlConfig['ams-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {}, 'configurations': ...}
2018-01-11 13:03:41,296 - Generating config: /etc/ambari-metrics-collector/conf/ams-site.xml
2018-01-11 13:03:41,296 - File['/etc/ambari-metrics-collector/conf/ams-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,339 - Writing File['/etc/ambari-metrics-collector/conf/ams-site.xml'] because contents don't match
2018-01-11 13:03:41,339 - XmlConfig['ssl-server.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {}, 'configurations': ...}
2018-01-11 13:03:41,346 - Generating config: /etc/ambari-metrics-collector/conf/ssl-server.xml
2018-01-11 13:03:41,346 - File['/etc/ambari-metrics-collector/conf/ssl-server.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,351 - Writing File['/etc/ambari-metrics-collector/conf/ssl-server.xml'] because it doesn't exist
2018-01-11 13:03:41,351 - Changing owner for /etc/ambari-metrics-collector/conf/ssl-server.xml from 0 to ams
2018-01-11 13:03:41,351 - Changing group for /etc/ambari-metrics-collector/conf/ssl-server.xml from 0 to hadoop
2018-01-11 13:03:41,351 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,357 - Generating config: /etc/ambari-metrics-collector/conf/hbase-site.xml
2018-01-11 13:03:41,357 - File['/etc/ambari-metrics-collector/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,384 - Writing File['/etc/ambari-metrics-collector/conf/hbase-site.xml'] because contents don't match
2018-01-11 13:03:41,385 - File['/etc/ambari-metrics-collector/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,387 - Writing File['/etc/ambari-metrics-collector/conf/log4j.properties'] because contents don't match
2018-01-11 13:03:41,390 - File['/etc/ambari-metrics-collector/conf/ams-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,391 - Writing File['/etc/ambari-metrics-collector/conf/ams-env.sh'] because contents don't match
2018-01-11 13:03:41,392 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,392 - Changing group for /var/log/ambari-metrics-collector from 0 to hadoop
2018-01-11 13:03:41,392 - Directory['/var/run/ambari-metrics-collector'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,392 - Changing group for /var/run/ambari-metrics-collector from 0 to hadoop
2018-01-11 13:03:41,393 - File['/usr/lib/ams-hbase/bin/hadoop'] {'owner': 'ams', 'mode': 0755}
2018-01-11 13:03:41,393 - Writing File['/usr/lib/ams-hbase/bin/hadoop'] because it doesn't exist
2018-01-11 13:03:41,394 - Changing owner for /usr/lib/ams-hbase/bin/hadoop from 0 to ams
2018-01-11 13:03:41,394 - Changing permission for /usr/lib/ams-hbase/bin/hadoop from 644 to 755
2018-01-11 13:03:41,394 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}
2018-01-11 13:03:41,397 - File['/etc/security/limits.d/ams.conf'] {'content': Template('ams.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}
2018-01-11 13:03:41,397 - Writing File['/etc/security/limits.d/ams.conf'] because it doesn't exist
2018-01-11 13:03:41,398 - Execute['/usr/lib/ams-hbase/bin/hbase-daemon.sh --config /etc/ams-hbase/conf stop regionserver'] {'on_timeout': 'ls /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid >/dev/null 2>&1 && ps `cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid` >/dev/null 2>&1 && ambari-sudo.sh -H -E kill -9 `ambari-sudo.sh cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid`', 'timeout': 30, 'user': 'ams'}
2018-01-11 13:03:41,448 - File['/var/run/ambari-metrics-collector//hbase-ams-regionserver.pid'] {'action': ['delete']}
2018-01-11 13:03:41,449 - Execute['/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf stop'] {'user': 'ams'}
2018-01-11 13:03:41,505 - Execute['ambari-sudo.sh rm -rf /var/lib/ambari-metrics-collector/hbase-tmp/*.tmp'] {}
2018-01-11 13:03:41,511 - File['/etc/ambari-metrics-collector/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,511 - File['/etc/ambari-metrics-collector/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,512 - Execute['/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf start'] {'user': 'ams'}
2018-01-11 13:08:55,668 - Skipping stack-select on AMBARI_METRICS because it does not exist in the stack-select package structure.

Command failed after 1 tries
{noformat}"
NameNode should use loginUser(hdfs) to serve iNotify requests,HDFS-10799,namenode,,,,"When a NameNode serves iNotify requests from a client, it verifies the client has superuser permission and then uses the client's Kerberos principal to read edits from journal nodes.

However, if the client does not renew its tgt tickets, the connection from NameNode to journal nodes may fail. In which case, the NameNode thinks the edits are corrupt, and prints a scary error message:
""During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 11577603, but we thought we could read up to transaction 11577606.  If you continue, metadata will be lost forever!""

However, the edits are actually good. NameNode _should not freak out when an iNotify client's tgt ticket expires_.

I think that an easy solution to this bug, is that after NameNode verifies client has superuser permission, call {{SecurityUtil.doAsLoginUser}} and then read edits. This will make sure the operation does not fail due to an expired client ticket.

Excerpt of related logs:
{noformat}
2016-08-18 19:05:13,979 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs@EXAMPLE.COM (auth:KERBEROS) cause:java.io.IOException: We encountered an error reading http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy, http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 11577603, but we thought we could read up to transaction 11577606.  If you continue, metadata will be lost forever!
2016-08-18 19:05:13,979 INFO org.apache.hadoop.ipc.Server: IPC Server handler 112 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getEditsFromTxid from [client IP:port] Call#73 Retry#0
java.io.IOException: We encountered an error reading http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy, http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 11577603, but we thought we could read up to transaction 11577606.  If you continue, metadata will be lost forever!
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1674)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1736)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1010)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1475)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
{noformat}"
DFS.setReplication should throw exception on EC files,HDFS-12921,erasure-coding,,,,"This was checked from {{o.a.h.fs.shell.SetReplication#processPath}}, however, {{DistributedFileSystem#setReplication()}} API is also a public API, we should move the check to {{DistributedFileSystem}} to prevent directly call this API on EC file."
Asserts are disabled in unit tests,HDFS-4411,build,,,,"Unlike 23, asserts are disabled for tests."
Backport HTTPFS to Branch 1,HDFS-4262,datanode,,,,"There are interests to backport HTTPFS back to Hadoop 1 branch.  After the initial investigation, there're quite some changes in HDFS-2178, and several related patches, including:

HDFS-2284 Write Http access to HDFS
HDFS-2646 Hadoop HttpFS introduced 4 findbug warnings
HDFS-2649 eclipse:eclipse build fails for hadoop-hdfs-httpfs
HDFS-2657 TestHttpFSServer and TestServerWebApp are failing on trunk
HDFS-2658 HttpFS introduced 70 javadoc warnings

The most challenge of backporting is all these patches, including HDFS-2178 are for 2.X, which  code base has been refactored a lot and quite different from 1.X, so it seems we have to backport the changes manually."
fix test TestSecureNameNode and improve test TestSecureNameNodeWithExternalKdc,HDFS-4312,,,,,"TestSecureNameNode does not work on Java6 without ""dfs.web.authentication.kerberos.principal"" config property set.

Also the following improved:
1) keytab files are checked for existence and readability to provide fast-fail on config error.
2) added comment to TestSecureNameNode describing the required sys props.
3) string literals replaced with config constants."
replica.getGenerationStamp() may be >= recoveryId,HDFS-5012,,,,,"The following was first observed by [~jdcryans] in TestReplicationQueueFailover running against 2.0.5-alpha:
{code}
2013-07-16 17:14:33,340 ERROR [IPC Server handler 7 on 35081] security.UserGroupInformation(1481): PriviledgedActionException as:ec2-user (auth:SIMPLE) cause:java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
2013-07-16 17:14:33,341 WARN  [org.apache.hadoop.hdfs.server.datanode.DataNode$2@64a1fcba] datanode.DataNode(1894): Failed to obtain replica info for block (=BP-1477359609-10.197.55.49-1373994849464:blk_4297992342878601848_1041) from datanode (=127.0.0.1:47006)
java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
{code}"
Backport HDFS-3626 to branch-1 (Creating file with invalid path can corrupt edit log),HDFS-3821,namenode,,,,Per [Todd's comment|https://issues.apache.org/jira/browse/HDFS-3626?focusedCommentId=13413509&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13413509] this issue affects v1 as well though the problem isn't as obvious because the shell doesn't use the Path(URI) constructor. To test the server side Todd modified the touchz command to use new Path(new URI(src)) and was able to reproduce the issue.
Combine READ_ONLY_SHARED DatanodeStorages with the same ID,HDFS-9808,,,,,"In HDFS-5318, each datanode that can reach a (read only) block reports itself as a valid location for the block. While accurate, this increases (redundant) block report traffic and- without partitioning on the backend- may return an overwhelming number of replica locations for each block.

Instead, a DN could report only that the shared storage is reachable. The contents of the storage could be reported separately/synthetically to the block manager, which can collapse all instances into a single storage. A subset of locations- closest to the client, etc.- can be returned, rather than all possible locations."
[SPS]: Use smaller batches of BlockMovingInfo into the block storage movement command,HDFS-11125,datanode,namenode,,,"This is a follow-up task of HDFS-11068, where it sends all the blocks under a trackID over single heartbeat response(DNA_BLOCK_STORAGE_MOVEMENT command). If blocks are many under a given trackID(For example: a file contains many blocks) then those requests go across a network and come with a lot of overhead. In this jira, we will discuss and implement a mechanism to limit the list of items into smaller batches with in trackID."
Port HDFS-4721 'Speed up lease/block recovery when DN fails and a block goes into recovery' to branch 1,HDFS-4796,,,,,"This was observed while doing HBase WAL recovery. HBase uses append to write to its write ahead log. So initially the pipeline is setup as

DN1 --> DN2 --> DN3

This WAL needs to be read when DN1 fails since it houses the HBase regionserver for the WAL.

HBase first recovers the lease on the WAL file. During recovery, we choose DN1 as the primary DN to do the recovery even though DN1 has failed and is not heartbeating any more.

To speedup lease/block recovery, we always choose the datanode with the most recent heartbeat."
TestAuditLogs#testAuditAllowedStat sometimes fails in trunk,HDFS-5831,,,,,"Running TestAuditLogs on Linux, I got:
{code}
testAuditAllowedStat[1](org.apache.hadoop.hdfs.server.namenode.TestAuditLogs)  Time elapsed: 6.677 sec  <<< FAILURE!
java.lang.AssertionError: null
        at org.junit.Assert.fail(Assert.java:92)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertNotNull(Assert.java:526)
        at org.junit.Assert.assertNotNull(Assert.java:537)
        at org.apache.hadoop.hdfs.server.namenode.TestAuditLogs.verifyAuditLogsRepeat(TestAuditLogs.java:312)
        at org.apache.hadoop.hdfs.server.namenode.TestAuditLogs.verifyAuditLogs(TestAuditLogs.java:295)
        at org.apache.hadoop.hdfs.server.namenode.TestAuditLogs.testAuditAllowedStat(TestAuditLogs.java:163)
{code}"
Ozone: More detailed documentation about the ozone components,HDFS-12464,HDFS-7240,,,,"I started to write a more detailed introduction about the Ozone components. The goal is to explain the basic responsibility of the components and the basic network topology (which components sends messages and to where?). 

 "
FSPermissionChecker.checkTraverse doesn't pass FsAction access properly,HDFS-11924,security,,,,"In 2.7.1, during file access check, the AccessControlEnforcer is called with the access parameter filled with FsAction values.

A thread dump in this case:
{code}
	FSPermissionChecker.checkPermission(INodesInPath, boolean, FsAction, FsAction, FsAction, FsAction, boolean) line: 189	
	FSDirectory.checkPermission(FSPermissionChecker, INodesInPath, boolean, FsAction, FsAction, FsAction, FsAction, boolean) line: 1698	
	FSDirectory.checkPermission(FSPermissionChecker, INodesInPath, boolean, FsAction, FsAction, FsAction, FsAction) line: 1682	
	FSDirectory.checkPathAccess(FSPermissionChecker, INodesInPath, FsAction) line: 1656	
	FSNamesystem.appendFileInternal(FSPermissionChecker, INodesInPath, String, String, boolean, boolean) line: 2668	
	FSNamesystem.appendFileInt(String, String, String, boolean, boolean) line: 2985	
	FSNamesystem.appendFile(String, String, String, EnumSet<CreateFlag>, boolean) line: 2952	
	NameNodeRpcServer.append(String, String, EnumSetWritable<CreateFlag>) line: 653	
	ClientNamenodeProtocolServerSideTranslatorPB.append(RpcController, ClientNamenodeProtocolProtos$AppendRequestProto) line: 421	
	ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message) line: not available	
	ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(RPC$Server, String, Writable, long) line: 616	
	ProtobufRpcEngine$Server(RPC$Server).call(RPC$RpcKind, String, Writable, long) line: 969	
	Server$Handler$1.run() line: 2049	
	Server$Handler$1.run() line: 2045	
	AccessController.doPrivileged(PrivilegedExceptionAction<T>, AccessControlContext) line: not available [native method]	
	Subject.doAs(Subject, PrivilegedExceptionAction<T>) line: 422	
	UserGroupInformation.doAs(PrivilegedExceptionAction<T>) line: 1657	

{code}

However, in 2.8.0 this value is changed to null, because in FSPermissionChecker.checkTraverse(FSPermissionChecker pc, INodesInPath iip, boolean resolveLink) couldn't pass the required information, so it's simply use 'null'.

This is a regression between 2.7.1 and 2.8.0, because external AccessControlEnforcer couldn't work properly"
Let distcp to bypass external attribute provider when calling getFileStatus etc at source cluster,HDFS-12294,hdfs,,,,"This is an alternative solution for HDFS-12202, which proposed introducing a new set of API, with an additional boolean parameter bypassExtAttrProvider, so to let NN bypass external attribute provider when getFileStatus. The goal is to avoid distcp from copying attributes from one cluster's external attribute provider and save to another cluster's fsimage.

The solution here is, instead of having an additional parameter, encode this parameter to the path itself, when calling getFileStatus (and some other calls), NN will parse the path, and figure out that whether external attribute provider need to be bypassed. The suggested encoding is to have a prefix to the path before calling getFileStatus, e.g. /ab/c becomes /.reserved/bypassExtAttr/a/b/c. NN will parse the path at the very beginning.

Thanks much to [~andrew.wang] for this suggestion. The scope of change is smaller and we don't have to change the FileSystem APIs.






 "
Add a field to FsServerDefaults to tell if external attribute provider is enabled,HDFS-12296,hdfs,,,,
NameNode to support file path prefix /.reserved/bypassExtAttr,HDFS-12295,hdfs,namenode,,,"Let NameNode to support prefix /.reserved/bypassExtAttr, so client can add thisprefix to a path before calling getFileStatus, e.g. /ab/c becomes /.reserved/bypassExtAttr/a/b/c. NN will parse the path at the very beginning, and bypass external attribute provider if the prefix is there.

"
Add MODIFY and REMOVE ECSchema editlog operations,HDFS-8295,,,,,"If MODIFY and REMOVE ECSchema operations are supported, then add these editlog operations to persist them. "
Move ErasureCodingPolicyManager to FSDirectory,HDFS-9604,,,,,"ErasureCodingPolicy is a part of directory metedata, it's better to put it in FSDirectory."
Erasure coding: updateBlockForPipeline sometimes returns non-striped block for striped file,HDFS-9386,erasure-coding,,,,"I've seen this bug a few times. The returned {{LocatedBlock}} from {{updateBlockForPipeline}} is sometimes not {{LocatedStripedBlock}}. However, {{FSNamesystem#bumpBlockGenerationStamp}} did return a {{LocatedStripedBlock}}. Maybe a bug in PB. I'm still debugging."
Provide new set of FileSystem API to bypass external attribute provider,HDFS-12202,hdfs,hdfs-client,,,"HDFS client uses 

{code}
  /**
   * Return a file status object that represents the path.
   * @param f The path we want information from
   * @return a FileStatus object
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation
   */
  public abstract FileStatus getFileStatus(Path f) throws IOException;

  /**
   * List the statuses of the files/directories in the given path if the path is
   * a directory.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   * <p>
   * Will not return null. Expect IOException upon access error.
   * @param f given path
   * @return the statuses of the files/directories in the given patch
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation
   */
  public abstract FileStatus[] listStatus(Path f) throws FileNotFoundException,
                                                         IOException;

{code}
to get FileStatus of files.

When external attribute provider (INodeAttributeProvider) is enabled for a cluster, the  external attribute provider is consulted to get back some relevant info (including ACL, group etc) and returned back in FileStatus, 

There is a problem here, when we use distcp to copy files from srcCluster to tgtCluster, if srcCluster has external attribute provider enabled, the data we copied would contain data from attribute provider, which we may not want.

Create this jira to add a new set of interface for distcp to use, so that distcp can copy HDFS data only and bypass external attribute provider data.

The new set API would look like
{code}
 /**
   * Return a file status object that represents the path.
   * @param f The path we want information from
   * @param bypassExtAttrProvider if true, bypass external attr provider
   *        when it's in use.
   * @return a FileStatus object
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation
   */
  public FileStatus getFileStatus(Path f,
      final boolean bypassExtAttrProvider) throws IOException;

  /**
   * List the statuses of the files/directories in the given path if the path is
   * a directory.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   * <p>
   * Will not return null. Expect IOException upon access error.
   * @param f
   * @param bypassExtAttrProvider if true, bypass external attr provider
   *        when it's in use.
   * @return
   * @throws FileNotFoundException
   * @throws IOException
   */
  public FileStatus[] listStatus(Path f,
      final boolean bypassExtAttrProvider) throws FileNotFoundException,
                                                  IOException;
{code}

So when bypassExtAttrProvider is true, external attribute provider will be bypassed.

Thanks.
"
testSetrepDecreasing UT fails due to timeout error,HDFS-11459,,,,,"{code}
Error Message

test timed out after 120000 milliseconds
Stacktrace

java.lang.Exception: test timed out after 120000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.fs.shell.SetReplication.waitForReplication(SetReplication.java:127)
	at org.apache.hadoop.fs.shell.SetReplication.processArguments(SetReplication.java:77)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:119)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:297)
	at org.apache.hadoop.hdfs.TestSetrepIncreasing.setrep(TestSetrepIncreasing.java:58)
	at org.apache.hadoop.hdfs.TestSetrepDecreasing.testSetrepDecreasing(TestSetrepDecreasing.java:27){code}"
NPE in Storage$StorageDirectory#unlock(),HDFS-9590,,,,,"The code looks to be possible to have race conditions in multiple-threaded runs.
{code}
    public void unlock() throws IOException {
      if (this.lock == null)
        return;
      this.lock.release();
      lock.channel().close();
      lock = null;
    }
{code}
This is called in a handful of places, and I don't see any protection. Shall we add some synchronization mechanism? Not sure if I missed any design assumptions here.
"
TestAclsEndToEnd#testCreateEncryptionZone failing very frequently.,HDFS-11944,encryption,test,,,"TestAclsEndToEnd#testCreateEncryptionZone is failing v frequently.
The way test is written makes very hard to debug.
Ideally each test case should test only one behavior.
But in this test case, it reset the dfs state many times in same test case.
It fails with the following stack trace.
{noformat}
Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 35.17 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestAclsEndToEnd
testCreateEncryptionZone(org.apache.hadoop.hdfs.TestAclsEndToEnd)  Time elapsed: 3.844 sec  <<< FAILURE!
java.lang.AssertionError: Allowed zone creation of zone with blacklisted GENERATE_EEK
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.apache.hadoop.hdfs.TestAclsEndToEnd.testCreateEncryptionZone(TestAclsEndToEnd.java:753)


Results :

Failed tests: 
  TestAclsEndToEnd.testCreateEncryptionZone:753 Allowed zone creation of zone with blacklisted GENERATE_EEK
{noformat}

It failed in the following pre-commits.
[HDFS-11885 precommit|https://issues.apache.org/jira/browse/HDFS-11885?focusedCommentId=16040117&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16040117]
[HDFS-11804 precommit|https://issues.apache.org/jira/browse/HDFS-11804?focusedCommentId=16039872&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16039872]"
fix spelling mistake in TestFsVolumeList.java ,HDFS-11812,,,,,We found a  spelling mistake in  TestFsVolumeList.java: // Mock reservedForReplcas should be // Mock reservedForReplicas銆?
Create a servlet for HDFS UI,HDFS-7239,,,,,"Currently the HDFS UI gathers most of its information from JMX. There are a couple disadvantages:

* JMX is also used by management tools, thus Hadoop needs to maintain compatibility across minor releases.
* JMX organizes information as <key, value> pairs. The organization does not fit well with emerging use cases like startup progress report and nntop.

This jira proposes to introduce a new servlet in the NN for the purpose of serving information to the UI.

It should be viewed as a part of the UI. There is *no* compatibility guarantees for the output of the servlet."
"Balancer.run() prints redundant included, excluded, source nodes.",HDFS-11731,balancer & mover,,,,"Included, excluded, and source nodes are printed twice by the Balancer. First as part of {{BalancerParameters.toString()}} in
{code}
    LOG.info(""parameters = "" + p);
{code}
And then separately
{code}
    LOG.info(""included nodes = "" + p.getIncludedNodes());
    LOG.info(""excluded nodes = "" + p.getExcludedNodes());
    LOG.info(""source nodes = "" + p.getSourceNodes());
{code}
The latter can be removed."
Ozone : implement StorageContainerManager#getStorageContainerLocations,HDFS-11872,ozone,,,,"We should implement {{StorageContainerManager#getStorageContainerLocations}} . 

Although the comment says it will be moved to KSM, the functionality of container lookup by name it should actually be part of SCM functionality."
"HDFS client with hedged read, handle exceptions from callable  when the hedged read thread pool is exhausted",HDFS-11819,,,,,"When the hedged read thread pool is exhausted, the current behavior is that callable will be executed in the current thread context. The callable can throw out IOExceptions which is not handled and it will not start a 'hedged' read. 

https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java#L1131

Please see the following exception:
{code}
2017-05-11 22:42:35,883 WARN org.apache.hadoop.hdfs.BlockReaderFactory: I/O error constructing remote block reader.
org.apache.hadoop.net.ConnectTimeoutException: 3000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/*.*.*.*:50010]
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3527)
        at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:840)
        at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:755)
        at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:376)
        at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1179)
        at org.apache.hadoop.hdfs.DFSInputStream.access$300(DFSInputStream.java:91)
        at org.apache.hadoop.hdfs.DFSInputStream$2.call(DFSInputStream.java:1141)
        at org.apache.hadoop.hdfs.DFSInputStream$2.call(DFSInputStream.java:1133)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor$CallerRunsPolicy.rejectedExecution(ThreadPoolExecutor.java:2022)
        at org.apache.hadoop.hdfs.DFSClient$2.rejectedExecution(DFSClient.java:3571)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
        at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:181)
        at org.apache.hadoop.hdfs.DFSInputStream.hedgedFetchBlockByteRange(DFSInputStream.java:1280)
        at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1477)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1439)
        at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
        at org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.read(FileLink.java:167)
        at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock.positionalReadWithExtra(HFileBlock.java:757)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1457)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockDataInternal(HFileBlock.java:1682)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockData(HFileBlock.java:1542)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:445)
        at org.apache.hadoop.hbase.util.CompoundBloomFilter.contains(CompoundBloomFilter.java:100)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesGeneralBloomFilter(StoreFile.java:1383)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesBloomFilter(StoreFile.java:1247)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.shouldUseScanner(StoreFileScanner.java:469)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.selectScannersFrom(StoreScanner.java:393)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.getScannersNoCompaction(StoreScanner.java:312)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:192)
        at org.apache.hadoop.hbase.regionserver.HStore.createScanner(HStore.java:2106)
        at org.apache.hadoop.hbase.regionserver.HStore.getScanner(HStore.java:2096)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:5544)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:2569)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:2555)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:2536)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:6791)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:6770)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2025)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:33644)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165)

{code}"
Thread safety in logEdit?,HDFS-11820,hdfs,,,,"Hi there,

I am new to Hadoop and trying to understand how things work under the hood by browsing through some of the codes.

I noticed a potential thread safety issue in in FSEditLog.java in version 2.7.1 where the following patterns is used (the current trunk also use the same pattern):
1. Instance of FSEditLogOp is retrieved from cache for reuse 
2. Set the attributes (e.g. path, timestamp, etc)
3. Invoke logEdit(*op*). This method has synchronized block in it, but also has *wait* if auto-sync is scheduled

Now, if I have two almost simultaneous rename operations, right after each is about to write edit log:
Thread #1 acquired instance of RenameOp, set the attributes, and invoked logEdit, then it waits because auto-sync is scheduled.
Thread #2 catches up, and acquires same instance of RenameOp, sets *different* attributes, and invokes logEdit.. It blocks because of synchronized block inside logEdit(...), but it manages to modify the attributes of RenameOp.

The second renameOp could end up being logged twice because both renameOps are actually the same instance. 
The fix is to have synchronized(*op*) prior to calling logEdit(*op*) or clone the op before using it.

I could be wrong. Am I missing something?

Thanks,

Alexander Koentjara"
Convert FSImage.removedStorageDirs into a map.,HDFS-361,,,,,{{FSImage.removedStorageDirs}} is declared as an {{ArrayList}}. In order to avoid adding the same directory twice into {{removedStorageDirs}} we should convert it into a map.
getTurnOffTip computes needed block incorrectly for threshold < 1 in b2.7,HDFS-10459,,,,,GetTurnOffTip overstates the number of blocks necessary to come out of safe mode by 1 due to an arbitrary '+1' in the code. 
[SPS]: fix issue of moving blocks with satisfier while changing replication factor ,HDFS-11284,datanode,namenode,,," When the real replication number of block doesn't match the replication factor. For example, the real replication is 2 while the replication factor is 3, the satisfier may encounter issue."
Add support for byte-ranges to hsftp,HDFS-594,hdfs-client,,,,HsftpFileSystem should be modified to support byte-ranges so it has the same semantics as HftpFileSystem after committing HDFS-235. 
Is there a way of loading cvs files to create hive tables with desired lengths for columns?,HDFS-11501,,,,,"We just got on Hadoop environment. Our data sources are cvs files. The hive tables created from the sources are seen all character /string columns have same length of 255 bytes, even for gender which has value with one byte. Is there a way of loading cvs files to create hive tables with desired lengths for string columns instead of 255 across all tables? Thank you for your help!"
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name,HDFS-5779,,,,,"xxxxxx@testhost:/home/xxxxxx/Lab/hdfs/namenodep$ hadoop namenode -format
Warning: $HADOOP_HOME is deprecated.

14/01/15 04:51:38 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = testhost.testhost1.net/xx.xxx.xxx.xxx
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.3
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1335192; compiled by 'hortonfo' on Tue May  8 20:31:25 UTC 2012
************************************************************/
Re-format filesystem in /home/xxxxxx/Lab/hdfs/namenodep ? (Y or N) Y
14/01/15 04:51:40 INFO util.GSet: VM type       = 32-bit
14/01/15 04:51:40 INFO util.GSet: 2% max memory = 19.33375 MB
14/01/15 04:51:40 INFO util.GSet: capacity      = 2^22 = 4194304 entries
14/01/15 04:51:40 INFO util.GSet: recommended=4194304, actual=4194304
14/01/15 04:51:40 ERROR namenode.NameNode: java.io.IOException: failure to login
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:490)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:452)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setConfigurationParameters(FSNamesystem.java:475)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:464)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1162)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1271)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
        at com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at javax.security.auth.login.LoginContext.invoke(Unknown Source)
        at javax.security.auth.login.LoginContext.access$000(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokeCreatorPriv(Unknown Source)
        at javax.security.auth.login.LoginContext.login(Unknown Source)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:471)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:452)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setConfigurationParameters(FSNamesystem.java:475)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:464)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1162)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1271)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)

        at javax.security.auth.login.LoginContext.invoke(Unknown Source)
        at javax.security.auth.login.LoginContext.access$000(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokeCreatorPriv(Unknown Source)
        at javax.security.auth.login.LoginContext.login(Unknown Source)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:471)
        ... 6 more

14/01/15 04:51:40 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at testhost.testhost1.net/10.129.254.129
************************************************************/
"
"AlreadyBeingCreatedException ""current leaseholder is trying to recreate file"" when trying to append to file",HDFS-11367,hdfs-client,,,,"We have code which creates a file in HDFS and continuously appends lines to the file, then closes the file at the end. This is done by a single dedicated thread.

We specifically instrumented the code to make sure only one 'client'/thread ever writes to the file because we were seeing ""current leaseholder is trying to recreate file"" errors.

For some background see this for example: https://community.cloudera.com/t5/Storage-Random-Access-HDFS/How-to-append-files-to-HDFS-with-Java-quot-current-leaseholder/m-p/41369

This issue is very critical to us as any error terminates a mission critical application in production.

Intermittently, we see the below exception, regardless of what our code is doing which is create the file, keep appending, then close:

org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to create file /data/records_20170125_1.txt for DFSClient_NONMAPREDUCE_-167421175_1 for client 1XX.2XX.1XX.XXX because current leaseholder is trying to recreate file.
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3075)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2905)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:3189)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:3153)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:612)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.append(AuthorizationProviderProxyClientProtocol.java:125)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:414)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
聽聽聽聽聽聽聽 at java.security.AccessController.doPrivileged(Native Method)
聽聽聽聽聽聽聽 at javax.security.auth.Subject.doAs(Subject.java:415)
聽聽聽聽聽聽聽 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1767)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
聽
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Client.call(Client.java:1411)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Client.call(Client.java:1364)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
聽聽聽聽聽聽聽 at com.sun.proxy.$Proxy24.append(Unknown Source)
聽聽聽聽聽聽聽 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
聽聽聽聽聽聽聽 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
聽聽聽聽聽聽聽 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
聽聽聽聽聽聽聽 at java.lang.reflect.Method.invoke(Method.java:483)
聽聽聽聽聽聽聽 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
聽聽聽聽聽聽聽 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
聽聽聽聽聽聽聽 at com.sun.proxy.$Proxy24.append(Unknown Source)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:282)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1586)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1626)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1614)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:313)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:309)
聽聽聽聽聽聽聽 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:309)
聽聽聽聽聽聽聽 at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)
聽聽聽聽聽聽聽 at com.myco.MyAppender.getOutputStream(MyAppender.java:147)"
Erasure coding: merge HDFS-8499 to EC branch and refactor BlockInfoStriped,HDFS-8796,namenode,,,,"Separating this change from the HDFS-8728 discussion. Per suggestion from [~szetszwo], clarifying the description of the change."
hadoop-7285-power,HDFS-11297,erasure-coding,,,,hadoop-7285-power
Patch for PPC64,HDFS-8519,,,,,"The attached patch enables Hadoop to work on PPC64.
That deals with SystemPageSize and BloclSize , which are not 4096 on PPC64.

There are changes in 3 files:
- hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java

where 4096 is replaced by getOperatingSystemPageSize() or by using PAGE_SIZE

The patch has been built on branch-2.7 ."
Patch for PPC64,HDFS-8518,,,,,"The attached patch enables Hadoop to work on PPC64.
That deals with SystemPageSize and BloclSize , which are not 4096 on PPC64.

There are changes in 3 files:
- hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java

where 4096 is replaced by getOperatingSystemPageSize() or by using PAGE_SIZE

The patch has been built on branch-2.7 ."
HDFS ignores HADOOP_CONF_DIR,HDFS-11245,hdfs,,,,"It seems that HDFS on trunk is ignoring {{HADOOP_CONF_DIR}}. On {{branch-2}} I could export {{HADOOP_CONF_DIR}} and use that to store my {{hdfs-site.xml}} and {{log4j.properties}}. But on trunk it appears to ignore the environment variable.

Also, even if hdfs can find the {{log4j.properties}}, it doesn't seem interested in opening and loading it.

On Ubuntu 16.10:

{code}
$ source env.sh
$ cat env.sh 
#!/bin/bash
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HOME=""$HOME""/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT
export HADOOP_LOG_DIR=""$(pwd)/log""
PATH=""$HADOOP_HOME""/bin:$PATH
export HADOOP_CLASSPATH=$(hadoop classpath):""$HADOOP_HOME""/share/hadoop/tools/lib/*
export HADOOP_USER_CLASSPATH_FIRST=true
{code}


Then I set the HADOOP_CONF_DIR:
{code}
$ export HADOOP_CONF_DIR=""$(pwd)/conf/nn""
$ ls $HADOOP_CONF_DIR
hadoop-env.sh  hdfs-site.xml  log4j.properties
{code}

Now, we try to run a namenode:
{code}
$ hdfs namenode
2016-12-14 14:04:51,193 ERROR [main] namenode.NameNode: Failed to start namenode.
java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.
        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:648)
        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddressCheckLogical(DFSUtilClient.java:677)
        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:639)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:556)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:687)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:707)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:916)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1633)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1701)
{code}

This is weird. We have the {{fs.defaultFS}} set:

{code}
$ grep -n2 fs.defaultFS $HADOOP_CONF_DIR/hdfs-site.xml
3-<configuration>
4-    <property>
5:        <name>fs.defaultFS</name>
6-        <value>hdfs://localhost:60010</value>
7-    </property>
{code}

So if isn't finding this config. Where is is looking and finding {{file:///}}?
{code}
$ strace -f -eopen,stat hdfs namenode 2>&1 | grep hdfs-site.xml
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
{code}

So it's ignoring {{HADOOP_CONF_DIR}}. We can work around it using {{-conf $(pwd)/conf/nn/hdfs-site.xml}}:
{code}
$ strace -f -eopen,stat hdfs namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep hdfs-site.xml
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16493] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
[pid 16493] stat(""/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/hdfs-site.xml"", {st_mode=S_IFREG|0644, st_size=2107, ...}) = 0
[pid 16493] open(""/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/hdfs-site.xml"", O_RDONLY) = 218

------8<------
{code}

Great! However, it's not finding my  log4j.properties for some reason. This is annoying because hdfs isn't printing anything or logging anywhere. Where is it looking?
{code}
$ strace -f hdfs namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep log4j.properties
stat(""/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/log4j.properties"", {st_mode=S_IFREG|0644, st_size=13641, ...}) = 0
{code}

It found it, but it only statted it. It never opened it! So it seems there's at least one bug here where {{log4j.properties}} is being ignored. But shouldn't {{HADOOP_OPTS}} be set and configuring it to print to the console and to my log dir?

{code}
$ hdfs --debug namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep HADOOP_OPTS
DEBUG: HADOOP_OPTS accepted -Dhdfs.audit.logger=INFO,NullAppender
DEBUG: Appending HDFS_NAMENODE_OPTS onto HADOOP_OPTS
DEBUG: HADOOP_OPTS accepted -Dyarn.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log
DEBUG: HADOOP_OPTS accepted -Dyarn.log.file=hadoop.log
DEBUG: HADOOP_OPTS accepted -Dyarn.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT
DEBUG: HADOOP_OPTS accepted -Dyarn.root.logger=INFO,console
DEBUG: HADOOP_OPTS accepted -Djava.library.path=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/lib/native
DEBUG: HADOOP_OPTS accepted -Dhadoop.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log
DEBUG: HADOOP_OPTS accepted -Dhadoop.log.file=hadoop.log
DEBUG: HADOOP_OPTS accepted -Dhadoop.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT
DEBUG: HADOOP_OPTS accepted -Dhadoop.id.str=ehigg90120
DEBUG: HADOOP_OPTS accepted -Dhadoop.root.logger=INFO,console
DEBUG: HADOOP_OPTS accepted -Dhadoop.policy.file=hadoop-policy.xml
DEBUG: HADOOP_OPTS declined -Dhadoop.security.logger=INFO,NullAppender
DEBUG: Final HADOOP_OPTS: -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dyarn.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT -Dyarn.root.logger=INFO,console -Djava.library.path=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/lib/native -Dhadoop.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT -Dhadoop.id.str=ehigg90120 -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml
{code}

So it seems it is being configured and passed to the namenode. It's just not obeying it as far as I can see. 

So maybe there are two possibly related bugs:

1. {{HADOOP_CONF_DIR}} is ignored
2. The logger is not using {{log4j.properties}} or the command line. I would expect it to use the {{log4j.properties}} in the {{HADOOP_CONF_DIR}}.

I feel like I must be misunderstanding something since this seems like a pretty big issue but I didn't find any open tickets about it or any tickets describing a new way of configuring clusters."
dfshealth_nonsecure.html should not be accessble in secure cluster.,HDFS-7958,namenode,,,,"In secure environment (kerberous + https)
the following url should not be accessible.
{code}
https://nn1:25003/dfshealth_nonsecure.html#tab-overview
{code}"
LightWeightHashSet can't remove blocks correctly which have a large number blockId,HDFS-11179,namenode,,,,"Our test cluster has faced a problem that {{postponedMisreplicatedBlocksCount}} has been going below zero. The version of the cluster is a recent 3.0. We haven't created any EC files yet. This is the NN's log:

{noformat}
Rescan of postponedMisreplicatedBlocks completed in 13 msecs. 448 blocks are left. 176 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 13 msecs. 272 blocks are left. 176 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 14 msecs. 96 blocks are left. 176 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 327 msecs. -77 blocks are left. 177 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 15 msecs. -253 blocks are left. 179 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 14 msecs. -432 blocks are left. 179 blocks are removed.
{noformat}

I looked into this issue and found that it is caused by {{LightWeightHashSet}} which is used for {{postponedMisreplicatedBlocks}} recently. When {{LightWeightHashSet}} remove blocks which have a large number blockId, overflows happen and the blocks can't be removed correctly(, let alone ec blocks whose blockId starts with the minimum of long)."
HDFS doesn't raise FileNotFoundException if the source of a rename() is missing,HDFS-6262,namenode,,,,"HDFS's {{rename(src, dest)}} returns false if src does not exist -all the other filesystems raise {{FileNotFoundException}}

This behaviour is defined in {{FSDirectory.unprotectedRenameTo()}} -the attempt is logged, but the operation then just returns false.

I propose changing the behaviour of {{DistributedFileSystem}} to be the same as that of the others -and of {{FileContext}}, which does reject renames with nonexistent sources"
DN decommissioning quirks,HDFS-2569,datanode,,,,"Decommissioning a node is working slightly odd in 0.23+:

The steps I did:

- Start HDFS via {{hdfs namenode}} and {{hdfs datanode}}. 1-node cluster.
- Zero files/blocks, so I go ahead and exclude-add my DN and do {{hdfs dfsadmin -refreshNodes}}
- I see the following log in NN tails, which is fine:
{code}
11/11/20 09:28:10 INFO util.HostsFileReader: Setting the includes file to 
11/11/20 09:28:10 INFO util.HostsFileReader: Setting the excludes file to build/test/excludes
11/11/20 09:28:10 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
11/11/20 09:28:10 INFO util.HostsFileReader: Adding 192.168.1.23 to the list of hosts from build/test/excludes
{code}
- However, DN log tail gets no new messages. DN still runs.
- The dfshealth.jsp page shows this table, which makes no sense -- why is there 1 live and 1 dead?:

|Live Nodes|1 (Decommissioned: 1)|
|Dead Nodes|1 (Decommissioned: 0)|
|Decommissioning Nodes|0|

- The live nodes page shows this, meaning DN is still up and heartbeating but is decommissioned:

|Node|Last Contact|Admin State|
|192.168.1.23|0|Decommissioned|

- The dead nodes page shows this, and the link to the DN is broken cause the port is linked as -1. Also, showing 'false' for decommissioned makes no sense when live node page shows that it is already decommissioned:

|Node|Decommissioned|
|192.168.1.23|false|

Investigating if this is a quirk only observed when the DN had 0 blocks on it in sum total."
Reschedule CompletedActionXCommand if the job is not completed,HDFS-11173,,,,,"We've encountered cases when the LauncherMapper stuck around after sending out the notifications to Oozie. If the callback is processed before the external job's status is updated to FINISHED, Oozie won't update the action's status for 10 minutes.

We could add a delayed check to [CompletedAcitonXCommand|https://github.com/apache/oozie/blob/master/core/src/main/java/org/apache/oozie/command/wf/CompletedActionXCommand.java#120 ] to avoid this.
"
viewfs shows resolved path in FileNotFoundException,HDFS-5692,hdfs-client,,,,"With the following config, if I call fs.listStatus(""/nn1/a/b"") when {{/nn1/a/b}} does not exist then ...

{noformat}
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>viewfs:///</value>
  </property>
  <property>
    <name>fs.viewfs.mounttable.default.link./nn1</name>
    <value>hdfs://host1:9000</value>
  </property>
  <property>
    <name>fs.viewfs.mounttable.default.link./nn2</name>
    <value>hdfs://host2:9000</value>
  </property>
</configuration>
{noformat}

I will see an error message like the following.  

{noformat}
java.io.FileNotFoundException: File /a/b does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:644)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:92)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:702)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:222)
        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:228)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:366)
{noformat}

I think it would be useful for ViewFS to wrap the FileNotFoundException from the inner filesystem, giving an error message like the following.  The following error message has the resolved and unresolved paths which is very useful for debugging.

{noformat}
java.io.FileNotFoundException: File /nn1/a/b does not exist.
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:366)
Caused by: java.io.FileNotFoundException: File /a/b does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:644)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:92)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:702)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:222)
        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:228)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:366)
{noformat}"
Refactor NN WebUI to no longer pass IP addresses in the URL,HDFS-4483,,,,,"Right now, the namenode passes its RPC address in WebUI URLs when it redirects to datanodes for things like browsing the filesystem. This is brittle and fails in different ways when wildcard addresses are configured (see HDFS-3932 and HDFS-4471).

A better solution would be to instead pass the NN's nameservice ID in the URL, and make DNs look up the appropriate RPC address for the nameservice from their conf. This fixes the wildcard issues and has the additional benefit of making browsing work after a NN failover."
Add unit tests to verify ACLs in safemode,HDFS-10950,fs,test,,,"This proposes adding unit tests to validate that getting Acls works when namende is in safemode, while setting Acls fails. Specifically, the following needs being covered in newly added tests.
test_getfacl_recursive
test_resetacl
test_setfacl_default"
dfsadmin set/clrSpaceQuota fail to recognize StorageType option,HDFS-11017,fs,,,,"dfsadmin setSpaceQuota or clrSpaceQuota don't recognize valid StorageType options, such as DISK or SSD, however, It's been supported by DFS."
multiple BlockFixer should be supported in order to improve scalability and reduce too much work on single BlockFixer,HDFS-4360,contrib/raid,,,,"current implementation can only run single BlockFixer since the fsck (in RaidDFSUtil.getCorruptFiles) only check the whole DFS file system. multiple BlockFixer will do the same thing and try to fix same file if multiple BlockFixer launched. 

the change/fix will be mainly in BlockFixer.java and RaidDFSUtil.getCorruptFile(), to enable fsck to check the different paths defined in separated Raid.xml for single RaidNode/BlockFixer"
TestRaidNode is failing,HDFS-3554,contrib/raid,test,,,"After MAPREDUCE-3868 re-enabled raid, TestRaidNode has been failing in Jenkins builds."
Consider remaining space during block blockplacement if dfs space is highly utilized,HDFS-8041,,,,,"This feature is helpful in avoiding smaller nodes (i.e. heterogeneous environment) getting constantly being full when the overall space utilization is over a certain threshold.  When the utilization is low, balancer can keep up, but once the average per-node byte goes over the capacity of the smaller nodes, they get full so quickly even after perfect balance.

This jira proposes an improvement that can be optionally enabled in order to slow down the rate of space usage growth of smaller nodes if the overall storage utilization is over a configured threshold.  It will not replace balancer, rather will help balancer keep up. Also, the primary replica placement will not be affected. Only the replicas typically placed in a remote rack will be subject to this check.

The appropriate threshold is cluster configuration specific. There is no generally good value to set, thus it is disabled by default. We have seen cases where the threshold of 85% - 90% would help. Figuring when {{totalSpaceUsed / numNodes}} becomes close to the capacity of a smaller node is helpful in determining the threshold."
Ability to use SimpleRegeratingCode to fix missing blocks,HDFS-3544,contrib/raid,,,,"ReedSolomon encoding (n, k) has n storage nodes and can tolerate n-k failures. Regenerating a block needs to access k blocks. This is a problem when n and k are large. Instead, we can use simple regenerating codes (n, k, f) that does first does ReedSolomon (n,k) and then does XOR with f stripe size. Then, a single disk failure needs to access only f nodes and f can be very small."
Support for pluggable erasure coding policy for HDFS,HDFS-600,contrib/raid,,,,"HDFS-503 introduces erasure coding for HDFS files. It currently uses ""xor"" algoritm as the Erasure coding algorithm. It would be nice if that Erasure Coding framework supports a pluggable API to allow plugging in other Erasure Coding policies.  A few of these policies are mentioned by Hong at https://issues.apache.org/jira/browse/HDFS-503?focusedCommentId=12735011&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12735011"
Handle disconnect and session timeout events at BKJM,HDFS-3562,,,,,"# Retry zookeeper operations for some amount of time in case of CONNECTIONLOSS/OPERATIONTIMEOUT exceptions.
# In case of Session expiry trigger shutdown"
"In HA mode, when there is a ledger in BK missing, which is generated after the last checkpoint, NN can not restore it.",HDFS-3908,namenode,,,,"If not HA, when the num of edits.dir is larger than 1. Missing of one editlog file in a dir will not relust problem cause of the replica in the other dir. 
However, when in HA mode(using BK as ShareStorage), if an ledger missing, the missing ledger will not restored at the phase of NN starting even if the related editlog file existing in local dir.
The missing maintains when NN is still in standby state. However, when the NN enters active state, it will read the editlog file(related to the missing ledger) in local. But, unfortunately, the ledger after the missing one in BK can't be readed at such a phase(cause of gap).
Therefore in the following situation, editlogs will not be restored even there is an editlog file either in BK or in local dir: 

In such a stituation, editlog can't be restored:
1銆乫siamge file: fsimage_0000000000000005946.md5
2銆乴egder in zk:
	\[zk: localhost:2181(CONNECTED) 0\] ls /hdfsEdit/ledgers/edits_00000000000000594
	edits_000000000000005941_000000000000005942
	edits_000000000000005943_000000000000005944
	edits_000000000000005945_000000000000005946
	edits_000000000000005949_000000000000005949   
锛坢issing edits_000000000000005947_000000000000005948锛?3銆乪ditlog in local editlog dir锛?	\-rw-r--r-- 1 root root      30 Sep  8 03:24 edits_0000000000000005947-0000000000000005948
	\-rw-r--r-- 1 root root 1048576 Sep  8 03:35 edits_0000000000000005950-0000000000000005950
	\-rw-r--r-- 1 root root 1048576 Sep  8 04:42 edits_0000000000000005951-0000000000000005951
	锛坢iss edits_0000000000000005949-0000000000000005919锛?4銆乤nd the seen_txid
	vm2:/tmp/hadoop-root/dfs/name/current # cat seen_txid
	5949

Here, we want to restored editlog from txid 5946(image) to txid 5949(seen_txid). The 5947-5948 is missing in BK, 5949-5949 is missing in local dir.
When start the NN, the following exception is thrown:

2012-09-08 06:26:10,031 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Error encountered requiring NN shutdown. Shutting down immediately.
java.io.IOException: There appears to be a gap in the edit log.  We expected txid 5949, but got txid 5950.
        at org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt(MetaRecoveryContext.java:94)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:163)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:692)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:223)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:182)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:599)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1325)
        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)
        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1233)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:990)
        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:924)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)
2012-09-08 06:26:10,036 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at vm2/160.161.0.155
************************************************************/"
WebHdfsFileSystem#toUrl does not perform character escaping for rename ,HDFS-6277,,,,,"Found this issue while testing HDFS-6141. WebHdfsFileSystem#toUrl  does not perform character escaping for rename and causes the operation to fail. 
This bug does not exist on 2.x

For e.g: 
$ hadoop dfs -rmr 'webhdfs://<namenode>:<port>/tmp/test dirname with spaces'
Problem with Trash.Unexpected HTTP response: code=400 != 200, op=RENAME, message=Bad Request. Consider using -skipTrash option
rmr: Failed to move to trash: webhdfs://<namenode>:<port>/tmp/test dirname with spaces"
DataNode should report&remove volume failures if DU cannot access files,HDFS-10777,datanode,,,,"HADOOP-12973 refactored DU and makes it pluggable. The refactory has a side-effect that if DU encounters an exception, the exception is caught, logged and ignored, essentially fixes HDFS-9908 (in which case runaway exceptions prevent DataNodes from handshaking with NameNodes).

However, this ""fix"" is not good, in the sense that if the disk is bad, there is no immediate action made by the DataNode other than logging the exception. Existing {{FsDatasetSpi#checkDataDir}} has been reduced to only check a few number of directories blindly. If a disk goes bad, it is often possible that only a few files are bad initially and that by checking only a small number of directories it is easy to overlook the degraded disk.

I propose: in addition to logging the exception, DataNode should proactively verify the files are not accessible, remove the volume, and make the failure visible by showing it in JMX, so that administrators can spot the failure via monitoring systems.

A different fix, based on HDFS-9908, is needed before Hadoop 2.8.0"
Fix TestWebHdfsProxySelector in branch-2.8,HDFS-10853,,,,,"Similar to HDFS-8948, we need to use GenericTestUtils to set log levels to avoid multiple binding errors."
libhdfs++:  Expose an InputStream interface for the apache ORC project,HDFS-10708,hdfs-client,,,,It seems fitting to connect a pure c++ implementation of the HDFS client to a pure c++ implementation of a parser for the ORC file format.  Implementing the orc::InputStream API is pretty straightforward.
webhdfs fails with filenames including semicolons,HDFS-10574,webhdfs,,,,"Via webhdfs or native HDFS, we can create files with semicolons in their names:

{code}
bhansen@::1 /tmp$ hdfs dfs -copyFromLocal /tmp/data ""webhdfs://localhost:50070/foo;bar""
bhansen@::1 /tmp$ hadoop fs -ls /
Found 1 items
-rw-r--r--   2 bhansen supergroup          9 2016-06-24 12:20 /foo;bar
{code}

Attempting to fetch the file via webhdfs fails:
{code}
bhansen@::1 /tmp$ curl -L ""http://localhost:50070/webhdfs/v1/foo%3Bbar?user.name=bhansen&op=OPEN""
{""RemoteException"":{""exception"":""FileNotFoundException"",""javaClassName"":""java.io.FileNotFoundException"",""message"":""File does not exist: /foo\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n""}}
{code}

It appears (from the attached TCP dump in curl_request.txt) that the namenode's redirect unescapes the semicolon, and the DataNode's HTTP server is splitting the request at the semicolon, and failing to find the file ""foo"".



Interesting side notes:
* In the attached dfs_copyfrom_local_traffic.txt, you can see the copyFromLocal command writing the data to ""foo;bar_COPYING_"", which is then redirected and just writes to ""foo"".  The subsequent rename attempts to rename ""foo;bar_COPYING_"" to ""foo;bar"", but has the same parsing bug so effectively renames ""foo"" to ""foo;bar"".

Here is the full range of special characters that we initially started with that led to the minimal reproducer above:
{code}
hdfs dfs -copyFromLocal /tmp/data webhdfs://localhost:50070/'~`!@#$%^& ()-_=+|<.>]}"",\\\[\{\*\?\;'\''data'
curl -L ""http://localhost:50070/webhdfs/v1/%7E%60%21%40%23%24%25%5E%26+%28%29-_%3D%2B%7C%3C.%3E%5D%7D%22%2C%5C%5B%7B*%3F%3B%27data?user.name=bhansen&op=OPEN&offset=0""
{code}

Thanks to [~anatoli.shein] for making a concise reproducer.
"
libhdfs++: Add additional type-safe getters to the Configuration class,HDFS-9632,hdfs-client,,,,"Notably, URIs and byte sizes are missing"
Datanode should tolerate disk scan failure during NN handshake,HDFS-9908,datanode,,,,"DN may treat a disk scan failure exception as an NN handshake exception, and this can prevent a DN to join a cluster even if most of its disks are healthy.

During NN handshake, DN initializes block pools. It will create a lock files per disk, and then scan the volumes. However, if the scanning throws exceptions due to disk failure, DN will think it's an exception because NN is inconsistent with the local storage (see {{DataNode#initBlockPool}}. As a result, it will attempt to reconnect to NN again.

However, at this point, DN has not deleted its lock files on the disks. If it reconnects to NN again, it will think the same disks are already being used, and then it will fail handshake again because all disks can not be used (due to locking), and repeatedly. This will happen even if the DN has multiple disks, and only one of them fails. The DN will not be able to connect to NN despite just one failing disk. Note that it is possible to successfully create a lock file on a disk, and then has error scanning the disk.

We saw this on a CDH 5.3.3 cluster (which is based on Apache Hadoop 2.5.0, and we still see the same bug in 3.0.0 trunk branch). The root cause is that DN treats an internal error (single disk failure) as an external one (NN handshake failure) and we should fix it.

{code:title=DataNode.java}
/**
   * One of the Block Pools has successfully connected to its NN.
   * This initializes the local storage for that block pool,
   * checks consistency of the NN's cluster ID, etc.
   * 
   * If this is the first block pool to register, this also initializes
   * the datanode-scoped storage.
   * 
   * @param bpos Block pool offer service
   * @throws IOException if the NN is inconsistent with the local storage.
   */
  void initBlockPool(BPOfferService bpos) throws IOException {
    NamespaceInfo nsInfo = bpos.getNamespaceInfo();
    if (nsInfo == null) {
      throw new IOException(""NamespaceInfo not found: Block pool "" + bpos
          + "" should have retrieved namespace info before initBlockPool."");
    }
    
    setClusterId(nsInfo.clusterID, nsInfo.getBlockPoolID());

    // Register the new block pool with the BP manager.
    blockPoolManager.addBlockPool(bpos);
    
    // In the case that this is the first block pool to connect, initialize
    // the dataset, block scanners, etc.
    initStorage(nsInfo);

    // Exclude failed disks before initializing the block pools to avoid startup
    // failures.
    checkDiskError();

    data.addBlockPool(nsInfo.getBlockPoolID(), conf);  <----- this line throws disk error exception
    blockScanner.enableBlockPoolId(bpos.getBlockPoolId());
    initDirectoryScanner(conf);
  }
{code}

{{FsVolumeList#addBlockPool}} is the source of exception.
{code:title=FsVolumeList.java}
  void addBlockPool(final String bpid, final Configuration conf) throws IOException {
    long totalStartTime = Time.monotonicNow();
    
    final List<IOException> exceptions = Collections.synchronizedList(
        new ArrayList<IOException>());
    List<Thread> blockPoolAddingThreads = new ArrayList<Thread>();
    for (final FsVolumeImpl v : volumes) {
      Thread t = new Thread() {
        public void run() {
          try (FsVolumeReference ref = v.obtainReference()) {
            FsDatasetImpl.LOG.info(""Scanning block pool "" + bpid +
                "" on volume "" + v + ""..."");
            long startTime = Time.monotonicNow();
            v.addBlockPool(bpid, conf);
            long timeTaken = Time.monotonicNow() - startTime;
            FsDatasetImpl.LOG.info(""Time taken to scan block pool "" + bpid +
                "" on "" + v + "": "" + timeTaken + ""ms"");
          } catch (ClosedChannelException e) {
            // ignore.
          } catch (IOException ioe) {
            FsDatasetImpl.LOG.info(""Caught exception while scanning "" + v +
                "". Will throw later."", ioe);
            exceptions.add(ioe);
          }
        }
      };
      blockPoolAddingThreads.add(t);
      t.start();
    }
    for (Thread t : blockPoolAddingThreads) {
      try {
        t.join();
      } catch (InterruptedException ie) {
        throw new IOException(ie);
      }
    }
    if (!exceptions.isEmpty()) {
      throw exceptions.get(0); <----- here's the original of exception
    }
    
    long totalTimeTaken = Time.monotonicNow() - totalStartTime;
    FsDatasetImpl.LOG.info(""Total time to scan all replicas for block pool "" +
        bpid + "": "" + totalTimeTaken + ""ms"");
  }
{code}"
downgrade from 2.7.2 to 2.5.0,HDFS-10767,,,,,"I have already upgrade my cluster鈥檚 namenodes(with one stand by for HA) and several datanodes from 2.5.0 folloing https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html#Downgrade_and_Rollback;
 
I take following steps:
1. hdfs dfsadmin -rollingUpgrade prepare;
2. hdfs dfsadmin -rollingUpgrade query;
3. hdfs dfsadmin -shutdownDatanode <host:port> upgrade
4. restart and upgrade datanode;

However, I terminated the upgrade by mistake with command ""hfs dfsadmin -rollingUpgrade finalize""

Currently, I have two 2.7.2 nematodes, and three 2.7.2 datanodes and 63 2.5.0 datanodes; Now I want to downgrade the nematodes and datanodes from 2.7.2 back to 2.5.0;

But when I try to downgrade nematode and restart with 鈥?rollingUpgrade downgrade鈥? namenode cannot get started, I get rolling exception:
2016-08-16 20:37:08,642 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /home/maintain/hadoop/data/hdfs-namenode. Reported: -63. Expecting = -57.
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:178)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:131)
        at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:608)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:228)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:323)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:955)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:700)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:529)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:585)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:751)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:735)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1407)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1473)
2016-08-16 20:37:08,645 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@dx-pipe-sata61-pm:50070
2016-08-16 20:37:08,745 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2016-08-16 20:37:08,746 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2016-08-16 20:37:08,746 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2016-08-16 20:37:08,746 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /home/maintain/hadoop/data/hdfs-namenode. Reported: -63. Expecting = -57.
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:178)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:131)
        at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:608)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:228)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:323)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:955)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:700)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:529)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:585)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:751)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:735)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1407)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1473)"
TestBalancer#testExitZeroOnSuccess fails intermittently,HDFS-6983,,,,,"TestBalancer#testExitZeroOnSuccess fails intermittently on branch-2. And probably fails on trunk too.

The test fails 1 in 20 times when I ran it in a loop. Here is the how it fails.

{noformat}
org.apache.hadoop.hdfs.server.balancer.TestBalancer
testExitZeroOnSuccess(org.apache.hadoop.hdfs.server.balancer.TestBalancer)  Time elapsed: 53.965 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Rebalancing expected avg utilization to become 0.2, but on datanode 127.0.0.1:35502 it remains at 0.08 after more than 40000 msec.
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.waitForBalancer(TestBalancer.java:321)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.runBalancerCli(TestBalancer.java:632)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:549)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:437)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.oneNodeTest(TestBalancer.java:645)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.testExitZeroOnSuccess(TestBalancer.java:845)


Results :

Tests in error: 
  TestBalancer.testExitZeroOnSuccess:845->oneNodeTest:645->doTest:437->doTest:549->runBalancerCli:632->waitForBalancer:321 Timeout
{noformat}"
error while creating collection in solr,HDFS-10669,,,,,"Hello Team,
   I have configured Solr in cloud mode on my apache hadoop 4 node cluster. I ""have created a collection with name tweets"". able to use the collection without any issues.

 When I try to create  new collection . I am getting below error but but directory gets created under solr in hdfs.  please help

user@Hadoop3:/usr/local/solr_download/solr-5.5.2$ sudo ./bin/solr create -c tweets1  -d data_driven_schema_configs

Connecting to ZooKeeper at localhost:9983 ...
Re-using existing configuration directory tweets1

Creating new collection 'tweets1' using command:
http://172.16.16.129:8983/solr/admin/collections?action=CREATE&name=tweets1&numShards=1&replicationFactor=1&maxShardsPerNode=1&collection.configName=tweets1


ERROR: Failed to create collection 'tweets1' due to: {172.16.16.129:8983_solr=org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException:Error from server at http://172.16.16.129:8983/solr: Error CREATEing SolrCore 'tweets1_shard1_replica1': Unable to create core [tweets1_shard1_replica1] Caused by: Illegal pattern component: T}

"
Namenode doesn't pass config to UGI in format,HDFS-9770,,,,,"The {{NameNode.format()}} method should call {{UserGroupInformation.setConfiguration(conf)}} before using the UGI.  Otherwise, the config that the UGI is using is not the same as what the NN is using."
repair test org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos,HDFS-4311,,,,,"Some of the test cases in this test class are failing because they are affected by static state changed by the previous test cases. Namely this is the static field org.apache.hadoop.security.UserGroupInformation.loginUser .
The suggested patch solves this problem.
Besides, the following improvements are done:
1) parametrized the user principal and keytab values via system properties;
2) shutdown of the Jetty server and the minicluster between the test cases is added to make the test methods independent on each other."
Create a generic function to synchronize async functions and methods. ,HDFS-9326,hdfs-client,,,,"The majority of the functionality in libhdfs++ is asynchronous, but some applications need synchronous operations.  At the time of filing this only happens in 3 places in the C API, however that number is going to grow a lot once the C and high level C++ APIs expose all of the namenode functions.

This synchronization is typically implemented like this:
auto promise = std::make_shared<std::promise<T>>()
std::future<T> = future(promise->get_future());

auto async_callback = [promise] () {promise->set_value(val);};

SomeClass::AsyncMethod(async_callback); 

auto result = future.get()

Ideally this could all be pushed into a templated function so that the promise and future don't need to be defined at the call site.  This would probably take the form of doing a std::bind to get all the arguments in place at the call site and then passing that to the synchronize function.

This appears to require some template magic that isn't always well supported; see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=51979.
"
Erasure Coding: Rename CorruptReplicasMap to CorruptRedundancyMap in BlockManager to more generic,HDFS-10407,namenode,,,,"The idea of this jira is to rename the following entity in BlockManager,

- {{CorruptReplicasMap}} to {{CorruptRedundancyMap}}"
DFSClient filesBeingWritten memory leak when client gets RemoteException - could only be replicated to 0 nodes instead of minReplication (=1),HDFS-10504,hdfs-client,,,,"I'm trying to migrate data from nfs to hdfs. I have about 2million files with small sizes. That takes about 4 hours in my env, but I randomly get an exception during migration. Got 12 of those during the test (stack below). 

Now when I'm getting the exception, I'm doing a sleep for one second, after I check if the file is there (api says yes, but it's reported size is zero bytes). So I'm removing the file, then start writing it again and at that point it succeeds. 

Here is the stack:
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File xxx/xxx/xxx could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1592)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3158)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3082)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:822)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1459)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1255)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)



When I write I'm using the try with resource which should call close method on the FSDataOutputStream. This triggers the 
dfsClient.endFileLease(fileId) to be called which should remove the ref from:
DFSClient:
synchronized(filesBeingWritten) {
      filesBeingWritten.remove(inodeId);
      if (filesBeingWritten.isEmpty()) {
        lastLeaseRenewal = 0;
      }
    }


But when the process finishes, I get:

2016-06-07 22:26:54,734 - ERROR [Thread-3] (DFSClient.closeAllFilesBeingWritten:940) - Failed to close inode 1675022
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xxx/xxx/xxx could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1592)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3158)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3082)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:822)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)


Now, when there is no space on the datanode, I get this error a lot which causes my migration java client to die with OutOfMemory. The cause is DFSClient.filesBeingWritten taking almost 1GB."
DiskBalancer: Refactor Execute Command,HDFS-9564,balancer & mover,,,,This is used to track refactoring execute command.
DiskBalancer: Refactor Plan Command,HDFS-9563,balancer & mover,,,,This is used to track refactoring plan command.
Enabled memory locking and now HDFS won't start up,HDFS-10502,fs,,,,"My goal is to speed up reads.  I have about 500k small files (2k to 15k) and I'm trying to use HDFS as a cache for serialized instances of java objects.

I've written the code to construct and serialize all the objects out to HDFS, and am now hoping to improve read performance, because accessing the objects from disk-based storage is proving to be too slow for my application's SLA's.

So my first question is, is using memory locking and hdfs cacheadmin pools and directives the right way to go, to cache my objects into memory, or should I create RAM disks, and do memory-based storage instead?

If hdfs cacheadmin is the way to go (it's the path I'm going down so far), then I need to figure out if what's happening is a bug or if I've configured something wrong, because when I start up HDFS with a gig of memory locked (both in limits.d for ulimit -l and also in hdfs-site.xml) and the server starts up, and presumably tries to cache things into memory, I get hours and hours of timeouts in the logs like this:

2016-06-08 07:42:50,856 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.SocketTimeoutException: Call From stgb-fe1.litle.com/10.1.9.66 to localhost:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51647 remote=localhost/127.0.0.1:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:554)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51647 remote=localhost/127.0.0.1:8020]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:520)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1084)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:979)
"
HDFS ZKFC HealthMonitor Throw a Exception Cause AutoFailOver,HDFS-10373,auto-failover,,,,"HDFS ZKFC HealthMonitor Throw a Exception 
2016-05-05 02:00:59,475 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at XXX-XXX-XXX-hadoop.jd.local/172.22.17
1.XX:8021: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ""XXX-XXX-XXX-hadoop.jd.local/172.22.171.XX""; destinat
ion host is: XXX-XXX-XXX-hadoop.jd.local"":8021;

Cause HA AutoFailOver"
"Can not read file from java.io.IOException: Need XXX bytes, but only YYY  bytes available",HDFS-10484,hdfs,,,,"We are running CDH 4.1.2 distro and trying to read file from HDFS. It ends up with exception @datanode saying


2016-06-02 10:43:26,354 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(X.X.X.X, storageID=DS-404876644-X.X.X.X-50010-1462535537579, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=cluster18;nsid=2115086255;c=0):Got exception while serving BP-2091182050-X.X.X.X-1358362115729:blk_5037101550399368941_420502314 to /X.X.X.X:58614
java.io.IOException: Need 10172416 bytes, but only 10072576 bytes available
at org.apache.hadoop.hdfs.server.datanode.BlockSender.waitForMinLength(BlockSender.java:387)
at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:189)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:268)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:88)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:63)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:219)
at java.lang.Thread.run(Thread.java:662)
2016-06-02 10:43:26,354 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: app112.rutarget.ru:50010:DataXceiver error processing READ_BLOCK operation src: /X.X.X.X:58614 dest: /X.X.X.X:50010
java.io.IOException: Need 10172416 bytes, but only 10072576 bytes available
at org.apache.hadoop.hdfs.server.datanode.BlockSender.waitForMinLength(BlockSender.java:387)
at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:189)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:268)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:88)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:63)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:219)
at java.lang.Thread.run(Thread.java:662)



FSCK shows file as being open for write, however hdfs client that handles writes to this file closed it long time ago -- so file stucked in RBW for a few last days. How can we get actual data  block in this case? I found only binary .meta file on datanode but not actual block with data.



-- "
Move block replication logic from BlockManager to a new class ReplicationManager,HDFS-9442,namenode,,,,"Currently the {{BlockManager}} is managing all replication logic for over- , under- and mis-replicated blocks. This jira proposes to move that code to a new class named {{ReplicationManager}} for cleaner code logic, shorter source files, and easier lock separating work in future.

The {{ReplicationManager}} is a package local class, providing {{BlockManager}} with methods that accesses its internal data structures of replication queue. Meanwhile, the class maintains the lifecycle of {{replicationThread}} and {{replicationQueuesInitializer}} daemon."
Cover package org.apache.hadoop.yarn.webapp.hamlet with unit tests,HDFS-4531,,,,,
Change WebHDFS to support file ID,HDFS-4398,webhdfs,,,,
 balancer-Test failed because of time out,HDFS-3747,balancer & mover,,,,When run a given test case for Banlancer Test. In the test the banlancer thread try to move some block cross the rack but it can't find any available blocks in the source rack. Then the thread won't interrupt until the tag isTimeUp reaches 20min. But maven judges the test failed because the thread have runned for 15min.
Invalid counter tag in HDFS balancer which lead to infinite loop,HDFS-3746,balancer & mover,,,,Everytime banlancer try to move a block cross the rack. For every NameNodeConnector it will instance a new banlancer. The tag notChangedIterations is reseted to 0. Then it won't reach 5 and exit the thread for not moved in 5 consecutive iterations. This lead to a infinite loop
"read(long position, byte[] buffer, int offset, int length) is not  behaving as expected",HDFS-3361,hdfs-client,,,,"Start NN and DN
write a file with size 1024
now try to read file using following api
fsin.read(writeBuff, 1024, fsin.available())..This is retuning correctly as expected.
fsin.read(10, writeBuff, 10, fsin.available())(this is retunring zero.)

Here it's returning zero..But actual file length is 1024
 
  *Java Docs provided* 
{code}
/**
   * Read bytes from the given position in the stream to the given buffer.
   *
   * @param position  position in the input stream to seek
   * @param buffer    buffer into which data is read
   * @param offset    offset into the buffer in which data is written
   * @param length    maximum number of bytes to read
   * @return total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached
   */
  public int read(long position, byte[] buffer, int offset, int length)
    throws IOException {
    return ((PositionedReadable)in).read(position, buffer, offset, length);
  }
{code}

But If I try with only ->fsin.read(10, writeBuff, 10, fsin.available())(client prog contains only one read method)

I am getting actual length...I am not sure actual cause..

Please correct me If I am wrong.........
"
Rolling upgrade won't finish if SBN is configured without StandbyCheckpointer,HDFS-6637,namenode,,,,"In HA setup cluster, for rolling upgrade, the image file ""fsimage_rollback"" is generated by StandbyCheckpointer thread of SBN. While if configuration ""dfs.ha.standby.checkpoints"" is set false, there will be no StandbyCheckpointer thread in SBN. This will lead to the rolling upgrade never finish. "
Browsing filesystem from specific datanode in live nodes page also should include delegation token in the url,HDFS-4223,,,,,"Browsing file system from the 'Browse the filesystem' link includes 'tokenString' as a parameter in the URL.

Same way browsing using specific datanode from live nodes page also should include 'tokenString' as a parameter to avoid following exception

{noformat}javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]{noformat}"
ViewFS should check the existence of the mapped namespace directories in the mount table,HDFS-5712,federation,,,,"ViewFS doesn't validate the mount table mapping. Even the mapped directory on NameNode doesn't exist, list directories or ""dfs -ls""  command can still show the mapped directory.

This confuses users and applications when they try to create files under the mapped directories. They will get file-not-exist error but viewfs shows the directory exists.

It would be less misleading if ViewFS can validate the  mount table and report found errors."
BKJM: Two namenodes usng bkjm can race to create the version znode,HDFS-4154,ha,,,,"nd one will get the following error.

2012-11-06 10:04:00,200 INFO hidden.bkjournal.org.apache.zookeeper.ClientCnxn: Session establishment complete on server 109-231-69-172.flexiscale.com/109.231.69.172:2181, sessionid = 0x13ad528fcfe0005, negotiated timeout = 4000
2012-11-06 10:04:00,710 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalArgumentException: Unable to construct journal, bookkeeper://109.231.69.172:2181;109.231.69.173:2181;109.231.69.174:2181/hdfsjournal
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1251)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals(FSEditLog.java:226)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initSharedJournalsForRead(FSEditLog.java:206)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog(FSImage.java:657)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:590)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:544)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:423)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:385)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:401)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:435)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:611)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:592)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1135)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1201)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1249)
        ... 14 more
Caused by: java.io.IOException: Error initializing zk
        at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.<init>(BookKeeperJournalManager.java:233)
        ... 19 more
Caused by: hidden.bkjournal.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hdfsjournal/version
        at hidden.bkjournal.org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
        at hidden.bkjournal.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at hidden.bkjournal.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:778)
        at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.<init>(BookKeeperJournalManager.java:222)
        ... 19 more"
"WebHdfsFileSystem execute get, renew and cancel delegationtoken operation should use spnego to authenticate",HDFS-6436,webhdfs,,,,"while in kerberos secure mode, when using WebHdfsFileSystem to access HDFS, it allways get an *org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized*, for example, when call WebHdfsFileSystem.listStatus it will execute a LISTSTATUS Op, and this Op should authenticate via *delegation token*, so it will execute a GETDELEGATIONTOKEN Op to get a delegation token(actually GETDELEGATIONTOKEN authenticates via *SPNEGO*), but it still use delegation token to authenticate, so it allways get an Unauthorized Exception.
Exception is like this:
{code:java}
19:05:11.758 [main] DEBUG o.a.h.hdfs.web.URLConnectionFactory - open URL connection
java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:287)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:82)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:538)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:406)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:434)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:430)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:1058)
19:05:11.766 [main] DEBUG o.a.h.security.UserGroupInformation - PrivilegedActionException as:bangtao@CYHADOOP.COM (auth:KERBEROS) cause:java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	at org.apache.hadoop.hdfs.web.TokenAspect.ensureTokenInitialized(TokenAspect.java:134)
19:05:11.767 [main] DEBUG o.a.h.security.UserGroupInformation - PrivilegedActionException as:bangtao@CYHADOOP.COM (auth:KERBEROS) cause:java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:213)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getAuthParameters(WebHdfsFileSystem.java:371)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toUrl(WebHdfsFileSystem.java:392)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractFsPathRunner.getUrl(WebHdfsFileSystem.java:602)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:533)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:406)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:434)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:430)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.listStatus(WebHdfsFileSystem.java:1037)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1483)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1523)
	at org.apache.hadoop.fs.FileSystem$4.<init>(FileSystem.java:1679)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:1678)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:1661)
	at org.apache.hadoop.fs.FileSystem$5.<init>(FileSystem.java:1723)
	at org.apache.hadoop.fs.FileSystem.listFiles(FileSystem.java:1720)
	at com.cyou.marketing.hop.filesystem.App$1.run(App.java:34)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at com.cyou.marketing.hop.filesystem.App.main(App.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	... 40 more
{code}"
JspHelper#bestNode() doesn't handle bad datanodes correctly,HDFS-5146,,,,,"JspHelper#bestNode() doesn't handle correctly if the chosen datanode is down.

{code}    while (s == null) {
      if (chosenNode == null) {
        do {
          if (doRandom) {
            index = DFSUtil.getRandom().nextInt(nodes.length);
          } else {
            index++;
          }
          chosenNode = nodes[index];
        } while (deadNodes.contains(chosenNode));
      }
      chosenNode = nodes[index];
{code}

In this part of the code, choosing the datanode will be done only once.
If the chosen datanode is down, then definitely exception will be thrown instead of re-chosing the available node."
OutOfMemory by BPServiceActor.offerService() takes down DataNode,HDFS-4475,,,,,"In DataNode, there are catchs around BPServiceActor.offerService() call but no catch for OutOfMemory as there is for the DataXeiver as introduced in 0.22.0.

The issue can be replicated like this:
1) Create a cluster of X DataNodes and 1 NameNode and low memory settings (-Xmx128M or something similar).
2) Flood HDFS with small file creations (any should work actually).
3) DataNodes will hit OoM, stop blockpool service, and shutdown.

The resolution is to catch the OoMException and handle it properly when calling BPServiceActor.offerService() in DataNode.java; like as done in 0.22.0 of Hadoop. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC.

LOG ERROR:
2013-02-04 11:46:01,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool BP-1105714849-10.10.10.110-1360005776467 (storage id DS-1952316202-10.10.10.112-50010-1360005820993) service to vmhost2-vm0/10.10.10.110:8020
java.lang.OutOfMemoryError: GC overhead limit exceeded
2013-02-04 11:46:01,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-1105714849-10.10.10.110-1360005776467 (storage id DS-1952316202-10.10.10.112-50010-1360005820993) service to vmhost2-vm0/10.10.10.110:8020"
Erasure Coding: Improve exception handling in ErasureCodingWorker#ReconstructAndTransferBlock,HDFS-9832,erasure-coding,,,,"There are three places in {{ErasureCodingWorker#ReconstructAndTransferBlock}} that I think can be improved.
1.In run method, the step3 transfer data will be failed sometimes, and this will cause buffers not be cleared completely, is better to invoke clearBuffer again in finally handling?
{code}
        while (positionInBlock < maxTargetLength) {
          final int toReconstruct = (int) Math.min(
              bufferSize, maxTargetLength - positionInBlock);
          // step1: read from minimum source DNs required for reconstruction.
          // The returned success list is the source DNs we do real read from
          Map<ExtendedBlock, Set<DatanodeInfo>> corruptionMap = new HashMap<>();
          try {
            success = readMinimumStripedData4Reconstruction(success,
                toReconstruct, corruptionMap);
          } finally {
            // report corrupted blocks to NN
            reportCorruptedBlocks(corruptionMap);
          }

          // step2: decode to reconstruct targets
          reconstructTargets(success, targetsStatus, toReconstruct);

          // step3: transfer data
          if (transferData2Targets(targetsStatus) == 0) {
            String error = ""Transfer failed for all targets."";
            throw new IOException(error);
          }

          clearBuffers();
          positionInBlock += toReconstruct;
        }
{code}

2.Is better to set null to buffers objects, targetsOutput and socket objects in finally handling code?
{code}
      } finally {
        datanode.decrementXmitsInProgress();
        // close block readers
        for (StripedReader stripedReader : stripedReaders) {
          closeBlockReader(stripedReader.blockReader);
        }
        for (int i = 0; i < targets.length; i++) {
          IOUtils.closeStream(targetOutputStreams[i]);
          IOUtils.closeStream(targetInputStreams[i]);
          IOUtils.closeStream(targetSockets[i]);
        }
      }
{code}

3.buffers in {{ReconstructAndTransferBlock}} are not released. In {{clearBuffers}}, it will finally invoke buffer.clear(), but this only change the index position and not really release the space. So this seems not a small problem."
TestDataNodeLifeline always fails on trunk on mac.,HDFS-10362,,,,,"TestDataNodeLifeline fails always on my local box but haven't seen any failure in jenkins build.
{noformat}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 29.861 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeLifeline
testSendLifelineIfHeartbeatBlocked(org.apache.hadoop.hdfs.server.datanode.TestDataNodeLifeline)  Time elapsed: 18.81 sec  <<< FAILURE!
java.lang.AssertionError: Expect DataNode to be kept alive by lifeline. expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeLifeline.testSendLifelineIfHeartbeatBlocked(TestDataNodeLifeline.java:185)


Results :

Failed tests: 
  TestDataNodeLifeline.testSendLifelineIfHeartbeatBlocked:185 Expect DataNode to be kept alive by lifeline. expected:<1> but was:<0>
{noformat}"
Inculde file id in these ClientProtocol RPCs which were originally only using path to identify a file,HDFS-4469,namenode,,,,"Like HDFS-4340(add fileId to addBlock), this JIRA is to track the change to add fileId to other RPC calls, such as append, complete and etc."
hdfs dfs -count of a .snapshot directory fails claiming file does not exist,HDFS-4847,snapshots,,,,"I successfully allow snapshots for /tmp and create three snapshots. I verify that the three snapshots are in /tmp/.snapshot.

However, when I attempt _hdfs dfs -count /tmp/.snapshot_ I get a file does not exist exception.

Running -count on /tmp finds /tmp successfully.

{code}
schu-mbp:~ schu$ hadoop fs -ls /tmp/.snapshot
2013-05-24 10:27:10,070 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxr-xr-x   - schu supergroup          0 2013-05-24 10:26 /tmp/.snapshot/s1
drwxr-xr-x   - schu supergroup          0 2013-05-24 10:27 /tmp/.snapshot/s2
drwxr-xr-x   - schu supergroup          0 2013-05-24 10:27 /tmp/.snapshot/s3
schu-mbp:~ schu$ hdfs dfs -count /tmp
2013-05-24 10:27:20,510 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
          12            0                  0 /tmp
schu-mbp:~ schu$ hdfs dfs -count /tmp/.snapshot
2013-05-24 10:27:30,397 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
count: File does not exist: /tmp/.snapshot
schu-mbp:~ schu$ hdfs dfs -count -q /tmp/.snapshot
2013-05-24 10:28:23,252 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
count: File does not exist: /tmp/.snapshot
schu-mbp:~ schu$
{code}

In the NN logs, I see:
{code}
2013-05-24 10:27:30,857 INFO  [IPC Server handler 6 on 8020] FSNamesystem.audit (FSNamesystem.java:logAuditEvent(6143)) - allowed=true	ugi=schu (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/.snapshot	dst=null	perm=null
2013-05-24 10:27:30,891 ERROR [IPC Server handler 7 on 8020] security.UserGroupInformation (UserGroupInformation.java:doAs(1492)) - PriviledgedActionException as:schu (auth:SIMPLE) cause:java.io.FileNotFoundException: File does not exist: /tmp/.snapshot
2013-05-24 10:27:30,891 INFO  [IPC Server handler 7 on 8020] ipc.Server (Server.java:run(1864)) - IPC Server handler 7 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:49738: error: java.io.FileNotFoundException: File does not exist: /tmp/.snapshot
java.io.FileNotFoundException: File does not exist: /tmp/.snapshot
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(FSDirectory.java:2267)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:3188)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:726)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:48057)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1842)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1838)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1836)
{code}

Likewise, the _hdfs dfs du_ command fails with the same problem. 

Hadoop version:
{code}
schu-mbp:~ schu$ hadoop version
Hadoop 3.0.0-SNAPSHOT
Subversion git://github.com/apache/hadoop-common.git -r ccaf5ea09118eedbe17fd3f5b3f0c516221dd613
Compiled by schu on 2013-05-24T04:45Z
From source with checksum ee94d984bcf5cc38ca12a1efedb68fc
This command was run using /Users/schu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar
{code}"
DNs may OOM under high webhdfs load,HDFS-6967,datanode,webhdfs,,,"Webhdfs uses jetty.  The size of the request thread pool is limited, but jetty will accept and queue infinite connections.  Every queued connection is ""heavy"" with buffers, etc.  Unlike data streamer connections, thousands of webhdfs connections will quickly OOM a DN.  The accepted requests must be bounded and excess clients rejected so they retry on a new DN."
Add the possibility to mark a node as 'low priority' for read in the DFSClient,HDFS-3705,hdfs-client,,,,"This has been partly discussed in HBASE-6435.

The DFSClient includes a 'bad nodes' management for reads and writes. Sometimes, the client application already know that some deads are dead or likely to be dead.
An example is the 'HBase Write-Ahead-Log': when HBase reads this file, it knows that the HBase regionserver died, and it's very likely that the box died so the datanode on the same box is dead as well. This is actually critical, because:
- it's the hbase recovery that reads these log files
- if we read them it means that we lost a box, so we have 1 dead replica out the the 3. 
- for all files read, we have 33% of chance to go to the dead datanode
- as the box just died, we're very likely to get a timeout exception so we're delaying the hbase recovery by 1 minute. For HBase, it means that the data is not available during this minute.
"
TestBlockReaderLocalLegacy flakes in MiniDFSCluster#shutdown,HDFS-4764,test,,,,"I've seen this fail on two test-patch runs, and I'm pretty sure it's unrelated.

{noformat}
Error Message

Test resulted in an unexpected exit
Stacktrace

java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1416)
	at org.apache.hadoop.hdfs.TestBlockReaderLocalLegacy.testBothOldAndNewShortCircuitConfigured(TestBlockReaderLocalLegacy.java:152)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{noformat}"
Only keep successfully loaded volumes in the configuration.,HDFS-7173,datanode,,,,"Hot swapping data volumes might fail. The user should be able to fix the failed volumes and disks, then ask the {{DataNode}} to retry the previously failed volumes. 

To attempt to reload the failed volume again on the same directory, this failed directory must not be presented in the {{Configuration}} object that {{DataNode has}}. Therefore, it should only put successfully loaded volumes into the {{Configuration}} object."
skip checksums when reading a cached block via non-local reads,HDFS-5521,datanode,,,,"The DataNode needs to skip checksumming when reading a cached block via non-local reads.  This is like HDFS-5182, but for non-short-circuit."
TestDatanodeManager#testNumVersionsReportedCorrect occasionally fails,HDFS-7471,test,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1957/ :

{code}
FAILED:  org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsReportedCorrect

Error Message:
The map of version counts returned by DatanodeManager was not what it was expected to be on iteration 237 expected:<0> but was:<1>

Stack Trace:
java.lang.AssertionError: The map of version counts returned by DatanodeManager was not what it was expected to be on iteration 237 expected:<0> but was:<1>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsReportedCorrect(TestDatanodeManager.java:150)
{code}"
Track new transactions per thread so that unnecessary logSync() calls can be avoided,HDFS-4191,namenode,,,,"With this fix, logSync() can be called anytime by a thread without affecting the batched syncs metrics. As discussed in HDFS-4186, we will put this feature only to trunk first and let it soak for some time. "
Target port chosen by Hftp/Hsftp for getting delegation token may be incorrect,HDFS-5275,security,,,,"The port selection to get the delegation token is confusing. Also the code documentation and tests appear to conflict.

The comment in {{HftpFileSystem#getCanonicalServiceName}} seems to indicate that the configured secure port should be chosen, ignoring the port from the URI.
{code}
  public String getCanonicalServiceName() {
    // unlike other filesystems, hftp's service is the secure port, not the
    // actual port in the uri
    return SecurityUtil.buildTokenService(nnSecureUri).toString();
  }
{code}

However {{TestHftpFileSystem#testHsftpCustomUriPortWithCustomDefaultPorts}} tests that the returned port is the one from the URI.
{code}
@Test
public void testHsftpCustomUriPortWithCustomDefaultPorts() throws IOException {
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_HTTPS_PORT_KEY, 456);

  URI uri = URI.create(""hsftp://localhost:789"");
  HsftpFileSystem fs = (HsftpFileSystem) FileSystem.get(uri, conf);

  assertEquals(456, fs.getDefaultPort());
  assertEquals(456, fs.getDefaultSecurePort());

  assertEquals(uri, fs.getUri());
  assertEquals(
      ""127.0.0.1:789"",
      fs.getCanonicalServiceName()
  );
}
{code}

The test still passes because {{HsftpFileSystem}} (incorrectly?) overrides {{getNamenodeSecureAddr}}.


Either the code needs to be fixed or we should document the correct behavior."
WebHDFS: Support EC commands through webhdfs,HDFS-7350,,,,,
Implement asynchronous setOwner for DistributedFileSystem,HDFS-10350,hdfs,hdfs-client,,,This is proposed to implement an asynchronous setOwner.
Allow trigger block report from all datanodes,HDFS-10359,datanode,,,,"Since we have HDFS-7278 allows trigger block report from one certain datanode. It would be helpful to add a option to this command to trigger block report from all datanodes.
Command maybe like this:
*hdfs dfsadmin -triggerBlockReport \[-incremental\] <datanode_host:ipc_port|all>*"
Support extensions to WebHdfsFileSystem,HDFS-9938,,,,,"This JIRA is to request opinion from the community, Can new file system use an extension of WebHDFS file system. 
Considering all the known limitation WebHDFS has over implementing a new client by extending FileSystem class.

Option we have is 
1. Use the namespace org.apache.hadoop.hdfs.web in new file system implementation to access protected functionality from WebHdfsFileSystem.
2. Change the WebHdfs to support extensions
3. Suggestion on different approach like new client by sub classing FileSystem.
"
Potential deadlock #HeartbeatManager,HDFS-8000,,,,,"Cluster loaded with 90000000+ Blocks
Restart DN
 access NN UI..Then NN will  got Hang

Will attach td.."
hdfs-default.xml shouldn't use hadoop.tmp.dir for dfs.data.dir (0.20 and lower) / dfs.datanode.dir (0.21 and up),HDFS-964,,,,,"This question/problem pops up all the time.  Can we *please* eliminate hadoop.tmp.dir's usage from the default in dfs.data.dir.  It is confusing to new people and results in all sorts of weird accidents.  If we want the same value, fine, but there are a lot of implied things by the variable re-use."
FSDataOutputStream.write() allocates new byte buffer on each operation,HDFS-10194,hdfs-client,,,,"This is the code:
{code}
 private DFSPacket createPacket(int packetSize, int chunksPerPkt, long offsetInBlock, long seqno, boolean lastPacketInBlock) throws InterruptedIOException {
     final byte[] buf;
     final int bufferSize = PacketHeader.PKT_MAX_HEADER_LEN +   packetSize;
 
     try {
       buf = byteArrayManager.newByteArray(bufferSize);
     } catch (InterruptedException ie) {
       final InterruptedIOException iioe = new InterruptedIOException(
           ""seqno="" + seqno);
       iioe.initCause(ie);
       throw iioe;
     }
 
     return new DFSPacket(buf, chunksPerPkt, offsetInBlock, seqno,
                          getChecksumSize(), lastPacketInBlock);
}
{code}

"
Jenkins pre-commit build does not pick up the correct attachment.,HDFS-2852,build,,,,"When two files are attached to a jira, slaves build twice but only the latest attachement.

For example, the patch_tested.txt from PreCommit-Admin shows correct attachment numbers for In HDFS-2784.
From https://builds.apache.org/job/PreCommit-Admin/56284/artifact/patch_tested.txt
{noformat}
...
HBASE-5271,12511722
HDFS-2784,12511725
HDFS-2836,12511727
HDFS-2784,12511726
{noformat}

But the Jenkins build slaves had built #12511726 twice."
"HDFS build is broken, ivy-resolve-common does not find hadoop-common",HDFS-1519,build,,,,"HADOOP_DIR/hdfs$ ant ivy-resolve-common
Buildfile: build.xml

ivy-download:
      [get] Getting: http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar
      [get] To: /usr/products/hadoop/v0_21_0/ANY/hdfs/ivy/ivy-2.1.0.jar
      [get] Not modified - so not downloaded

ivy-init-dirs:

ivy-probe-antlib:

ivy-init-antlib:

ivy-init:
[ivy:configure] :: Ivy 2.1.0 - 20090925235825 :: http://ant.apache.org/ivy/ ::
[ivy:configure] :: loading settings :: file = /usr/products/hadoop/v0_21_0/ANY/hdfs/ivy/ivysettings.xml

ivy-resolve-common:
[ivy:resolve] 
[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: WARNINGS
[ivy:resolve] 		module not found: org.apache.hadoop#hadoop-common;0.21.0
[ivy:resolve] 	==== apache-snapshot: tried
[ivy:resolve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.pom
[ivy:resolve] 	  -- artifact org.apache.hadoop#hadoop-common;0.21.0!hadoop-common.jar:
[ivy:resolve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.jar
[ivy:resolve] 	==== maven2: tried
[ivy:resolve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.pom
[ivy:resolve] 	  -- artifact org.apache.hadoop#hadoop-common;0.21.0!hadoop-common.jar:
[ivy:resolve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.jar
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: org.apache.hadoop#hadoop-common;0.21.0: not found
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

BUILD FAILED
/usr/products/hadoop/v0_21_0/ANY/hdfs/build.xml:1549: impossible to resolve dependencies:
	resolve failed - see output for details

Total time: 3 seconds
"
Test RPC timeout fix of HADOOP-12672 against HDFS,HDFS-9954,,,,,
FsVolume should tolerate few times check-dir failed due to deletion by mistake,HDFS-9819,,,,,"FsVolume should tolerate few times check-dir failed because sometimes we will do a delete dir/file operation by mistake in datanode data-dirs. Then the {{DataNode#startCheckDiskErrorThread}} will invoking checkDir method periodicity and find dir not existed, throw exception. The checked volume will be added to failed volume list. The blocks on this volume will be replicated again. But actually, this is not needed to do. We should let volume can be tolerated few times check-dir failed like config {{dfs.datanode.failed.volumes.tolerated}}."
Get input/output error while copying 800 small files to NFS Gateway mount point ,HDFS-9152,nfs,,,,"We have around *800 3-5K* files on local file system, we have nfs gateway mounted on */hdfs/*, when we tried to copy these files to HDFS by 

*cp ~/userdata/* /hdfs/user/cqdemo/demo3.data/*

most of files are failed because of 

cp: writing `/hdfs/user/cqdemo/demo3.data/TRAFF_201408011220.csv': Input/output error
cp: writing `/hdfs/user/cqdemo/demo3.data/TRAFF_201408011221.csv': Input/output error
cp: writing `/hdfs/user/cqdemo/demo3.data/TRAFF_201408011222.csv': Input/output error

for same set of files, I tried to use hadoop dfs -put command to do the copy, it works fine."
Change all read only operation audit log to debug level,HDFS-9828,,,,,"Audit log should only log important operations such as create new file/folder , , delete file/folder and so on, but the read only operation need to be kept in log, otherwise the log size will become very big quickly if heavy load."
DataNode doesn't log any shutdown info when the process of DataNode exiting,HDFS-9863,,,,,"One of my datanodes exited without any shutdown info. 
{code}
2016-02-25 14:46:00,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1942012336-XX.XX.2.191-1406726500544:blk_1730224536_658031130, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2016-02-25 15:03:55,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = XX.XX6032/XX.XX.6.32
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
{code}
I think maybe full gc causes this problem, so I looked the datanode gc log. There is a cms gc but the time of this gc is after than restart datanode time. 
{code}
2016-02-25T15:03:57.930+0800: 2.756: [GC2016-02-25T15:03:57.930+0800: 2.756: [ParNew: 1677824K->24417K(1887488K), 0.0249280 secs] 1677824K->24417K(8178944K), 0.0251010 secs] [Times: user=0.24 sys=0.07, real=0.02 secs]
2016-02-25T15:12:46.498+0800: 531.324: [GC [1 CMS-initial-mark: 0K(6291456K)] 780481K(8178944K), 0.0554170 secs] [Times: user=0.06 sys=0.00, real=0.07 secs]
2016-02-25T15:12:46.567+0800: 531.393: [CMS-concurrent-mark-start]
2016-02-25T15:12:46.574+0800: 531.400: [CMS-concurrent-mark: 0.006/0.007 secs] [Times: user=0.07 sys=0.02, real=0.01 secs]
2016-02-25T15:12:46.574+0800: 531.400: [CMS-concurrent-preclean-start]
2016-02-25T15:12:46.589+0800: 531.415: [CMS-concurrent-preclean: 0.015/0.015 secs] [Times: user=0.16 sys=0.06, real=0.01 secs]
{code}
It seems this is not the main reason. Gc of time before datanode exiting seems normal.
{code}
2016-02-25T14:45:39.743+0800: 5431411.796: [GC2016-02-25T14:45:39.743+0800: 5431411.796: [ParNew: 1686799K->22696K(1887488K), 0.0385700 secs] 2908579K->1244476K(8178944K) icms_dc=0 , 0.0388280 secs] [Times: user=0.23 sys=0.01, real=0.04 secs]
{code}
So it looks confusion. Attach the complete gc logs and datanode log."
inconsistent message while running rename command if target exists,HDFS-9824,,,,,"In the following case, the message <mv: `/tmp/src/1.log': Input/output error> is not friendly, it's better to show <mv: `/tmp/dest/1.log': File exists>.

Source dir:
{noformat}
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:23 /tmp/src/1.log
{noformat}
Dest dir:
{noformat}
-rw-r--r--   3 root hdfs       8526 2016-02-17 22:00 /tmp/dest/1.log
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:17 /tmp/dest/2.log
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:18 /tmp/dest/3.log
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:18 /tmp/dest/4.log
{noformat}
Running <hadoop fs -mv /tmp/src/1.log /tmp/dest> displays inconsistent message that complains input/output error, while <hadoop fs -mv /tmp/src/1.log /tmp/dest/1.log> will show <mv: `/tmp/dest/1.log': File exists>. The behavior of the two should be similar."
Make DelegationTokenFetcher a Tool,HDFS-9846,tools,,,,"Currently the {{org.apache.hadoop.hdfs.tools.DelegationTokenFetcher}} is not implementing the {{Tool}} interface, while it should.

This jira is to track the effort of refactoring the code to implement the {{Tool}} interface. The main benefits are unified generic option parsing, modifying the configurations, and conjunction with {{ToolRunner}}."
HDFS Non DFS Used (History server),HDFS-9813,,,,,"<Reproduce Steps>
1. su hdfs
2. cd /home/hdfs
3. hadoop jar tkfc-static-wordlist-10.jar 1 issue money 20150601 (extracting data elements from webpages)

<Actual Result>
1. usage non used DFS Continued to increase
2. HDFS service will shut down"
PeerCache evicts too frequently causing connection restablishments,HDFS-9520,,,,,"Env: 20 node setup
dfs.client.socketcache.capacity = 16

Issue:
======
Monitored PeerCache and it was evicting lots of connections during close. Set ""dfs.client.socketcache.capacity=20"" and tested again. Evictions still happened. Screenshot of profiler is attached in the JIRA.

Workaround:
===========
Temp fix was to set ""dfs.client.socketcache.capacity=1000"" to prevent eviction. 


Added more debug logs revealed that multimap.size() was 40 instead of 20. LinkedListMultimap returns the total values instead of key size causing lots of evictions.

{code}
   if (capacity == multimap.size()) {
      evictOldest();
    }
{code}

Should this be (capacity == multimap.keySet().size())  or is it expected that the ""dfs.client.socketcache.capacity"" be set to very high value?

\cc [~gopalv], [~sseth]
"
Read apis in ByteRangeInputStream does not read all the bytes specified when chunked transfer-encoding is used in the server,HDFS-8943,webhdfs,,,,"With the default Webhdfs server implementation the read apis in ByteRangeInputStream work as expected reading the correct number of bytes for these apis :

{{public int read(byte b[], int off, int len)}}

{{public int read(long position, byte[] buffer, int offset, int length)}}

But when a custom Webhdfs server implementation is plugged in which uses chunked Transfer-encoding, these apis read only the first chunk. Simple fix would be to loop and read till bytes specified similar to {{readfully()}}"
o.a.h.hdfs.TestRecoverStripedFile fails intermittently in trunk,HDFS-9716,erasure-coding,,,,"See recent builds:
* https://builds.apache.org/job/PreCommit-HDFS-Build/14269/testReport/org.apache.hadoop.hdfs/TestRecoverStripedFile/testRecoverThreeDataBlocks1/
* https://builds.apache.org/job/PreCommit-HADOOP-Build/8477/testReport/org.apache.hadoop.hdfs/TestRecoverStripedFile/testRecoverThreeDataBlocks/"
Use a throttler for replica write in datanode,HDFS-7265,datanode,,,,"BlockReceiver process packets in BlockReceiver.receivePacket() as follows
# read from socket
# enqueue the ack
# write to downstream
# write to disk

The above steps is repeated for each packet in a single thread.  When there are a lot of concurrent writes in a datanode, the write time in #4 becomes very long.  As a result, it leads to SocketTimeoutException since it cannot read from the socket for a long time."
Long running Balancer should renew TGT,HDFS-9698,balancer & mover,security,,,"When the {{Balancer}} runs beyond the configured TGT lifetime, the current logic won't renew TGT."
HDFS file append failing in single node configuration,HDFS-6953,,,,,"The following issue happens in both fully distributed and single node setup. 
I have looked to the thread(https://issues.apache.org/jira/browse/HDFS-4600) about simiral issue in multinode cluster and made some changes of my configuration however it does not changed anything. The configuration files and application sources are attached.

Steps to reproduce:

$ ./test_hdfs

2014-08-27 14:23:08,472 WARN  [Thread-5] hdfs.DFSClient (DFSOutputStream.java:run(628)) - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[127.0.0.1:50010], original=[127.0.0.1:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:969)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1035)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:532)
FSDataOutputStream#close error:
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[127.0.0.1:50010], original=[127.0.0.1:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:969)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1035)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:532)

I have tried to run a simple example in java, that uses append function. It failed too.

I have tried to get hadoop environment settings from java application. It has shown the default ones. Not the settings that ones that are mentioned in core-site.xml and hdfs-site.xml files.
"
WebHDFS write commands fail when running multiple DataNodes on one machine,HDFS-8701,,,,,"For testing purposes, we are running multiple DataNodes per machine. {{hadoop fs}} commands work fine when using the {{hdfs://}} protocol, but when using {{webhdfs://}}, any command that writes to HDFS (e.g.: {{-put}} or {{-touchz}}) fails:
{code}
$ hadoop fs -put test.txt webhdfs://<host>:<port>/user/foo
put: <machine>-dn-4
{code}"
Erasure Coding: Move DFSStripedIO stream related classes to hadoop-hdfs-client,HDFS-9172,,,,,"The idea of this jira is to move the striped stream related classes to {{hadoop-hdfs-client}} project. This will help to be in sync with the HDFS-6200 proposal.

- DFSStripedInputStream
- DFSStripedOutputStream
- StripedDataStreamer"
FileContext tests fail on Windows,HDFS-815,test,,,,"The following FileContext-related tests are failing on windows because of incorrect use ""test.build.data"" system property for setting hdfs paths, which end up containing ""C:"" as a path component, which hdfs does not support.
{code}
org.apache.hadoop.fs.TestFcHdfsCreateMkdir
org.apache.hadoop.fs.TestFcHdfsPermission
org.apache.hadoop.fs.TestHDFSFileContextMainOperations
org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
{code}"
Wrong checking penultimate block replicated in FSNamesystem,HDFS-9662,,,,,"There is a bug in checking penultimate block replicated in FSNamesystem.
{code}
/**
   * Check that the indicated file's blocks are present and
   * replicated.  If not, return false. If checkall is true, then check
   * all blocks, otherwise check only penultimate block.
   */
  boolean checkFileProgress(String src, INodeFile v, boolean checkall) {
    assert hasReadLock();
    if (checkall) {
      return blockManager.checkBlocksProperlyReplicated(src, v
          .getBlocks());
    } else {
      // check the penultimate block of this file
      BlockInfo b = v.getPenultimateBlock();
      return b == null ||
          blockManager.checkBlocksProperlyReplicated(
              src, new BlockInfo[] { b });
    }
  }
{code}
When the param checkall is true, the checking operations is true.But if checkall is false, it will check the penultimate block of this file. And if the BlockInfo b is null, it will return true by this code, but actually it should be return false because the penultimate block is not replicated and has no blockInfo."
Erasure Coding: cover more test situations of datanode failure during client writing,HDFS-8889,,,,,"Currently 9 streamers are working together for the client writing. A small number of failed datanodes (<= 3) for a block group should not influence the writing. There鈥檙e a lot of datanode failure cases and we should cover as many as possible in unit test.
Suppose streamer 4 fails, the following situations for the next block group should be considered:
1)	all streamers succeed
2)	Streamer 4 still fails
3)	only streamer 1 fails
4)	only streamer 8 fails (test parity streamer)
5)	streamer 4 and 6 fail
6)	streamer 4 and 1,6 fail
7)	streamer 4 and 1,2,6 fail
8)	streamer 2, 6 fail
Suppose streamer 2 and 4 fail, the following situations for the next block group should be considered:
1)	only streamer 2 and 4 fail
2)	streamer 2, 4, 8 fail
3)	only streamer 2 fails
4)	streamer 3 , 8 fail
For a single streamer, we should consider the following situations of the time of datanode failure:
1)	before writing the first byte
2)	before finishing writing the first cell
3)	right after finishing writing the first cell
4)	before writing the last byte of the block
Other situations:
1)	more than 3 streamers fail at the first block group
2)	more than 3 streamers fail at the last block group
<more 鈥?"
"my java client use muti-thread to put a same file to a same hdfs uri, after no lease error锛宼hen client OutOfMemoryError",HDFS-9617,,,,,"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /Tmp2/43.bmp.tmp (inode 2913263): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_2084151715_1, pendingcreates: 250]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3358)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3160)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3042)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1653)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)


my java client(JVM -Xmx=2G) :
jmap TOP15锛?num     #instances         #bytes  class name
----------------------------------------------
   1:         48072     2053976792  [B
   2:         45852        5987568  <constMethodKlass>
   3:         45852        5878944  <methodKlass>
   4:          3363        4193112  <constantPoolKlass>
   5:          3363        2548168  <instanceKlassKlass>
   6:          2733        2299008  <constantPoolCacheKlass>
   7:           533        2191696  [Ljava.nio.ByteBuffer;
   8:         24733        2026600  [C
   9:         31287        2002368  org.apache.hadoop.hdfs.DFSOutputStream$Packet
  10:         31972         767328  java.util.LinkedList$Node
  11:         22845         548280  java.lang.String
  12:         20372         488928  java.util.concurrent.atomic.AtomicLong
  13:          3700         452984  java.lang.Class
  14:           981         439576  <methodDataKlass>
  15:          5583         376344  [S"
Add valgrind suppression for statically initialized library objects,HDFS-9192,hdfs-client,,,,"When using --leak-check=full there's a lot of noise due to static initialization of constants and memory pools, most of them from protobuf.

Add a suppression file that helps cut down on this noise but is selective enough that real issues aren't going to be masked as well."
Fix NPE in MiniKMS.start(),HDFS-9508,test,,,,"Sometimes, KMS resource file can not be loaded. When this happens, an InputStream variable will be a null pointer which will subsequently throw NPE.

This is a supportability JIRA that makes the error message more explicit, and explain why NPE is thrown. Ultimately, leads us to understand why the resource files can not be loaded."
NameNode and DataNode metric log file name should follow the other log file name format.,HDFS-9114,,,,,"Currently datanode and namenode metric log file name is {{datanode-metrics.log}} and {{namenode-metrics.log}}.
This file name should be like {{hadoop-hdfs-namenode-metric-host192.log}} same as namenode log file {{hadoop-hdfs-namenode-host192.log}}.
This will help when we will copy log for issue analysis from different node.
"
LlapServiceDriver can fail if only the packaged logger config is present,HDFS-9567,,,,,"I was incrementally updating my setup on some VM and didn't have the logger config file, so the packaged one was picked up apparently, which caused this:
{noformat}
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: jar:file:/home/vagrant/llap/apache-hive-2.0.0-SNAPSHOT-bin/lib/hive-llap-server-2.0.0-SNAPSHOT.jar!/llap-daemon-log4j2.properties
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.run(LlapServiceDriver.java:234)
	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.main(LlapServiceDriver.java:58)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: jar:file:/home/vagrant/llap/apache-hive-2.0.0-SNAPSHOT-bin/lib/hive-llap-server-2.0.0-SNAPSHOT.jar!/llap-daemon-log4j2.properties
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	... 3 more
{noformat}"
standby nn can't started,HDFS-8011,ha,,,,"We have seen crash when starting the standby namenode, with fatal errors. Any solutions, workarouds, or ideas would be helpful for us.
1. Here is the context: 
	At begining we have 2 namenodes, take A as active and B as standby. For some resons, namenode A was dead, so namenode B is working as active.
	When we try to restart A after a minute, it can't work. During this time a lot of files were put to HDFS, and a lot of files were renamed. 
	Nodenode A crashed when ""awaiting reported blocks in safemode"" each time.
 
2. We can see error log below:
	1)2015-03-30  ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation CloseOp [length=0, inodeId=0, path=/xxx/_temporary/xxx/part-r-00074.bz2, replication=3, mtime=1427699913947, atime=1427699081161, blockSize=268435456, blocks=[blk_2103131025_1100889495739], permissions=dm:dm:rw-r--r--, clientName=, clientMachine=, opCode=OP_CLOSE, txid=7632753612]
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction.setGenerationStampAndVerifyReplicas(BlockInfoUnderConstruction.java:247)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction.commitBlock(BlockInfoUnderConstruction.java:267)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.forceCompleteBlock(BlockManager.java:639)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.updateBlocks(FSEditLogLoader.java:813)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:383)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:209)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:122)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:737)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:227)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:321)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$0(EditLogTailer.java:302)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1528)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:413)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:292)
        
   2)2015-03-30  FATAL org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unknown error encountered while tailing edits. Shutting down standby N
N.
java.io.IOException: Failed to apply edit log operation AddBlockOp [path=/xxx/_temporary/xxx/part-m-00121, penultimateBlock=blk_2102331803_1100888911441, lastBlock=blk_2102661068_1100889009168, RpcClientId=, RpcCallId=-2]: error
null
        at org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt(MetaRecoveryContext.java:94)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:215)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:122)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:737)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:227)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:321)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$0(EditLogTailer.java:302)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1528)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:413)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:292)
        
"
Confusing WebHDFS exception when host doesn't resolve,HDFS-4488,webhdfs,,,,"{noformat}
$ hadoop fs -ls webhdfs://unresolvable-host/
ls: unresolvable-host
$ echo $?
1
{noformat}"
Re-replication for files with enough replicas in single rack,HDFS-9127,,,,,"Found while debugging testcases in HDFS-8647

 *Scenario:* 
=======

Start a cluster with Single rack with three DN's
write a file with RF=3
adde two Nodes with different racks

As per blockplacement policy ([Rack Awareness|http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html]) atleast one replica needs to replicate to newly added rack.But it is not happening..Because of following reason.

{color:blue}
when cluster was single rack,block will be removed from {{neededReplications}} after 3 replicas.

later, after adding new rack, only replications will happen which are present in {{neededReplications}}

So for the blocks which already have enough replicas, new rack replications will not take place..
{color}"
"Balancer exits if fs.defaultFS is set to a different, but semantically identical, URI from dfs.namenode.rpc-address",HDFS-3439,balancer & mover,,,,"The balancer determines the set of NN URIs to balance by looking at fs.defaultFS and all possible dfs.namenode.(service)rpc-address settings. If fs.defaultFS is, for example, set to ""hdfs://foo.example.com:8020/"" (note the trailing ""/"") and the rpc-address is set to ""hdfs://foo.example.com:8020"" (without a ""/""), then the balancer will conclude that there are two NNs and try to balance both. However, since both of these URIs refer to the same actual FS instance, the balancer will exit with ""java.io.IOException: Another balancer is running.  Exiting ..."""
balancer.Balancer: java.lang.NullPointerException while HADOOP_CONF_DIR is empty or wrong,HDFS-2220,balancer & mover,,,,"When HADOOP_CONF_DIR is empty or wrongly set and balancer is called without proper --config , in clientside STDOUT we get NPE.

$ echo $HADOOP_CONF_DIR
$ hadoop balancer
Balancing took 46.0 milliseconds
11/06/13 05:14:04 ERROR balancer.Balancer: java.lang.NullPointerException
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:136)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:176)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:206)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:200)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.createNamenode(Balancer.java:911)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.init(Balancer.java:860)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1475)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:811)

I think it would be good to give more meaningful error messege instead of NPE"
NN old UI (block_info_xml) not available in 2.7.x,HDFS-9064,namenode,,,,"In 2.6.x hadoop deploys, given a blockId it was very easy to find out the file name and the locations of replicas (also whether they are corrupt or not).
This was the REST call:
{noformat}
 http://<nnAddress>:<port#>/block_info_xml.jsp?blockId=xxx
{noformat}
But this was removed by HDFS-6252 in 2.7 builds.
Creating this jira to restore that functionality."
ACL permission check does not union groups to determine effective permissions,HDFS-8748,security,,,,"In the ACL permission checking routine, the implemented named group section does not match the design document.

In the design document, its shown in the pseudo-code that if the requester is not the owner or a named user, then the applicable groups are unioned together to form effective permissions for the requester.


Instead, the current implementation will search for the first group that grants access and will use that. It will not union the permissions together.

Here is the design document's description of the desired behavior
{quote}
If the user is a member of the file's group or at least one group for which there is a
named group entry in the ACL, then effective permissions are calculated from groups.
This is the union of the file group permissions (if the user is a member of the file group)
and all named group entries matching the user's groups. For example, consider a user
that is a member of 2 groups: sales and execs. The user is not the file owner, and the
ACL contains no named user entries. The ACL contains named group entries for both
groups as follows: group:sales:r颅颅\-\-, group:execs:\-颅w\-颅. In this case, the user's effective
permissions are rw颅-.
{quote}

 ??https://issues.apache.org/jira/secure/attachment/12627729/HDFS-ACLs-Design-3.pdf page 10??

The design document's algorithm matches that description:

*Design Document Algorithm*
{code:title=DesignDocument}
if (user == fileOwner) {
    effectivePermissions = aclEntries.getOwnerPermissions()
} else if (user 鈭?aclEntries.getNamedUsers()) {
    effectivePermissions = aclEntries.getNamedUserPermissions(user)
} else if (userGroupsInAcl != 鈭? {
    effectivePermissions = 鈭?    if (fileGroup 鈭?userGroupsInAcl) {
        effectivePermissions = effectivePermissions 鈭?        aclEntries.getGroupPermissions()
    }
    for ({group | group 鈭?userGroupsInAcl}) {
        effectivePermissions = effectivePermissions 鈭?        aclEntries.getNamedGroupPermissions(group)
    }
} else {
    effectivePermissions = aclEntries.getOthersPermissions()
}
{code}
??https://issues.apache.org/jira/secure/attachment/12627729/HDFS-ACLs-Design-3.pdf page 9??

The current implementation does NOT match the description.
*Current Trunk*
{code:title=FSPermissionChecker.java}
    // Use owner entry from permission bits if user is owner.
    if (getUser().equals(inode.getUserName())) {
      if (mode.getUserAction().implies(access)) {
        return;
      }
      foundMatch = true;
    }

    // Check named user and group entries if user was not denied by owner entry.
    if (!foundMatch) {
      for (int pos = 0, entry; pos < aclFeature.getEntriesSize(); pos++) {
        entry = aclFeature.getEntryAt(pos);
        if (AclEntryStatusFormat.getScope(entry) == AclEntryScope.DEFAULT) {
          break;
        }
        AclEntryType type = AclEntryStatusFormat.getType(entry);
        String name = AclEntryStatusFormat.getName(entry);
        if (type == AclEntryType.USER) {
          // Use named user entry with mask from permission bits applied if user
          // matches name.
          if (getUser().equals(name)) {
            FsAction masked = AclEntryStatusFormat.getPermission(entry).and(
                mode.getGroupAction());
            if (masked.implies(access)) {
              return;
            }
            foundMatch = true;
            break;
          }
        } else if (type == AclEntryType.GROUP) {
          // Use group entry (unnamed or named) with mask from permission bits
          // applied if user is a member and entry grants access.  If user is a
          // member of multiple groups that have entries that grant access, then
          // it doesn't matter which is chosen, so exit early after first match.
          String group = name == null ? inode.getGroupName() : name;
          if (getGroups().contains(group)) {
            FsAction masked = AclEntryStatusFormat.getPermission(entry).and(
                mode.getGroupAction());
            if (masked.implies(access)) {
              return;
            }
            foundMatch = true;
          }
        }
      }
    }
{code}

As seen in the GROUP section, the permissions check will succeed if and only if a single group (either owning group or named group) has all of the requested permissions. The permissions check should instead succeed if the requested permissions can be obtained by unioning all of the groups permissions."
Move the quota commands out from dfsadmi.,HDFS-8638,namenode,,,,"Currently for setQuota() API in FSNamesystem we don't have any superuser check.
So with the reference of [HDFS-7323 | https://issues.apache.org/jira/browse/HDFS-7323] we should move quota commands from dfsadmin or if we want it in dfsadmin then we should add check for superuser privileges. 
"
Cherry-pick HDFS-7546 to branch-2,HDFS-8618,security,,,,
resolved,HDFS-8318,,,,,resolved
Get HDFS file name based on block pool id and block id,HDFS-8246,hdfs-client,namenode,,,"This feature provides HDFS shell command and C/Java API to retrieve HDFS file name based on block pool id and block id.

1. The Java API in class DistributedFileSystem
public String getFileName(String poolId, long blockId) throws IOException
2. The C API in hdfs.c
char* hdfsGetFileName(hdfsFS fs, const char* poolId, int64_t blockId)
3. The HDFS shell command 
 hdfs dfs [generic options] -fn <poolId> <blockId>

This feature is useful if you have HDFS block file name in local file system and want to  find out the related HDFS file name in HDFS name space (http://stackoverflow.com/questions/10881449/how-to-find-file-from-blockname-in-hdfs-hadoop).  Each HDFS block file name in local file system contains both block pool id and block id, for sample HDFS block file name /hdfs/1/hadoop/hdfs/data/current/BP-97622798-10.3.11.84-1428081035160/current/finalized/subdir0/subdir0/blk_1073741825,  the block pool id is BP-97622798-10.3.11.84-1428081035160 and the block id is 1073741825. The block  pool id is uniquely related to a HDFS name node/name space,  and the block id is uniquely related to a HDFS file within a HDFS name node/name space, so the combination of block pool id and a block id is uniquely related a HDFS file name. 

The shell command and C/Java API do not map the block pool id to name node, so it鈥檚 user鈥檚 responsibility to talk to the correct name node in federation environment that has multiple name nodes. The block pool id is used by name node to check if the user is talking with the correct name node.

The implementation is straightforward. The client request to get HDFS file name reaches the new method String getFileName(String poolId, long blockId) in FSNamesystem in name node through RPC,  and the new method does the followings,
(1)	Validate the block pool id.
(2)	Create Block  based on the block id.
(3)	Get BlockInfoContiguous from Block.
(4)	Get BlockCollection from BlockInfoContiguous.
(5)	Get file name from BlockCollection.
"
Erasure Coding: client fails to write large file when one datanode fails,HDFS-8704,,,,,"I test current code on a 5-node cluster using RS(3,2).  When a datanode is corrupt, client succeeds to write a file smaller than a block group but fails to write a large one. {{TestDFSStripeOutputStreamWithFailure}} only tests files smaller than a block group, this jira will add more test situations.


A streamer may encounter some bad datanodes when writing blocks allocated to it. When it fails to connect datanode or send a packet, the streamer needs to prepare for the next block. First it removes the packets of current  block from its data queue. If the first packet of next block has already been in the data queue, the streamer will reset its state and start to wait for the next block allocated for it; otherwise it will just wait for the first packet of next block. The streamer will check periodically if it is asked to terminate during its waiting.
"
A bug in BlocksMap that  cause NameNode  memory leak.,HDFS-7592,namenode,,,,"In our HDFS production environment, NameNode FGC frequently after running for 2 months, we have to restart NameNode manually.
We dumped NameNode's Heap for objects statistics.
Before restarting NameNode:
    num #instances #bytes class name
    ----------------------------------------------
聽聽聽     1: 59262275 3613989480 [Ljava.lang.Object;
聽聽聽聽聽聽聽     ...
聽聽      10: 8549361 615553992 org.apache.hadoop.hdfs.server.namenode.BlockInfoUnderConstruction
聽聽      11: 5941511 427788792 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction
After restarting NameNode:
    num #instances #bytes class name
    ----------------------------------------------
聽聽聽      1: 44188391 2934099616 [Ljava.lang.Object;
聽聽聽聽聽聽聽聽聽     ...
聽聽      23: 721763 51966936 org.apache.hadoop.hdfs.server.namenode.BlockInfoUnderConstruction
聽聽      24: 620028 44642016 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction
We find the number of BlockInfoUnderConstruction is abnormally large before restarting NameNode.
As we know, BlockInfoUnderConstruction keeps block state when the file is being written. But the write pressure of
our cluster is far less than million/sec. We think there is a memory leak in NameNode.
We fixed the bug as followsing patch.
diff --git a/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java b/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
index 7a40522..857d340 100644
--- a/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
+++ b/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
@@ -205,6 +205,8 @@ class BlocksMap {
       DatanodeDescriptor dn = currentBlock.getDatanode(idx);
       dn.replaceBlock(currentBlock, newBlock);
     }
+    // change to fix bug about memory leak of NameNode
+    map.remove(newBlock);
     // replace block in the map itself
     map.put(newBlock, newBlock);
     return newBlock;"
Delegation token is not created generateNodeDataHeader method of NamenodeJspHelper$NodeListJsp,HDFS-5263,namenode,webhdfs,,,"When Kerberos authentication is enabled, we are unable to browse to the data nodes using ( Name node web page --> Live Nodes --> Select any of the data nodes). The reason behind this is the delegation token is not provided as part of the url in the method (generateNodeDataHeader method of NodeListJsp)

{code}
      String url = HttpConfig.getSchemePrefix() + d.getHostName() + "":""
          + d.getInfoPort()
          + ""/browseDirectory.jsp?namenodeInfoPort="" + nnHttpPort + ""&dir=""
          + URLEncoder.encode(""/"", ""UTF-8"")
          + JspHelper.getUrlParam(JspHelper.NAMENODE_ADDRESS, nnaddr);
{code}

But browsing the file system using name node web page --> Browse the file system -> <any directory> is working fine as the redirectToRandomDataNode method of NamenodeJspHelper creates the delegation token

{code}
    redirectLocation = HttpConfig.getSchemePrefix() + fqdn + "":"" + redirectPort
        + ""/browseDirectory.jsp?namenodeInfoPort=""
        + nn.getHttpAddress().getPort() + ""&dir=/""
        + (tokenString == null ? """" :
           JspHelper.getDelegationTokenUrlParam(tokenString))
        + JspHelper.getUrlParam(JspHelper.NAMENODE_ADDRESS, addr);
{code}

I will work on providing a patch for this issue."
FSImage.getFsImageName should check whether fsimage exists,HDFS-5396,namenode,,,,"In https://issues.apache.org/jira/browse/HDFS-5367, fsimage may not write to all IMAGE dir, so we need to check whether fsimage exists before FSImage.getFsImageName returned."
Create EC zone should not need superuser privilege,HDFS-8333,,,,,"create EC zone should not need superuser privilege, for example, in multiple tenant scenario, common users only manage their own directory and subdirectory."
hadoop-hdfs-client dependency convergence error,HDFS-8128,build,,,,"Found the following in https://builds.apache.org/job/PreCommit-HDFS-Build/10258/consoleFull
{noformat}
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
Failed while enforcing releasability the error(s) are [
Dependency convergence error for org.apache.hadoop:hadoop-annotations:3.0.0-SNAPSHOT paths to dependency are:
+-org.apache.hadoop:hadoop-hdfs-client:3.0.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:3.0.0-SNAPSHOT
    +-org.apache.hadoop:hadoop-annotations:3.0.0-SNAPSHOT
and
+-org.apache.hadoop:hadoop-hdfs-client:3.0.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-annotations:3.0.0-20150410.234534-6484
]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
{noformat}"
remove replica and not add replica with wrong genStamp,HDFS-9298,,,,,"currently, in setGenerationStampAndVerifyReplicas, replica with wrong gen stamp is not really removed, only StorageLocation of that replica is removed. Moreover, we should check genStamp before addReplicaIfNotPresent"
"Erasure Coding: unifying common constructs like coding work, block reader and block writer across client and DataNode",HDFS-7679,,,,,"Based on the work done, we will have similar constructs like coding work, local/remote block reader/writer in both client and DataNode side, so it's possible to refactor the codes further and unify these constructs to eliminate possible duplicate codes."
Block Readers and Writers used in both client side and datanode side,HDFS-7653,,,,,"There're a lot of block read/write operations in HDFS-EC, for example, when client writes a file in striping layout, client has to write several blocks to several different datanodes; if a datanode wants to do an encoding/decoding task, it has to read several blocks from itself and other datanodes, and writes one or more blocks to itself or other datanodes.  "
Erasure Coding: Update last cellsize calculation according to whether the erasure codec has chunk boundary,HDFS-8376,,,,,"Current calculation for last cell size is as following. For parity cell, the last cell size is the same as the first data cell.  But some erasure codec has chunk boundary, then the last cellsize for parity block is the codec chunk size.
{code}
private static int lastCellSize(int size, int cellSize, int numDataBlocks,
      int i) {
    if (i < numDataBlocks) {
      // parity block size (i.e. i >= numDataBlocks) is the same as 
      // the first data block size (i.e. i = 0).
      size -= i*cellSize;
      if (size < 0) {
        size = 0;
      }
    }
    return size > cellSize? cellSize: size;
  }
{code}"
Erasure Coding: Add more EC zone management APIs (get/list EC zone(s)),HDFS-8087,,,,,
Fix some tests failed because of JUnit run in parallel after HDFS-9139,HDFS-9248,,,,,
"Namenode shutdown for ""ReplicationMonitor thread received Runtime exception""",HDFS-8426,,,,,"1.Last git commit code : 041c936e3b677f9d61e8a2c5deb20e7b2dd8292a
2.NameNode log for this error: [^5-15 NameNode shutdow log segment]"
hdfs-test artifact doesn't include config file for cluster execution,HDFS-2442,build,,,,With HDFS-1762 in place testConfCluster.xml needs to be packaged along with test classes so it can be used for testing on a real cluster.
Separate Platform specific funtions,HDFS-7768,hdfs-client,,,,"Current code has several platform-specific parts (e.g., get environment variables, get local addresses, print stack). We should separate these parts into platform folders.

This issue will do just that. Posix systems will be able to compile successfully. Windows will fail to compile due to unimplemented parts. The implementation for the Windows parts will be handle at HDFS-7188 "
Turn off TestDFSOverAvroRpc,HDFS-2660,test,,,,"With HDFS-2647, protobuf based RPCs are enabled for some of the protocol. With this, Avro RPC based protocol is not working. Avro based RPC needs to be turned on similar to how protobuf based RPCs are done. Until such a time, I propose turning off the test."
HDFS ivy dependencies aren't on classpath of bin/hdfs script,HDFS-1771,scripts,,,,"In working on another patch, I added guava as a dependency of HDFS. It ended up in ./build/ivy/lib/hadoop-hdfs/common/guava-r07.jar, but running bin/hdfs doesn't put this directory on the classpath. Instead it only puts the ivy dependencies of Common"
Remove TestDFSOverAvroRpc,HDFS-2664,,,,,"With HDFS-2647, HDFS has transitioned to protocol buffers. The server side implementation registers <Protocol>PB.class and a BlockingService as implementation. Client side uses <Protocol>PB.class as the interface. The RPC engine used is protobuf both for the RPC proxy and the server. With this TestDFSOverAvroRpc fails. I propose removing this test."
TestDfsOverAvroRpc is failing on trunk,HDFS-2298,test,,,,"The relevant bit of the error:

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdfs.TestDfsOverAvroRpc
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.486 sec <<< FAILURE!
testWorkingDirectory(org.apache.hadoop.hdfs.TestDfsOverAvroRpc)  Time elapsed: 1.424 sec  <<< ERROR!
org.apache.avro.AvroTypeException: Two methods with same name: delete
{noformat}"
"""hdfs version"" should print out information similar to what ""hadoop version"" prints out",HDFS-2119,,,,,Implement version in hdfs CLI
TestNNWithQJM#testNewNamenodeTakesOverWriter occasionally fails in trunk,HDFS-5897,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1665/testReport/junit/org.apache.hadoop.hdfs.qjournal/TestNNWithQJM/testNewNamenodeTakesOverWriter/ :
{code}
java.lang.Exception: test timed out after 30000 milliseconds
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:632)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1195)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
	at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog$1.run(EditLogFileInputStream.java:412)
	at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog$1.run(EditLogFileInputStream.java:401)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
{code}
I saw:
{code}
2014-02-06 11:38:37,970 ERROR namenode.EditLogInputStream (RedundantEditLogInputStream.java:nextOp(221)) - Got error reading edit log input stream http://localhost:40509/getJournal?jid=myjournal&segmentTxId=3&storageInfo=-51%3A1571339494%3A0%3AtestClusterID; failing over to edit log http://localhost:56244/getJournal?jid=myjournal&segmentTxId=3&storageInfo=-51%3A1571339494%3A0%3AtestClusterID
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 0; expected file to go up to 4
	at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:194)
	at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:83)
	at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.skipUntil(EditLogInputStream.java:140)
	at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:178)
	at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:83)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:167)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:708)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:606)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:263)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:874)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:634)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:446)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:502)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1291)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:939)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:824)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:678)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:359)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:340)
	at org.apache.hadoop.hdfs.qjournal.TestNNWithQJM.testNewNamenodeTakesOverWriter(TestNNWithQJM.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
{code}"
BlockInfo#numNodes should be numStorages,HDFS-8784,namenode,,,,The method actually returns the number of storages holding a block.
move hasClusterEverBeenMultiRack to NetworkTopology,HDFS-8689,,,,,
Convert BlockInfoUnderConstruction as an interface,HDFS-8835,namenode,,,,"Per discussion under HDFS-8499, this JIRA aims to convert {{BlockInfoUnderConstruction}} as an interface and {{BlockInfoContiguousUnderConstruction}} as its implementation. The HDFS-7285 branch will add {{BlockInfoStripedUnderConstruction}} as another implementation."
Implement ShrinkableHashMap extends java HashMap and use properly,HDFS-8912,,,,,"Currently {{LightWeightHashSet}} and {{LightWeightLinkedSet}} are used in hdfs, there are two advantages compared to java HashSet: one is the entry requires fewer memory, another is it's shrinkable.  In real cluster, hdfs is a long running service, and {{set}} may become large at some time and may become small after that, so shrinking the {{set}} when size hits the shrink threshold is necessary, it can improve the NN memory.

Same situation for {{map}}, some HashMap used in BlockManager (e.g., the hashmap in CorruptReplicasMap), it's better to be shrinkable. 
 I think it's worth to implement ShrinkableHashMap extends the java HashMap, for quick glance, seems few code is needed."
OzoneHandler : Enable stand-alone local testing mode,HDFS-8756,ozone,,,,"Enable ""jetty run"" in pom.xml , this will allow Ozone server to run in a stand-alone mode not as part of WebHDFS. This will allow a development mode for ozone where ozone will listen on Port 8080 and commands can be run against Ozone. This is strictly used for development purposes."
"S3:Hadoop tools jars should be added in ""dfs"" class path.",HDFS-8685,hdfs-client,,,,"{code}
./hdfs dfs -ls s3a://xyz:xyz/
-ls: Fatal internal error
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2224)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2638)
{code}"
Erasure Coding: add test for namenode process over replicated striped block,HDFS-8819,test,,,,
TestPersistBlocks#TestRestartDfsWithFlush appears to be flaky,HDFS-3811,,,,,"This test failed on a recent Jenkins build, but passes for me locally. Seems flaky.

See:

https://builds.apache.org/job/PreCommit-HDFS-Build/3021//testReport/org.apache.hadoop.hdfs/TestPersistBlocks/TestRestartDfsWithFlush/"
TestDatanodeBlockScanner.testBlockCorruptionRecoveryPolicy1 times out,HDFS-3532,test,,,,I've seen this test time out on recent trunk jenkins test patch runs even though HDFS-3266 was put in a couple weeks ago.
Erasure Coding: DFSStripedOutputStream#close throws NullPointerException exception in some cases,HDFS-8313,,,,,"{code}
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DataStreamer$LastException.check(DataStreamer.java:193)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:422)
{code}

 DFSStripedOutputStream#close throws NullPointerException exception in some cases"
Track BlockInfo instead of Block in CorruptReplicasMap,HDFS-8652,namenode,,,,"Currently {{CorruptReplicasMap}} uses {{Block}} as its key and records the list of DataNodes with corrupted replicas. For Erasure Coding since a striped block group contains multiple internal blocks with different block ID, we should use {{BlockInfo}} as the key.

HDFS-8619 is the jira to fix this for EC. To ease merging we will use jira to first make changes in trunk/branch-2.

"
ClassCastException in BlockManager.addStoredBlock() due to that blockReceived came after file was closed.,HDFS-4746,namenode,,,,"In some cases the last block replica of a file can be reported after the file was closed. In this case file inode is of type INodeFile. BlockManager.addStoredBlock() though expects it to be INodeFileUnderConstruction, and therefore class cast to MutableBlockCollection fails."
"Always read DU value from the cached ""dfsUsed"" file on datanode startup",HDFS-8710,,,,,"Currently, DataNode will cache DU value in ""dfsUsed"" file termly. When DataNode starts or restarts, it will read in the cached DU value from ""dfsUsed"" file if the value is less than 600 seconds old, otherwise, it will run DU command, which is a very time-consuming operation(may up to dozens of minutes) when DataNode has huge number of blocks.

Since slight imprecision of dfsUsed is not critical, and the DU value will be updated every 600 seconds (the default DU interval) after DataNode started, we can always read DU value from the cached file (Regardless of whether this value is less than 600 seconds old or not) and skip DU operation on DataNode startup to significantly shorten the startup time.
"
"""no suitable constructor found"" while building Hadoop 2.6.0",HDFS-7724,build,,,,"I'm getting the following error while building Hadoop 2.6.0. My objective is to compile Hadoop, and run Pig on it.

[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:25 min
[INFO] Finished at: 2015-02-02T14:38:51+05:30
[INFO] Final Memory: 49M/117M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-auth-examples: Compilation failure
[ERROR] D:\h\hadoop-2.6.0-src\hadoop-common-project\hadoop-auth-examples\src\main\java\org\apache\hadoop\security\authentication\examples\WhoClient.java:[36,31] error: no suitable constructor found for AuthenticatedURL(no arguments)
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-auth-examples: Compilation failure
D:\h\hadoop-2.6.0-src\hadoop-common-project\hadoop-auth-examples\src\main\java\org\apache\hadoop\security\authentication\examples\WhoClient.java:[36,31] error: no suitable constructor found for AuthenticatedURL(no arguments)

	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:347)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:154)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:582)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.CompilationFailureException: Compilation failure
D:\h\hadoop-2.6.0-src\hadoop-common-project\hadoop-auth-examples\src\main\java\org\apache\hadoop\security\authentication\examples\WhoClient.java:[36,31] error: no suitable constructor found for AuthenticatedURL(no arguments)

	at org.apache.maven.plugin.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:729)
	at org.apache.maven.plugin.CompilerMojo.execute(CompilerMojo.java:128)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	... 19 more
[ERROR] 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-auth-examples
"
Add and optimize for get LocatedFileStatus  in DFSClient,HDFS-8598,,,,,"If we want to get all files block locations in one directory, we have to call getFileBlockLocations for each file, it will take long time because of too many request. 
LocatedFileStatus has block location, but we can find it also call getFileBlockLocations  for each file in DFSClient. this jira is trying to optimize with only one RPC. "
2NN doesn't start with fs.defaultFS set to a viewfs URI unless service RPC address is also set,HDFS-3465,federation,namenode,,,"Looks like the 2NN first tries servicerpc-address then falls back on fs.defaultFS, which won't work in the case of federation since fs.defaultFS doesn't refer to an RPC address. Instead, the 2NN should first check servicerpc-address, then rpc-address, then fall back on fs.defaultFS.

{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Invalid
URI for NameNode address (check fs.defaultFS): viewfs:/// has no
authority.
       at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:315)
       at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:303)
       at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:296)
       at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:214)
       at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:178)
       at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:582)
{noformat}"
count with -h option displays namespace quota in human readable format,HDFS-8625,hdfs-client,,,,"When 'count' command is executed with '-h' option , namespace quota is displayed in human readable format --

Example :

hdfs dfsadmin -setQuota {color:red}1048576{color} /test

hdfs dfs -count -q -h -v /test
       {color:red}QUOTA       REM_QUOTA{color}     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME
         {color:red}1 M           1.0 M{color}            none             inf            1            0                  0 /test

QUOTA and REM_QUOTA shows 1 M (human readable format) which actually should give count value 1048576"
Implement GETDELEGATIONTOKEN and GETDELEGATIONTOKENS operation for WebImageViewer,HDFS-8621,,,,,"In Hadoop 2.7.0, WebImageViewer supports the following operations:
{code}
   .GETFILESTATUS
   .LISTSTATUS
   .GETACLSTATUS
{code}

I'm thinking it would be better for administrators if  {code} .GETDELEGATIONTOKEN  {code}  and  {code} .GETDELEGATIONTOKENS  {code}  are supported."
hadoop archive command looks at local files when using wildcard,HDFS-8514,,,,,"When using a wildcard in the {{hadoop archive}} command, it looks at the local filesystem.  For example:

{noformat}
>> [56] 18:13 : localdir :: hadoop fs -ls /tmp/dir
Found 2 items
-rw-r--r--   1 rkanter supergroup          0 2015-06-01 17:57 /tmp/dir/f1.txt
-rw-r--r--   1 rkanter supergroup          0 2015-06-01 17:57 /tmp/dir/f2.txt
>> [57] 18:13 : localdir :: ls -l
total 0
-rw-r--r--  1 rkanter  staff  0 Jun  1 18:11 local-file.log
-rw-r--r--  1 rkanter  staff  0 Jun  1 18:08 local-file.txt
>> [58] 18:14 : localdir :: hadoop archive -archiveName foo.har -p /tmp/dir/ * /tmp/
The resolved paths set is empty.  Please check whether the srcPaths exist, where srcPaths = [/tmp/dir/local-file.log, /tmp/dir/local-file.txt]
>> [59] 18:15 : localdir :: hadoop archive -archiveName foo.har -p /tmp/dir/ *.txt /tmp/
The resolved paths set is empty.  Please check whether the srcPaths exist, where srcPaths = [/tmp/dir/local-file.txt]
>> [60] 18:15 : localdir :: hadoop archive -archiveName foo.har -p hdfs://localhost:8020/tmp/dir/ *.txt /tmp/
The resolved paths set is empty.  Please check whether the srcPaths exist, where srcPaths = [hdfs://localhost:8020/tmp/dir/local-file.txt]
{noformat}"
Question: Why Namenode doesn't judge the status of replicas when convert block status from commited to complete? ,HDFS-8459,,,,,"  Why Namenode doesn't judge the status of replicas when convert block status from commited to complete?
  When client finished write block and call namenode::complete(), namenode do things as follow
  (in BlockManager::commitOrCompleteLastBlock):
       final boolean b = commitBlock((BlockInfoUnderConstruction)lastBlock, commitBlock);
      if(countNodes(lastBlock).liveReplicas() >= minReplication)
        completeBlock(bc, bc.numBlocks()-1, false);
      return b;
 
  But  the NameNode doesn't care how many replicas which status is finalized this block has! 
  It should be this: if there is no one replica which status is not finalized, the block should not convert to complete status!

  Because According to the appendDesign3.pdf (https://issues.apache.org/jira/secure/attachment/12445209/appendDesign3.pdf):
   Complete:鈥〢 鈥ヽomplete 鈥゜lock 鈥﹊s 鈥゛ 鈥゜lock 鈥﹚hose 鈥﹍ength鈥?and鈥?GS 鈥゛re 鈥ゝinalized 鈥゛nd鈥?NameNode鈥?has 鈥﹕een鈥?a鈥?GS/len 鈥﹎atched鈥?finalized 鈥﹔eplica 鈥﹐f 鈥﹖he鈥? block.鈥? 
"
Erasure Coding: test failed in TestDFSStripedInputStream.testStatefulRead() when use ByteBuffer,HDFS-8343,,,,,"It's failed since last commit

{code}
commit c61c9c855e7cd1d20f654c061ff16341ce2d9936
{code}"
Changing the replication factor for a directory should apply to new files under the directory too,HDFS-8436,,,,,"Changing the replication factor for a directory will only affect the existing files and the new files under the directory will get created with the default replication factor (dfs.replication from hdfs-site.xml) of the cluster. 

I would expect new files written under a directory to have the same replication factor set for the directory itself."
NFS gateway should throttle the data dumped on local storage,HDFS-7558,nfs,,,,"During file uploading, NFS gateway could dump the reordered write on local storage when the accumulated data size exceeds a limit.

Currently there is no data throttle for the data dumping, which could easily saturate the local disk especially when the client is on the same host as the gateway. 
"
hadoop fs -ls globbing gives inconsistent exit code,HDFS-2685,,,,,"_hadoop fs -ls_ command gives exit code for globbed input path, which is the exit code for the last resolved absolute path. Whereas _ls_ command always give same exit code regardless of position of non-existent path in globbing.

{code}$ hadoop fs -mkdir input/20110{1,2,3}/{A,B,C,D}/{1,2} {code}

Since directory 'input/201104/' is not present, the following command gives 255 as exit code.
{code}$ hadoop fs -ls input/20110{1,2,3,4}/ ; echo $? {code}
{noformat}
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/D
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/D
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/D
ls: Cannot access input/201104/: No such file or directory.
255
{noformat}


The directory 'input/201104/' is not present but given as second last parameter in globbing.
The following command gives 0 as exit code, because directory 'input/201103/' is present.
{code}$ hadoop fs -ls input/20110{1,2,4,3}/ ; echo $? {code}
{noformat}
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/D
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/D
ls: Cannot access input/201104/: No such file or directory.
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/D
0
{noformat}



Whereas, on Linux, ls command gives non-zero(2) as exit code, irrespective of position of non-existent path in globbing.
{code}$ mkdir -p input/20110{1,2,3,4}/{A,B,C,D}/{1,2} {code}


{code}$ ls input/20110{1,2,4,3}/ ; echo $? {code}
{noformat}
/bin/ls: input/201104/: No such file or directory
input/201101/:
./  ../  A/  B/  C/  D/

input/201102/:
./  ../  A/  B/  C/  D/

input/201103/:
./  ../  A/  B/  C/  D/
2
{noformat}



{code}$ ls input/20110{1,2,3,4}/ ; echo $? {code}
{noformat}
/bin/ls: input/201104/: No such file or directory
input/201101/:
./  ../  A/  B/  C/  D/

input/201102/:
./  ../  A/  B/  C/  D/

input/201103/:
./  ../  A/  B/  C/  D/
2
{noformat}
"
Namenode in trunk has much slower performance than Namenode in MR-279 branch,HDFS-2142,namenode,,,,"I am measureing the performance of the namenode by running the org.apache.hadoop.fs.loadGenerator.LoadGenerator application. This application shows there is a very large slowdown in the processing of opens, writes, closes, and operations per second in the trunk when compared to the MR-279 branch

There have been some race conditions and locking issues fixed in trunk, which is a very good thing because these race conditions were causing the namenode to crash under load conditions (see HDFS:1257). However, the slowdown to the namenode is considerable.

I am still trying to verify which changes caused the slowdown. It was originally suggested that the HDFS:988 may have caused the slowdown, but I don't think it was the culprit. I have checked out and built from SVN 3 revisions previous to HDFS988 and they all have about the same performance.

Here is my environment:
Host0: namenode daemon
Host1-9: simulate many datanodes using org.apache.hadoop.hdfs.DataNodeCluster
 
LoadGenerator output on MR-279 branch:
Average open execution time: 1.8496516782773909ms
Average deletion execution time: 2.956340167046317ms
Average create execution time: 3.725259427992913ms
Average write_close execution time: 11.151860288534548ms
Average operations per second: 1053.3666666666666ops/s

LoadGenerator output on trunk:
Average open execution time: 28.603515625ms
Average deletion execution time: 32.20792079207921ms
Average create execution time: 32.37326732673267ms
Average write_close execution time: 82.84752475247525ms
Average operations per second: 135.13333333333333ops/s
"
Unnecessary disk check triggered when socket operation has problem.,HDFS-5745,datanode,,,,"When BlockReceiver transfer data fails, it can be found SocketOutputStream translates the exception as IOException with the message ""The stream is closed"":
2014-01-06 11:48:04,716 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():
java.io.IOException: The stream is closed
at org.apache.hadoop.net.SocketOutputStream.write
at java.io.BufferedOutputStream.flushBuffer
at java.io.BufferedOutputStream.flush
at java.io.DataOutputStream.flush
at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run
at java.lang.Thread.run

Which makes the checkDiskError method of DataNode called and triggers the disk scan.

Can we make the modifications like below in checkDiskError to avoiding this unneccessary disk scan operations?:
{code}
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -938,7 +938,8 @@ public class DataNode extends Configured
          || e.getMessage().startsWith(""An established connection was aborted"")
          || e.getMessage().startsWith(""Broken pipe"")
          || e.getMessage().startsWith(""Connection reset"")
-         || e.getMessage().contains(""java.nio.channels.SocketChannel"")) {
+         || e.getMessage().contains(""java.nio.channels.SocketChannel"")
+         || e.getMessage().startsWith(""The stream is closed"")) {
       LOG.info(""Not checking disk as checkDiskError was called on a network"" +
         "" related exception""); 
       return;
{code}
"
libhdfs hdfs_read example uses hdfsRead wrongly,HDFS-923,libhdfs,,,,"In the examples of libhdfs,  the hdfs_read.c uses hdfsRead wrongly. 

{noformat}
    // read from the file
    tSize curSize = bufferSize;
    for (; curSize == bufferSize;) {
        curSize = hdfsRead(fs, readFile, (void*)buffer, curSize);
    }
{noformat} 

the condition curSize == bufferSize has problem."
resolved,HDFS-8119,,,,,resolved
test-patch comment doesn't show names of failed FI tests,HDFS-2009,build,test,,,"Looks like test-patch.sh only looks at the build/test/*xml test results, but it should also look at build-fi/test/*xml I think"
libhdfs: add hdfsFile cache,HDFS-7693,libhdfs,,,,Add an hdfsFile cache inside libhdfs.
Find a way to make encryption zone deletion work with HDFS trash.,HDFS-7271,encryption,,,,"Currently when HDFS trash is enabled, deletion of encryption zone will have issue:
{quote}
rmr: Failed to move to trash: ... can't be moved from an encryption zone.
{quote}
A simple way is to add ignore trash flag for fs rm operation."
BlockReceiver did not close ReplicaOuputStreams ,HDFS-7569,datanode,,,,"{{BlockReceiver#streams}} is a {{ReplicaOutputStreams}}, which holds two {{FileOutputStream}}s, i.e., {{ReplicaOutputStream#dataOut}} and {{checksumOut}}. The {{ReplicaOutputStreams#close}} is never be called in non-test code to close these two streams."
haadmin command usage prints incorrect command name,HDFS-7324,ha,tools,,,"Scenario:
=======
Try the help command for hadadmin like following..
Here usage is coming as ""DFSHAAdmin -ns"", Ideally this not availble which we can check following command.


[root@linux156 bin]#  *{color:red}./hdfs haadmin{color}* 
No GC_PROFILE is given. Defaults to medium.
 *{color:red}Usage: DFSHAAdmin [-ns <nameserviceId>]{color}* 
    [-transitionToActive <serviceId> [--forceactive]]
    [-transitionToStandby <serviceId>]
    [-failover [--forcefence] [--forceactive] <serviceId> <serviceId>]
    [-getServiceState <serviceId>]
    [-checkHealth <serviceId>]
    [-help <command>]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|jobtracker:port>    specify a job tracker
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]

 *{color:blue}[root@linux156 bin]# ./hdfs DFSHAAdmin -ns 100{color}*  
Error: Could not find or load main class DFSHAAdmin"
Consider renaming StorageID,HDFS-5264,datanode,namenode,,,"We should consider renaming StorageID to something else since we have changed the meaning of the field. Previously it was used to identify the single logical storage attached to a datanode and hence it was a de-facto identifier for a Datanode. Now the StorageID identifies a single storage. To avoid confusion of meaning especially when merging with other feature branches it may be best to rename it to something else.

We can do so when merging phase 1 of the Heterogeneous Storage work into trunk.

A partial list of places to update:
# FsVolumeSpi#storageID
# DatanodeStorageInfo#storageID
# DatanodeStorage#storageID
# StorageReceivedDeletedBlocks#storageID
# StorageReport#storageID
# LocatedBlock#storageIDs
# processFirstBlockReport
# DatanodeStorage#getStorageInfo
# TestDatanodeDescriptor#testBlocksCounter
# TestBlockManager.java
# FsDatasetSpi#getBlockReports"
Warnings When Starting hadoop,HDFS-7975,,,,,"When i am starting hadoop getting some warnings

how to ramove them ?/

hduser@sajid:~$ start-all.sh 
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
15/03/23 20:06:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-sajid.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-sajid.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-sajid.out
15/03/23 20:07:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-sajid.out
Java HotSpot(TM) 64-Bit Server VM warning: You have loaded library /usr/local/hadoop/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-sajid.out
"
getStoragePolicy() regards HOT policy as EC policy,HDFS-7981,,,,,"Now, {{testStoragePoliciesCK()}} in {{TestFsck}} is failed in EC branch.

A part of the test result is below:
{noformat}
A part of the test result is odd:
Blocks NOT satisfying the specified storage policy:
Storage Policy                  Specified Storage Policy      # of blocks       % of blocks
DISK:3(EC)                          HOT                           1              33.3333%
{noformat}

I found that {{getStoragePolicy(StorageType[] storageTypes)}} in {{StoragePolicySummary}} regarding HOT policy as EC policy. We should fix this problem.
"
Report removed storages after removing them by DataNode#checkDirs(),HDFS-8006,,,,,"Similar to HDFS-7961,  after DN removes storages due to disk errors (HDFS-7722), DN should send a full block report to NN to remove storages (HDFS-7960)

"
HTTP request queue size limit can be made configurable,HDFS-7992,,,,,"The queue size for httpserver is hardcoded as 128 in 

{code}
  public static Connector createDefaultChannelConnector() {
    SelectChannelConnector ret = new SelectChannelConnector();
    ret.setLowResourceMaxIdleTime(10000);
    ret.setAcceptQueueSize(128);
{code}

It will be better if this can made configurable

In the connection can add a configurable limit of queue size for the http request (鈥渉adoop.http.max.queue.size鈥?."
WebHDFS cannot open file,HDFS-6496,namenode,,,,WebHDFS cannot open the file on the name node web UI. I attched screen.
"Unifying HA support in HftpFileSystem, HsftpFileSystem and WebHdfsFileSystem",HDFS-5193,,,,,"Recent changes in HDFS-5122 implement the HA support for the WebHDFS client. 
Similar to WebHDFS client, both HftpFileSystem and HsftpFilesystem access HDFS via HTTP, but their current implementation hinders the implementation of HA support.

I propose to refactor HftpFileSystem, HsftpFileSystem, and WebHdfsFileSystem to provide unified abstractions to support HA cluster over HTTP."
Initial refactoring to allow ConsensusNode implementation,HDFS-6940,namenode,,,,Minor refactoring of FSNamesystem to open private methods that are needed for CNode implementation.
Enable journal protocol based editlog streaming for standby namenode,HDFS-3092,ha,namenode,,,"Currently standby namenode relies on reading shared editlogs to stay current with the active namenode, for namespace changes. BackupNode used streaming edits from active namenode for doing the same. This jira is to explore using journal protocol based editlog streams for the standby namenode. A daemon in standby will get the editlogs from the active and write it to local edits. To begin with, the existing standby mechanism of reading from a file, will continue to be used, instead of from shared edits, from the local edits."
HA: Support multiple shared edits dirs,HDFS-2782,ha,,,,Supporting multiple shared dirs will improve availability (eg see HDFS-2769). You may want to use multiple shared dirs on a single filer (eg for better fault isolation) or because you want to use multiple filers/mounts. Per HDFS-2752 (and HDFS-2735) we need to do things like use the JournalSet in EditLogTailer and add tests.
Add tests for Namenode active standby states,HDFS-2394,ha,namenode,test,,
Namenode format should not create the storage directory if it doesn't exist ,HDFS-3095,namenode,,,,"The storage directory can be a mount point. 
Automatically creating the mount point could be problematic. "
Balancer tests failing in trunk,HDFS-2721,test,,,,"Looks Balancer tests started failing.....
https://builds.apache.org/job/Hadoop-Hdfs-trunk/lastCompletedBuild/testReport/
Also seen timing out of TestBalancer in precommit build https://builds.apache.org/job/PreCommit-HDFS-Build/1737//console"
move webapps/ into the JAR,HDFS-3011,build,datanode,namenode,,"Currently the webapps dir is in the filesystem and added to the classpath.

As effectively it is picked up from the classpath, we could move into the component JAR itself.
"
hadoop distcp hftp://192.168.80.31:50070/user/wp hdfs://192.168.210.10:8020/,HDFS-7605,distcp,,,,"Error: java.io.IOException: File copy failed: hftp://192.168.80.31:50070/user/wp/test.txt --> hdfs://192.168.210.10:8020/wp/test.txt
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hftp://192.168.80.31:50070/user/wp/test.txt to hdfs://192.168.210.10:8020/wp/test.txt
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.net.SocketTimeoutException: connect timed out
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:303)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:248)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:529)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:158)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:411)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:525)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:208)
	at sun.net.www.http.HttpClient.New(HttpClient.java:291)
	at sun.net.www.http.HttpClient.New(HttpClient.java:310)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:987)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:923)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:841)
	at sun.net.www.protocol.http.HttpURLConnection.followRedirect(HttpURLConnection.java:2156)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1390)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderUrlOpener.connect(HftpFileSystem.java:370)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:383)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:388)
	at org.apache.hadoop.hdfs.web.HftpFileSystem.open(HftpFileSystem.java:404)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:299)
	... 16 more

15/01/12 18:04:06 INFO mapreduce.Job:  map 67% reduce 0%
15/01/12 18:04:06 INFO mapreduce.Job: Task Id : attempt_1420685403662_0029_m_000001_0, Status : FAILED
Error: java.io.IOException: File copy failed: hftp://192.168.80.31:50070/user/wp/t.txt --> hdfs://192.168.210.10:8020/wp/t.txt
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hftp://192.168.80.31:50070/user/wp/t.txt to hdfs://192.168.210.10:8020/wp/t.txt
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.net.SocketTimeoutException: connect timed out
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:303)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:248)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:529)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:158)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:411)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:525)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:208)
	at sun.net.www.http.HttpClient.New(HttpClient.java:291)
	at sun.net.www.http.HttpClient.New(HttpClient.java:310)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:987)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:923)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:841)
	at sun.net.www.protocol.http.HttpURLConnection.followRedirect(HttpURLConnection.java:2156)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1390)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderUrlOpener.connect(HftpFileSystem.java:370)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:383)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:388)
	at org.apache.hadoop.hdfs.web.HftpFileSystem.open(HftpFileSystem.java:404)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:299)
	... 16 more"
Null pointer exception comes when Namenode recovery happens and there is no response from client to NN more than the hardlimit for NN recovery and the current block is more than the prev block size in NN ,HDFS-1951,namenode,,,,"Null pointer exception comes when Namenode recovery happens and there is no response from client to NN more than the hardlimit for NN recovery and the current block is more than the prev block size in NN 
1. Write using a client to 2 datanodes
2. Kill one data node and allow pipeline recovery.
3. write somemore data to the same block
4. Parallely allow the namenode recovery to happen
Null pointer exception will come in addStoreBlock api.


 

"
tomcat tar in the apache archive is corrupted,HDFS-2723,,,,,"when running mvn package , getting the following error and hence not able to create tarball

{noformat}
    [mkdir] Created dir: /root/ravi/mapred/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/tomcat.exp
     [exec] Current OS is Linux
     [exec] Executing 'sh' with arguments:
     [exec] './tomcat-untar.sh'
     [exec] 
     [exec] The ' characters around the executable and arguments are
     [exec] not part of the command.
Execute:Java13CommandLauncher: Executing 'sh' with arguments:
'./tomcat-untar.sh'

The ' characters around the executable and arguments are
not part of the command.
     [exec] 
     [exec] gzip: stdin: unexpected end of file
     [exec] tar: Unexpected EOF in archive
     [exec] tar: Unexpected EOF in archive
     [exec] tar: Error is not recoverable: exiting now
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................ SUCCESS [7.984s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [3.028s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [5.451s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [2.987s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [13.675s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [5.766s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [6.258s]
[INFO] Apache Hadoop Common .............................. SUCCESS [3:50.945s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.539s]
[INFO] Apache Hadoop HDFS ................................ SUCCESS [3:01.761s]
[INFO] Apache Hadoop HttpFS .............................. FAILURE [30.532s]

{noformat}

It is because the tomcat tarball available in ""http://archive.apache.org/dist/tomcat/tomcat-6/v6.0.32/bin/apache-tomcat-6.0.32.tar.gz"" is corrupted.
Getting ""Unexpected End of Archive"" when trying to untar this tarball.
"
"Use TestDFSIO to test HDFS, and Failed with the exception: All datanodes are bad. Aborting...",HDFS-2774,test,,,,"use TestDFSIO to test the HDFS
use the commond:  hadoop jar ....TestDFSIO - write -nrFiles 10 -fileSize 500
when running ,errors occurs:
12/01/09 16:00:45 INFO mapred.JobClient: Task Id : attempt_201201091556_0001_m_000006_2, Status : FAILED
java.io.IOException: All datanodes 192.168.0.17:50010 are bad. Aborting...
 at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2556)
 at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1600(DFSClient.java:2102)
 at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2265)
attempt_201201091637_0002_m_000005_0: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).
attempt_201201091637_0002_m_000005_0: log4j:WARN Please initialize the log4j system properly.

I don't know why?"
New architectural documentation created,HDFS-1961,documentation,,,,"This material provides an overview of the HDFS architecture and is intended for contributors. The goal of this document is to provide a guide to the overall structure of the HDFS code so that contributors can more effectively understand how changes that they are considering can be made, and the consequences of those changes. The assumption is that the reader has a basic understanding of HDFS, its purpose, and how it fits into the Hadoop project suite. 

An HTML version of the architectural documentation can be found at:  http://kazman.shidler.hawaii.edu/ArchDoc.html

All comments and suggestions for improvements are appreciated."
Null pointer exception is thrown when NN restarts with a block lesser in size than the block that is present in DN1 but the generation stamp is greater in the NN ,HDFS-1982,namenode,,,,"Conisder the following scenario. 
WE have a cluster with one NN and 2 DN.

We write some file.

One of the block is written in DN1 but not yet completed in DN2 local disk.

Now DN1 gets killed and so pipeline recovery happens for the block with the size as in DN2 but the generation stamp gets updated in the NN.

DN2 also gets killed.

Now restart NN and DN1
Now if NN restarts, the block that NN has greater time stamp but the size is lesser in the NN.

This leads to Null pointer exception in addstoredblock api


"
Improvements to HDFS-1204 test,HDFS-1247,test,,,,The test from HDFS-1204 currently generates some warnings when compiling. Here's a small patch to clean up the test.
Hftp file read should retry a different datanode if the chosen best datanode fails to connect to NameNode,HDFS-1553,namenode,,,,"Currently when reading a file through HftpFileSystem interface, namenode deterministically selects the ""best"" datanode from which the file is read. But this can cause the read to fail if the best datanode fails to connect to namenode because it never tries another datanode.

The proposed solution is to send a list of datanode candidates when namenode redirects the read request to the chosen best datanode. The the datanode could redirect the request to the next good datanode when it fails to connect to namenode."
hftp only supports remote hdfs servers,HDFS-2336,,,,,"The new token renewal implementation appears to introduce invalid assumptions regarding token kinds.

Any token acquired over http is assumed to be hftp, so the kind is unconditionally changed to hftp.  This precludes the acquisition of any other token types over http.  This new limitation was added to a generic method in a public class.  It should have been encapsulated in the hftp class, not the generic http token fetching methods.

Furthermore, hftp will unconditionally change a hftp token's kind to hdfs.  I believe this assumption means that hftp is now broken if the remote cluster's default filesystem is not hdfs."
Update HDFS dependency of Java for deb package,HDFS-2192,build,,,,"Java dependency for Debian package is specified as open JDK, but it should depends on Sun version of Java. This dependency can be implicitly defined by hadoop-common dependency. Hence, there is no need to explicitly defined in hadoop-hdfs."
Avoid duplicate entries in Block Scan verification logs ,HDFS-6111,datanode,,,,"Duplicate entries could be generated in datanode verification logs if the datanode gets shutdown during the rolling of scan verification logs.
If this happens multiple times in huge cluster, then size of the verification logs could be huge.

Avoid these duplicate entries in the next roll."
New blocks scanning will be delayed due to issue in BlockPoolSliceScanner#updateBytesToScan(..),HDFS-6147,datanode,,,,"New blocks scanning will be delayed if old blocks deleted after datanode restart.

Steps:
1. Write some blocks and wait till all scans over
2. Restart the datanode
3. Delete some of the blocks
4. Write new blocks which are less in size compared to deleted blocks.

Problem:
{{BlockPoolSliceScanner#updateBytesToScan(..)}} updates {{bytesLeft}} based on following comparison
{code}   if (lastScanTime < currentPeriodStart) {
      bytesLeft += len;
    }{code}

But in {{BlockPoolSliceScanner#assignInitialVerificationTimes()}} {{bytesLeft}} decremented using below comparison
{code}if (now - entry.verificationTime < scanPeriod) {{code}

Hence when the old blocks are deleted {{bytesLeft}} going negative.
new blocks will not be scanned until it becomes positive again.

So in both places verificationtime should be compared against scanperiod."
TestIncrementalBlockReports#testReplaceReceivedBlock fails occasionally in trunk,HDFS-6037,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1688/testReport/junit/org.apache.hadoop.hdfs.server.datanode/TestIncrementalBlockReports/testReplaceReceivedBlock/ :
{code}
datanodeProtocolClientSideTranslatorPB.blockReceivedAndDeleted(
    <any>,
    <any>,
    <any>
);
Wanted 1 time:
-> at org.apache.hadoop.hdfs.server.datanode.TestIncrementalBlockReports.testReplaceReceivedBlock(TestIncrementalBlockReports.java:198)
But was 2 times. Undesired invocation:
-> at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportReceivedDeletedBlocks(BPServiceActor.java:303)
{code}"
Wire-encription in QJM,HDFS-5688,ha,journal-node,security,,"When HA is implemented with QJM and using kerberos, it's not possible to set wire-encrypted data.
If it's set property hadoop.rpc.protection to something different to authentication it doesn't work propertly, getting the error:

ERROR security.UserGroupInformation: PriviledgedActionException as:principal@REALM (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and ser

With NFS as shared storage everything works like a charm"
TestRetryCacheWithHA#testCreateSymlink occasionally fails in trunk,HDFS-6081,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1696/testReport/junit/org.apache.hadoop.hdfs.server.namenode.ha/TestRetryCacheWithHA/testCreateSymlink/ :
{code}
2014-03-09 13:18:47,515 WARN  security.UserGroupInformation (UserGroupInformation.java:doAs(1600)) - PriviledgedActionException as:jenkins (auth:SIMPLE) cause:java.io.IOException: failed to create link /testlink either because the filename is invalid or the file exists
2014-03-09 13:18:47,515 INFO  ipc.Server (Server.java:run(2093)) - IPC Server handler 0 on 39303, call org.apache.hadoop.hdfs.protocol.ClientProtocol.createSymlink from 127.0.0.1:32909 Call#682 Retry#1: error: java.io.IOException: failed to create link /testlink either because the filename is invalid or the file exists
java.io.IOException: failed to create link /testlink either because the filename is invalid or the file exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlinkInt(FSNamesystem.java:2053)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlink(FSNamesystem.java:2023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSymlink(NameNodeRpcServer.java:965)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.createSymlink(ClientNamenodeProtocolServerSideTranslatorPB.java:844)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2071)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2067)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1597)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2065)
2014-03-09 13:18:47,522 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2475)) - Total number of blocks            = 1
2014-03-09 13:18:47,523 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2476)) - Number of invalid blocks          = 0
2014-03-09 13:18:47,523 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2477)) - Number of under-replicated blocks = 0
2014-03-09 13:18:47,523 INFO  ha.TestRetryCacheWithHA (TestRetryCacheWithHA.java:run(1162)) - Got Exception while calling createSymlink
org.apache.hadoop.ipc.RemoteException(java.io.IOException): failed to create link /testlink either because the filename is invalid or the file exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlinkInt(FSNamesystem.java:2053)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlink(FSNamesystem.java:2023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSymlink(NameNodeRpcServer.java:965)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.createSymlink(ClientNamenodeProtocolServerSideTranslatorPB.java:844)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2071)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2067)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1597)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2065)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at $Proxy17.createSymlink(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.createSymlink(ClientNamenodeProtocolTranslatorPB.java:794)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:189)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA$DummyRetryInvocationHandler.invokeMethod(TestRetryCacheWithHA.java:114)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at $Proxy18.createSymlink(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.createSymlink(DFSClient.java:1507)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA$CreateSymlinkOp.invoke(TestRetryCacheWithHA.java:658)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA$1.run(TestRetryCacheWithHA.java:1154)
{code}
In a successful run, I don't see the above error."
FSImage layout version should be only once file is complete,HDFS-957,namenode,,,,"Right now, the FSImage save code writes the LAYOUT_VERSION at the head of the file, along with some other headers, and then dumps the directory into the file. Instead, it should write a special IMAGE_IN_PROGRESS entry for the layout version, dump all of the data, then seek back to the head of the file to write the proper LAYOUT_VERSION. This would make it very easy to detect the case where the FSImage save got interrupted."
Warm HA NameNode going Hot,HDFS-2064,namenode,,,,"This is the design for automatic hot HA for HDFS NameNode. It involves use of HA software and LoadReplicator - external to Hadoop components, which substantially simplify the architecture by separating HA- from Hadoop-specific problems. Without the external components it provides warm standby with manual failover."
Datanode blockId layout upgrade threads should be daemon thread,HDFS-7666,datanode,,,,"This jira is to mark the layout upgrade thread as daemon thread.

{code}
     int numLinkWorkers = datanode.getConf().getInt(
         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,
         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);
    ExecutorService linkWorkers = Executors.newFixedThreadPool(numLinkWorkers);
{code}"
"TestEditLog assumes that FSNamesystem.getFSNamesystem().dir is non-null, even after the FSNameSystem is closed",HDFS-135,,,,,"In my modified services, I'm setting {{FSNameSystem.dir}} to {{null}} on {{close()}}:
{code}
        if(dir != null) {
         dir.close();
         dir =  null;
        }
{code}

This breaks TestEditLog
{code}
java.lang.NullPointerException
at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:620)
at org.apache.hadoop.hdfs.server.namenode.TestEditLog.testEditLog(TestEditLog.java:148)
{code}

There are two possible conclusions here. 
# Setting dir=null in {{FSNameSystem.close()}} is a regression and should be fixed
# The test contains some assumptions that are not valid

I will leave it to others to decide; I will try and fix the code whichever approach is chosen. Personally, I'd go for setting dir=null as it is cleaner, but there is clearly some risk of backward's compatibility problems, at least in test code
"
Not all datanodes are displayed on the namenode http tab,HDFS-7117,namenode,,,,"On a single machine, I have three ""fake nodes"" (each node use different dfs.datanode.address, dfs.datanode.ipc.address, dfs.datanode.http.address)

- node1 starts the namenode and a datanode
- node2 starts a datanode
- node3 starts a datanode

In the namenode http console, on the overview, I can see 3 live nodes:

{code}
http://localhost:50070/dfshealth.html#tab-overview
{code}

but, when clicking on the ""Live Nodes"":

{code}
http://localhost:50070/dfshealth.html#tab-datanode
{code}

I can see only one node row."
Verify initializations of LocatedBlock/RecoveringBlock,HDFS-5423,datanode,namenode,,,"Tracking Jira to make sure we verify initialization of LocatedBlock and RecoveringBlock, possibly reorg the constructors to make missing initialization of StorageIDs less likely."
NN -> JN communication should use reusable authentication methods,HDFS-7580,journal-node,namenode,,,"It appears that NNs talk to JNs via general SaslRPC in secure mode, causing all requests to be carried out with a kerberos authentication. This can cause delays and occasionally NN failures if the KDC used does not respond in its default timeout period (30s, whereas the QJM writes come with default of 20s)."
HDFS dangerously uses @Beta methods from very old versions of Guava,HDFS-7040,,,,,"HDFS uses LimitInputStream from Guava. This was introduced as @Beta and is risky for any application to use.

The problem is further exacerbated by Hadoop's dependency on Guava version 11.0.2, which is quite old for an active project (Feb. 2012).

Because Guava is very stable, projects which depend on Hadoop and use Guava themselves, can use up through Guava version 14.x

However, in version 14, Guava deprecated LimitInputStream and provided a replacement. Because they make no guarantees about compatibility about @Beta classes, they removed it in version 15.

What should be done: Hadoop should updated its dependency on Guava to at least version 14 (currently Guava is on version 19). This should have little impact on users, because Guava is so stable.

HDFS should then be patched to use the provided alternative to LimitInputStream, so that downstream packagers, users, and application developers requiring more recent versions of Guava (to fix bugs, to use new features, etc.) will be able to swap out the Guava dependency without breaking Hadoop.

Alternative: While Hadoop cannot predict the marking and removal of deprecated code, it can, and should, avoid the use of @Beta classes and methods that do not offer guarantees. If the dependency cannot be bumped, then it should be relatively trivial to provide an internal class with the same functionality, that does not rely on the older version of Guava."
start-dfs.sh does not start remote DataNode due to escape characters,HDFS-6239,scripts,,,,"start-dfs.sh fails to start remote data nodes and task nodes, though it is possible to start them manually through hadoop-daemon.sh.

I've been able to debug and find the root cause the bug, and I thought it was a trivial fix, but I do not know how to do it. Can't figure out a way to handle this seemingly trivial bug.

hadoop-daemons.sh calls slave.sh:

exec ""$bin/slaves.sh"" --config $HADOOP_CONF_DIR cd ""$HADOOP_HOME"" \; ""$bin/hadoop-daemon.sh"" --config $HADOOP_CONF_DIR ""$@""

This is the issue when I debug using bash -x: In slaves.sh, the \; becomes ';'

+ ssh xxxx.xx.xxxx.xxx cd /afs/xx.xxxx.xxx/x/x/x/xx/xxxxx/libexec/.. ';' /afs/xx.xxxx.xxx/x/x/x/xx/xxxx/bin/hadoop-daemon.sh --config /afs/xx.xxxx.xxx/x/x/x/xx/xxxx/libexec/../conf start datanode

The problem is ';' . Because the semi-colon is surrounded by quotes, it doesn't execute the code after that. I manually ran the above command, and as expected the data node did not start. When I removed the quotes around the semi-colon, everything works. Please note that you can see the issue only when you do bash -x. If you echo the statement, the quotes around the semi-colon are not visible.

This issue is always reproducible for me, and because of it, I have to manually start daemons on each machine. "
we should add a wait for non-safe mode and call dfsadmin -report in start-dfs,HDFS-2256,scripts,,,,I think we should add a call to wait for safe mode exit and print the dfs report to show upgrades that are in progress.
NameNode: implement Global ACL Set as a memory optimization.,HDFS-5620,namenode,,,,The {{AclManager}} can maintain a Global ACL Set to store all distinct ACLs in use by the file system.  All inodes that have the same ACL entries can share the same ACL instance.
Disable check for jsvc on windows,HDFS-3749,datanode,,,,Jsvc doesn't make sense on windows and thus we should not require the datanode to start up under it on that platform.
Remove unused TokenRenewer implementation from WebHdfsFileSystem and HftpFileSystem,HDFS-4010,,,,,"WebHdfsFileSystem and HftpFileSystem implement TokenRenewer without using anywhere.

As we are in the process of migrating them to not use tokens, this code should be removed."
WebHdfsFileSystem and HftpFileSystem don't need delegation tokens,HDFS-4009,,,,,"Parent JIRA to track the work of removing delegation tokens from these filesystems. 

This JIRA has evolved from the initial issue of these filesystems not stopping the DelegationTokenRenewer thread they were creating.

After further investigation, Daryn pointed out - ""If you can get a token, you don't need a token""! Hence, these filesystems shouldn't use delegation tokens.

Evolution of the JIRA is listed below:
Update 2:
DelegationTokenRenewer is not required. The filesystems that are using it already have Krb tickets and do not need tokens. Remove DelegationTokenRenewer and all the related logic from WebHdfs and Hftp filesystems.

Update1:
DelegationTokenRenewer should be Singleton - the instance and renewer threads should be created/started lazily. The filesystems using the renewer shouldn't need to explicity start/stop the renewer, and only register/de-register for token renewal.

Initial issue:
HftpFileSystem and WebHdfsFileSystem should stop the DelegationTokenRenewer thread when they are closed. "
Add configurable maximum block count for datanode,HDFS-6088,,,,,"Currently datanode resources are protected by the free space check and the balancer.  But datanodes can run out of memory simply storing too many blocks. If the sizes of blocks are small, datanodes will appear to have plenty of space to put more blocks.

I propose adding a configurable max block count to datanode. Since datanodes can have different heap configurations, it will make sense to make it datanode-level, rather than something enforced by namenode."
getBlockLocationsUpdateTimes missing handle exception may cause fsLock dead lock,HDFS-7253,namenode,,,,"One day my active namenode hanged and I dumped the program stacks by jstack.In the stacks file, I saw most threads were waiting FSNamesystem.fsLock, both  readLock and writeLock were unacquirable, but no thread was holding writeLock.
I tried to access the web interface of this namenode but was blocked. and I tried to failover the active node to another namenode manually (zkfs did not discover this node was hanging) but it was also failed. So I killed this namenode trying to recover the production environment, then the failover was triggered, standby nn transited to active, and then, the new active namenode hanged.
My following steps are useless and can be ignored. At last, I thought it was caused by an incorrect lock handling in FSNamesystem.getBlockLocationsUpdateTimes, which I will describe in the first comment.
"
TestWebHdfsFileSystemContract fails occassionally,HDFS-7070,webhdfs,,,,"org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract.testResponseCode
and  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract.testRenameDirToSelf 
failed recently.

Need to determine whether it's  introduced by some latest code change due to file descriptor leak; or it's a similar issue as HDFS-6694 reported.


E.g. https://builds.apache.org/job/PreCommit-HDFS-Build/8026/testReport/org.apache.hadoop.hdfs.web/TestWebHdfsFileSystemContract/testResponseCode/.

{code}
2014-09-15 12:52:18,866 INFO  datanode.DataNode (DataXceiver.java:writeBlock(749)) - opWriteBlock BP-23833599-67.195.81.147-1410785517350:blk_1073741827_1461 received exception java.io.IOException: Cannot run program ""stat"": java.io.IOException: error=24, Too many open files
2014-09-15 12:52:18,867 ERROR datanode.DataNode (DataXceiver.java:run(243)) - 127.0.0.1:47221:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38112 dst: /127.0.0.1:47221
java.io.IOException: Cannot run program ""stat"": java.io.IOException: error=24, Too many open files
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:470)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:485)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)
	at org.apache.hadoop.fs.HardLink.getLinkCount(HardLink.java:495)
	at org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.unlinkBlock(ReplicaInfo.java:288)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:702)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:680)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:101)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:193)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:604)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:126)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:72)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: java.io.IOException: error=24, Too many open files
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
	... 14 more
2014-09-15 12:52:18,867 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1400)) - Exception in createBlockOutputStream
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2101)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1210)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:530)
2014-09-15 12:52:18,870 WARN  hdfs.DFSClient (DFSOutputStream.java:run(883)) - DFSOutputStream ResponseProcessor exception  for block BP-23833599-67.195.81.147-1410785517350:blk_1073741827_1461
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2099)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:798)
2014-09-15 12:52:18,870 WARN  hdfs.DFSClient (DFSOutputStream.java:run(627)) - DataStreamer Exception
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DFSOutputStream$Packet.writeTo(DFSOutputStream.java:273)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:579)
{code}


"
Tests fail on windows due to BindException.,HDFS-11,,,,,"On Windows a series of tests fail starting from {{TestAbandonBlock}}.
The reason is that name-nodes or data-nodes cannot start because of the ""BindException: Address already in use: connect"".
Seems like some of the tests that run before {{TestAbandonBlock}} are not releasing resources or not closing sockets."
setLowResourceMaxIdleTime(10000) in HttpServer2 is dead,HDFS-7234,,,,,"In HttpSever2.java, the code call {{ret.setLowResourceMaxIdleTime(10000);}} but it does not set the {{LowResourceConnections}} in jetty, thus the setting becomes useless."
Fix incorrect layout version caused by bad merge,HDFS-7160,namenode,,,,The layout version was not correctly updated while merging from trunk.
Implement compression in the HTTP server of SNN / SBN instead of FSImage,HDFS-5722,,,,,"The current FSImage format support compression, there is a field in the header which specifies the compression codec used to compress the data in the image. The main motivation was to reduce the number of bytes to be transferred between SNN / SBN / NN.

The main disadvantage, however, is that it requires the client to access the FSImage in strictly sequential order. This might not fit well with the new design of FSImage. For example, serializing the data in protobuf allows the client to quickly skip data that it does not understand. The compression built-in the format, however, complicates the calculation of offsets and lengths. Recovering from a corrupted, compressed FSImage is also non-trivial as off-the-shelf tools like bzip2recover is inapplicable.

This jira proposes to move the compression from the format of the FSImage to the transport layer, namely, the HTTP server of SNN / SBN. This design simplifies the format of FSImage, opens up the opportunity to quickly navigate through the FSImage, and eases the process of recovery. It also retains the benefits of reducing the number of bytes to be transferred across the wire since there are compression on the transport layer.

"
the shell script error for Cygwin on windows7,HDFS-4198,scripts,,,,See the following [comment|https://issues.apache.org/jira/browse/HDFS-4198?focusedCommentId=13498818&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13498818] for detailed description.
Can namenode recover from a edit log file?,HDFS-7043,auto-failover,,,,"I have delete all the data dir on the namenode,causing lost of all the data.
I can only recover a edit.new file from the disk, whtich is quite huge(12GB),
Maybe most of recent operation is on the edit log
Is it possible to recover data from this edit log file?

Try to add all the file or direcotry on every edit log to recover the filesystem?

anyone could help?
many thx"
Refactor Raid so it can work better with new erasure codes,HDFS-3543,contrib/raid,,,,"Refactor Raid so it can work with
https://issues.apache.org/jira/browse/MAPREDUCE-3361"
Path#makeQualified copies authority when scheme does not match,HDFS-7031,hdfs-client,,,,"I have an application that calls {{makeQualified}} that amounts to this:

{code:java}
new Path(""file:/some/local/path"").makeQualified(
    URI.create(""hdfs://nn:8020""), new Path(""/""));
{code}

This unexpectedly produces {{file://nn:8020/some/local/path}}, using the authority section from the default URI even though the path that is being qualified doesn't have a scheme that matches the default URI.

In {{Path}}, there is a check to see if the default URI should be used:
{code:java}
    if (scheme != null &&
        (authority != null || defaultUri.getAuthority() == null))
      return path;
{code}

I think this should be:
{code:java}
    // if the scheme matches and there is no authority, use the default
    if (scheme != null && scheme.equals(defaultUri.getScheme()) &&
        (authority != null || defaultUri.getAuthority() == null))
      return path;
{code}
"
Stub implementation of getrlimit for Windows.,HDFS-5204,,,,,The HDFS-4949 feature branch adds a JNI wrapper over the {{getrlimit}} function.  This function does not exist on Windows.  We need to provide a stub implementation so that the codebase can compile on Windows.
Update datanode replacement policy to make writes more robust,HDFS-6016,datanode,ha,hdfs-client,namenode,"As discussed in HDFS-5924, writers that are down to only one node due to node failures can suffer if a DN does not restart in time. We do not worry about writes that began with single replica. "
libhdfs with gdb got SEGV,HDFS-866,libhdfs,,,,"I'm now trying to integrate HDFS into our server programs using libhdfs.

I found that hdfs_write and hdfs_read (sample program in libhdfs directory) got segv with the gdb execution.
Without gdb, it runs OK. Valgrind also outputs nothing.

By printf debug, the SEGV seems to be occurred in invokeMethod() function in hdfsRead() and hdfsWrite().

Following is the information.

* Code
IHDFS trunk code (revision  894610)

* Java
java version ""1.6.0_16""
Java(TM) SE Runtime Environment (build 1.6.0_16-b01)
Java HotSpot(TM) 64-Bit Server VM (build 14.2-b01, mixed mode)

* Kernel
Linux pfisedueindexer 2.6.28-14-generic #47-Ubuntu SMP Sat Jul 25 01:19:55 UTC 2009 x86_64 GNU/Linux

* GCC
4.3.3

* GDB
GNU gdb 6.8-debian

* libhdfs
CFLAGS=""-g -D_REENTRANT""
LDFLAGS=""-g""

* gdb log
{quote}
kzk@pfisedueindexer:~/apacheprojects/hdfs/src/c++/libhdfs$ gdb ./hdfs_write 
GNU gdb 6.8-debian
Copyright (C) 2008 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu""...
(gdb) r testfile 100000000 1000000
Starting program: /home/kzk/apacheprojects/hdfs/src/c++/libhdfs/hdfs_write testfile 100000000 1000000
[Thread debugging using libthread_db enabled]
[New Thread 0x7fa8971f36f0 (LWP 10490)]
[New Thread 0x7fa7d241f950 (LWP 10493)]
[New Thread 0x7fa7d231e950 (LWP 10494)]
[New Thread 0x7fa7d221d950 (LWP 10495)]
[New Thread 0x7fa7d211c950 (LWP 10496)]
[New Thread 0x7fa7d201b950 (LWP 10497)]
[New Thread 0x7fa7d1f1a950 (LWP 10498)]
[New Thread 0x7fa7d1e19950 (LWP 10499)]
[New Thread 0x7fa7d1d18950 (LWP 10500)]
[New Thread 0x7fa7d193f950 (LWP 10501)]
[New Thread 0x7fa7d183e950 (LWP 10502)]
[New Thread 0x7fa7d173d950 (LWP 10503)]
[New Thread 0x7fa7d15bb950 (LWP 10504)]
[New Thread 0x7fa7d14ba950 (LWP 10505)]
[New Thread 0x7fa7d13b9950 (LWP 10506)]
[New Thread 0x7fa7d12b8950 (LWP 10507)]
[New Thread 0x7fa7d11b7950 (LWP 10508)]
[New Thread 0x7fa7d0ffc950 (LWP 10509)]
[Thread 0x7fa7d0ffc950 (LWP 10509) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10511)]
[Thread 0x7fa7d0ffc950 (LWP 10511) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10512)]
[New Thread 0x7fa7d0efb950 (LWP 10514)]
[Thread 0x7fa7d0ffc950 (LWP 10512) exited]
[Thread 0x7fa7d0efb950 (LWP 10514) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10515)]
[Thread 0x7fa7d0ffc950 (LWP 10515) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10517)]
[Thread 0x7fa7d0ffc950 (LWP 10517) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10518)]
[New Thread 0x7fa7d0efb950 (LWP 10522)]
[New Thread 0x7fa7d0acd950 (LWP 10523)]
[New Thread 0x7fa7d09cc950 (LWP 10524)]
[New Thread 0x7fa7d07d6950 (LWP 10527)]
[Thread 0x7fa7d07d6950 (LWP 10527) exited]
[New Thread 0x7fa7d07d6950 (LWP 10530)]

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fa7d07d6950 (LWP 10530)]
0x00007fa891cb45d4 in ?? ()
(gdb) bt
#0  0x00007fa891cb45d4 in ?? ()
#1  0x00007fa856dc5c08 in ?? ()
#2  0x00007fa7d2f0d630 in ?? ()
#3  0x0000000000000000 in ?? ()


(gdb) thread apply all bt
Thread 29 (Thread 0x7fa7d07d6950 (LWP 10530)):
#0  0x00007fa891cb45d4 in ?? ()
#1  0x00007fa856dc5c08 in ?? ()
#2  0x00007fa7d2f0d630 in ?? ()
#3  0x0000000000000000 in ?? ()

Thread 27 (Thread 0x7fa7d09cc950 (LWP 10524)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896960bbf in os::sleep () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa8967f0028 in JVM_Sleep () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa891c34f50 in ?? ()
#5  0x00000125dfcbbb3b in ?? ()
#6  0x00007fa8967da917 in JVM_CurrentTimeMillis () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa891c29a22 in ?? ()
#8  0x00007fa7d2e673f0 in ?? ()
#9  0x00007fa891c31a58 in ?? ()
#10 0x00000000000003e8 in ?? ()
#11 0x00007fa891c31a57 in ?? ()
#12 0x00007fa7d09cba58 in ?? ()
#13 0x00007fa7d350860f in ?? ()
#14 0x00007fa7d09cbac0 in ?? ()
#15 0x00007fa7d3508c90 in ?? ()
#16 0x0000000000000000 in ?? ()

Thread 26 (Thread 0x7fa7d0acd950 (LWP 10523)):
#0  0x00007fa895fb9e5b in write () from /lib/libpthread.so.0
#1  0x00007fa7d0ad2470 in Java_sun_nio_ch_FileDispatcher_write0 () from /usr/lib/jvm/java-6-sun-1.6.0.16/jre/lib/amd64/libnio.so
#2  0x00007fa891c34f50 in ?? ()
#3  0x00007fa7d0acc4d0 in ?? ()
#4  0x0000000000000000 in ?? ()

Thread 25 (Thread 0x7fa7d0efb950 (LWP 10522)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a17212 in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x00000125dfcbbdbb in ?? ()
#7  0x00007fa8967da917 in JVM_CurrentTimeMillis () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#8  0x00007fa891c29a22 in ?? ()
#9  0x0000000000000000 in ?? ()
---Type <return> to continue, or q <return> to quit---

Thread 24 (Thread 0x7fa7d0ffc950 (LWP 10518)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a17212 in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x0000000001d64380 in ?? ()
#7  0x0000000001d64758 in ?? ()
#8  0x00007fa7c40f3000 in ?? ()
#9  0x00007fa891c3541d in ?? ()
#10 0x00007fa7d0ffb950 in ?? ()
#11 0x0000000000000000 in ?? ()

Thread 17 (Thread 0x7fa7d11b7950 (LWP 10508)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896960d3d in os::sleep () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a46e3b in WatcherThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#6  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#7  0x0000000000000000 in ?? ()

Thread 16 (Thread 0x7fa7d12b8950 (LWP 10507)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8968fdbf2 in LowMemoryDetector::low_memory_detector_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 15 (Thread 0x7fa7d13b9950 (LWP 10506)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c0fa in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896650de3 in CompileQueue::get () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896652882 in CompileBroker::compiler_thread_loop () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
---Type <return> to continue, or q <return> to quit---
#6  0x00007fa896a4e1e9 in compiler_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#8  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#9  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#10 0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#11 0x0000000000000000 in ?? ()

Thread 14 (Thread 0x7fa7d14ba950 (LWP 10505)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c0fa in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896650de3 in CompileQueue::get () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896652882 in CompileBroker::compiler_thread_loop () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa896a4e1e9 in compiler_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#8  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#9  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#10 0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#11 0x0000000000000000 in ?? ()

Thread 13 (Thread 0x7fa7d15bb950 (LWP 10504)):
#0  0x00007fa895fb9281 in sem_wait () from /lib/libpthread.so.0
#1  0x00007fa896963635 in check_pending_signals () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89695c537 in signal_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#6  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#7  0x0000000000000000 in ?? ()

Thread 12 (Thread 0x7fa7d173d950 (LWP 10503)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a175da in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x00007fa7d173cb68 in ?? ()
#7  0x00007fa7cc001000 in ?? ()
#8  0x00007fa856cf03b8 in ?? ()
#9  0x00007fa7d173cb58 in ?? ()
#10 0x00007fa7d173cae0 in ?? ()
#11 0x0000000000000000 in ?? ()
---Type <return> to continue, or q <return> to quit---

Thread 11 (Thread 0x7fa7d183e950 (LWP 10502)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a175da in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x00007fa7d183db18 in ?? ()
#7  0x0000000001e17000 in ?? ()
#8  0x00007fa7d183d9f0 in ?? ()
#9  0x00007fa7d183d9c8 in ?? ()
#10 0x0000000000000000 in ?? ()

Thread 10 (Thread 0x7fa7d193f950 (LWP 10501)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693bbcb in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896a96873 in VMThread::loop () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896a9646e in VMThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 9 (Thread 0x7fa7d1d18950 (LWP 10500)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 8 (Thread 0x7fa7d1e19950 (LWP 10499)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
---Type <return> to continue, or q <return> to quit---
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 7 (Thread 0x7fa7d1f1a950 (LWP 10498)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 6 (Thread 0x7fa7d201b950 (LWP 10497)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 5 (Thread 0x7fa7d211c950 (LWP 10496)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 4 (Thread 0x7fa7d221d950 (LWP 10495)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
---Type <return> to continue, or q <return> to quit---
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 3 (Thread 0x7fa7d231e950 (LWP 10494)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 2 (Thread 0x7fa7d241f950 (LWP 10493)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 1 (Thread 0x7fa8971f36f0 (LWP 10490)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a16192 in ObjectMonitor::EnterI () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a16752 in ObjectMonitor::enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896a147eb in ObjectSynchronizer::slow_enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896a146e4 in ObjectSynchronizer::fast_enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa8969d5449 in SharedRuntime::complete_monitor_locking_C () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa891c87efe in ?? ()
#8  0x00007fa856cc7190 in ?? ()
---Type <return> to continue, or q <return> to quit---
#9  0x00007fa891cb6548 in ?? ()
#10 0x00007fa856cc72a0 in ?? ()
#11 0x0000020000000000 in ?? ()
#12 0x00007fa856cc74b8 in ?? ()
#13 0x0000000001d6c800 in ?? ()
#14 0x00007fff9f209680 in ?? ()
#15 0x00007fa896a147eb in ObjectSynchronizer::slow_enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
Backtrace stopped: previous frame inner to this frame (corrupt stack?)
#0  0x00007fa891cb45d4 in ?? ()
{quote}"
DFSClient should use IV generated based on the configured CipherSuite with codecs used,HDFS-6737,hdfs-client,,,,"Seems like we are using IV as like Encrypted data encryption key iv. But the underlying Codec's cipher suite may expect different iv length. So, we should generate IV from the Coec's cipher suite configured.

{code}
 final CryptoInputStream cryptoIn =
          new CryptoInputStream(dfsis, CryptoCodec.getInstance(conf, 
              feInfo.getCipherSuite()), feInfo.getEncryptedDataEncryptionKey(),
              feInfo.getIV());
{code}

So, instead of using feinfo.getIV(), we should generate like

{code}
byte[] iv = new byte[codec.getCipherSuite().getAlgorithmBlockSize()]; 
codec.generateSecureRandom(iv);
{code}

"
make datanodes do graceful shutdown,HDFS-1350,datanode,,,,"we found that the Datanode doesn't do a graceful shutdown and a block can be corrupted (data + checksum amounts off)

we can make the DN do a graceful shutdown in case there are open files. if this presents a problem to a timely shutdown, we can make a it a parameter of how long to wait for the full graceful shutdown before just exiting
"
StreamFile is a datanode servlet but it is misplaced in the namenode package,HDFS-1294,datanode,namenode,,,The StreamFile servlet is only used by datanode but not namenode.
"Generation Stamp mismatches, leading to failed append",HDFS-1231,hdfs-client,,,,"- Summary: the recoverBlock is not atomic, leading retrial fails when 
facing a failure.
 
- Setup:
+ # available datanodes = 3
+ # disks / datanode = 1
+ # failures = 2
+ failure type = crash
+ When/where failure happens = (see below)
 
- Details:
Suppos"
CRC does not match when retrying appending a partial block,HDFS-1228,datanode,,,,"- Summary: when appending to partial block, if is possible that
retrial when facing an exception fails due to a checksum mismatch.
Append operation is not atomic (either complete or fail completely).
 
- Setup:
+ # available datanodes = 2
+# disks / datan"
SocketException: Protocol not available for some JVMs,HDFS-1115,,,,,"Here, input is a folder containing all .xml files from ./conf  
Then trying the command:
./bin/hadoop fs -copyFromLocal input input

The following message is displayed: 
{noformat}
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
{noformat}
However, only empty files are created on HDFS."
Write HDFS wire protocols in AVRO IDL,HDFS-1069,,,,,"As part of the the move to AVRO and wire compatibility, write all HDFS protocols in AVRO IDL"
Make HDFS tests pass using Avro RPC,HDFS-1066,namenode,,,,"Some HDFS tests fail when Avro RPCs are enabled.

To see this, try 'ant test-core -Dtest.hdfs.rpc.engine=org.apache.hadoop.ipc.AvroRpcEngine'

This is an umbrella issue.  Sub-tasks will be added for particular classes of failures."
"Append/flush should support concurrent ""tailer"" use case",HDFS-1060,datanode,hdfs-client,,,"Several people have a usecase for a writer logging edits and using hflush() while one or more readers ""tails"" the file by periodically reopening and seeking to get the latest bytes. Currently there are several bugs. Using this ticket as a supertask for those bugs (and to add test cases for this use case)"
completeFile loops forever if the block's only replica has become corrupt,HDFS-1059,,,,,"If a writer is appending to a block with replication factor 1, and that block has become corrupt, a reader will report the corruption to the NN. Then when the writer tries to complete the file, it will loop forever with an error like:

    [junit] 2010-03-21 17:40:08,093 INFO  namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(1613)) - BLOCK* NameSystem.checkFileProgress: block blk_-4256412191814117589_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[127.0.0.1:56782|RBW]]} has not reached minimal replication 1
    [junit] 2010-03-21 17:40:08,495 INFO  hdfs.DFSClient (DFSOutputStream.java:completeFile(1435)) - Could not complete file /TestReadWhileWriting/file1 retrying...

Should add tests that cover the case of a writer appending to a block that is corrupt while a reader accesses it."
Potential NN deadlock in processDistributedUpgradeCommand,HDFS-862,namenode,,,,"Haven't seen this in practice, but the lock order is inconsistent. processReport locks FSNamesystem, then calls UpgradeManager.startUpgrade, getUpgradeState, and getUpgradeStatus (each of which locks the UpgradeManager). FSNameSystem.processDistributedUpgradeCommand calls upgradeManager.processUpgradeCommand which is synchronized on UpgradeManager, which can call FSNameSystem.leaveSafeMode which synchronizes on FSNamesystem."
Move Ivy related build targets from the list of public targets,HDFS-807,build,,,,"Current list of  build targets is quite long. It contains targets that aren't normally executed by a user, e.g.
{noformat}
 ivy-download                To download ivy
 ivy-report                  Generate
 ivy-retrieve                Retrieve Ivy-managed artifacts
 ivy-retrieve-checkstyle     Retrieve Ivy-managed artifacts for the checkstyle configurations
 ivy-retrieve-common         Retrieve Ivy-managed artifacts for the compile configurations
 ivy-retrieve-javadoc        Retrieve Ivy-managed artifacts for the javadoc configurations
 ivy-retrieve-jdiff          Retrieve Ivy-managed artifacts for the javadoc configurations
 ivy-retrieve-releaseaudit   Retrieve Ivy-managed artifacts for the compile configurations
 ivy-retrieve-test           Retrieve Ivy-managed artifacts for the test configurations
{noformat}

I'd suggest to take these out of the list of top-level build targets"
Add SureLogic annotations' jar into Ivy and Eclipse configs,HDFS-801,build,tools,,,"In order to use SureLogic analysis tools and allow their concurrency analysis annotations in HDFS code the annotations library has to be automatically pulled from a Maven repo. Also, it has to be added to Eclipse .classpath template."
HDFS should enforce a max block size,HDFS-583,namenode,,,,"When DataNode creates a replica, it should enforce a max block size, so clients can't go crazy. One way of enforcing this is to make BlockWritesStreams to be filter steams that check the block size."
Display the most recent GC info on NN webUI,HDFS-6747,,,,,It will be handy if the recent GC information is available on NN webUI. admins don't need to dig out GC logs.
Update startup scripts to start Checkpoint node instead of SecondaryNameNode,HDFS-272,,,,,"Start up script {{start-dfs.sh}} should start Checkpoint node instead of SecondaryNameNode.
It should provide an option to start Checkpoint or Backup node or secondary.
The default should be checkpoint."
Expose NN and DN hooks to service plugins,HDFS-460,datanode,namenode,,,This is the other half of the old HADOOP-5640 (Allow ServicePlugins to hook callbacks into key service events). It adds hooks to the NN and DN to expose certain events to plugins.
Create unencrypted streams interface,HDFS-6554,namenode,security,,,There needs to be an interface to encrypted files that streams the unencrypted data.
Implement seek for HftpFileSystem,HDFS-269,,,,,Support seek in the HftpFileSystem. This is useful for a host of applications that need to access data from a hadoop cluster running a different version of hadoop.
Support manually fsck in DataNode,HDFS-366,,,,,"Now DataNode only support scan all blocks periodically.  Our site need a tool to check some blocks and files manually. 

My current design is to add a parameter to DFSck to indicate deeply and manually fsck request, then let NameNode collect property block identifies and sent them to associated DataNode. 
I'll let DataBlockScanner runs in two ways: periodically ( original one ) and manually. 

Any suggestions on this are welcome. "
DF should use used + available as the capacity of this volume,HDFS-4,,,,,"Generally speaking, UNIX tends to keep certain percentage of disk space reserved for root used only (can be changed via tune2fs or when mkfs). Therefore, Hadoop's DF class should not use the 1st number in df output as the capacity of this volume. Instead, it should use used+available as its capacity.

Otherwise, datanode may think this volume is not full but in fact it is.

The code in question is src/core/org/apache/hadoop/fs/DF.java, method parseExecResult()
"
ZKFailoverController failed to recognize the quorum is not met,HDFS-6706,,,,,"Thanks Kenny Zhang for finding this problem.
The zkfc cannot be startup due to ha.zookeeper.quorum is not met. ""zkfc -format"" doesn't log the real problem. And then user will see the error message instead of the real issue when starting zkfc:
2014-07-01 17:08:17,528 FATAL ha.ZKFailoverController (ZKFailoverController.java:doRun(213)) - Unable to start failover controller. Parent znode does not exist.
Run with -formatZK flag to initialize ZooKeeper.

2014-07-01 16:00:48,678 FATAL ha.ZKFailoverController (ZKFailoverController.java:fatalError(365)) - Fatal error occurred:Received create error from Zookeeper. code:NONODE for path /hadoop-ha/prodcluster/ActiveStandbyElectorLock
2014-07-01 17:24:44,202 - INFO ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@627 - Got user-level KeeperException when processing sessionid:0x346f36191250005 type:create cxid:0x2 zxid:0xf00000033 txntype:-1 reqpath:n/a Error Path:/hadoop-ha/prodcluster/ActiveStandbyElectorLock Error:KeeperErrorCode = NodeExists for /hadoop-ha/prodcluster/ActiveStandbyElectorLock

To reproduce the problem:
1. use HDFS cluster with automatic HA enable and set the ha.zookeeper.quorum to 3.
2. start two zookeeper servers.
3. do ""hdfs zkfc -format"", and then ""hdfs zkfc""
"
Group changes cause FUSE-DFS I/O error,HDFS-2249,fuse-dfs,,,,"If a user utilizes the FUSE mount, then has a group change, they will be unable to write into FUSE using the new group information.

To duplicate (assuming user brian starts only in group brian):
1) Write file into FUSE (or do any other action)
2) Add user brian to group brian2
3) chown anything brian owns to brian:brian2.  The error message that gets passed along is:

[brian@red ~]$ chown brian:brian2 /mnt/hadoop/user/brian/test_group_perms2
chown: changing ownership of `/mnt/hadoop/user/brian/test_group_perms2': Input/output error

I believe this is due to the fact that group information is only looked up at NN connection time?

If you then remount the FUSE mount, the chown command succeeds."
hadoop fs -put should return different code for different failures,HDFS-332,,,,,"
hadoop fs -put may fail due to different reasons, such as the source file does not exist, the destination file already exists, permission denied, or exceptions during writing.
However, it returns the same code (-1), making it impossible to tell what is the actual cause of the failure.
"
"in branch-1, libhdfs makes jni lib calls after setting errno in some places",HDFS-465,,,,,"errno can be affected by other lib calls, so should always be set right before return stmt and never before making other library calls."
Automate test for Hadoop-4597,HDFS-406,,,,,Just do it.
Synthetic Load Generator for NameNode testing -- Next Generation,HDFS-408,,,,,"A review of the Synthetic Load Generator identified several candidates for improvement. None are so urgent as to stand in the way of the present facility, but all are of sufficient interest to merit inclusion in this note.
"
Transparent archival and restore of files from HDFS,HDFS-220,,,,,"There should be a facility to migrate old files away from a production cluster. Access to those files from applications should continue to work transparently, without changing application code, but maybe with reduced performance. The policy engine  that does this could be layered on HDFS rather than being built into HDFS itself."
Cross-system causal tracing within Hadoop,HDFS-232,,,,,"Much of Hadoop's behavior is client-driven, with clients responsible for contacting individual datanodes to read and write data, as well as dividing up work for map and reduce tasks.  In a large deployment with many concurrent users, identifying the effects of individual clients on the infrastructure is a challenge.  The use of data pipelining in HDFS and Map/Reduce make it hard to follow the effects of a given client request through the system.

This proposal is to instrument the HDFS, IPC, and Map/Reduce layers of Hadoop with X-Trace.  X-Trace is an open-source framework for capturing causality of events in a distributed system.  It can correlate operations making up a single user request, even if those operations span multiple machines.  As an example, you could use X-Trace to follow an HDFS write operation as it is pipelined through intermediate nodes.  Additionally, you could trace a single Map/Reduce job and see how it is decomposed into lower-layer HDFS operations.

Matei Zaharia and Andy Konwinski initially integrated X-Trace with a local copy of the 0.14 release, and I've brought that code up to release 0.17.  Performing the integration involves modifying the IPC protocol, inter-datanode protocol, and some data structures in the map/reduce layer to include 20-byte long tracing metadata.  With release 0.18, the generated traces could be collected with Chukwa.

I've attached some example traces of HDFS and IPC layers from the 0.17 patch to this JIRA issue.

More information about X-Trace is available from http://www.x-trace.net/ as well as in a paper that appeared at NSDI 2007, available online at http://www.usenix.org/events/nsdi07/tech/fonseca.html"
Unable to access data from non hadoop application (Version mismatch in DataNode),HDFS-65,,,,,"Hi, I'm trying to access the hdfs of my hadoop cluster in a non hadoop application. Hadoop 0.17.1 is running on standart ports (The same error also occured on earlier verisons). The code however will fail, as there is a version conflict.


This is the code I use:

FileSystem fileSystem = null;
                String hdfsurl = ""hdfs://localhost:50010"";
fileSystem = new DistributedFileSystem();

                try {
                        fileSystem.initialize(new URI(hdfsurl), new Configuration());
                } catch (Exception e) {
                        e.printStackTrace();
                        System.out.println(""init error:"");
                        System.exit(1);

                }


which fails with the exception:


java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:559)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)
        at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:313)
        at org.apache.hadoop.dfs.DFSClient.createRPCNamenode(DFSClient.java:102)
        at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:178)
        at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:68)
        at com.iterend.spider.conf.Config.getRemoteFileSystem(Config.java:72)
        at tests.RemoteFileSystemTest.main(RemoteFileSystemTest.java:22)
init error:


The haddop logfile contains the following error:
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = bluelu-PC/192.168.1.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.17.1
STARTUP_MSG:   build = http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 669344; compiled by 'hadoopqa' on Thu Jun 19 01:18:25 UTC 2008 
2008-07-10 23:05:47,840 INFO org.apache.hadoop.dfs.Storage: Storage directory \hadoop\tmp\hadoop-sshd_server\dfs\data is not formatted.
2008-07-10 23:05:47,840 INFO org.apache.hadoop.dfs.Storage: Formatting ...
2008-07-10 23:05:47,928 INFO org.apache.hadoop.dfs.DataNode: Registered FSDatasetStatusMBean
2008-07-10 23:05:47,929 INFO org.apache.hadoop.dfs.DataNode: Opened server at 50010
2008-07-10 23:05:47,933 INFO org.apache.hadoop.dfs.DataNode: Balancing bandwith is 1048576 bytes/s
2008-07-10 23:05:48,128 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-07-10 23:05:48,344 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-07-10 23:05:48,346 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-07-10 23:05:48,346 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-07-10 23:05:49,047 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@15bc6c8
2008-07-10 23:05:49,244 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-07-10 23:05:49,247 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:50075
2008-07-10 23:05:49,247 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@47a0d4
2008-07-10 23:05:49,257 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=DataNode, sessionId=null
2008-07-10 23:05:49,535 INFO org.apache.hadoop.dfs.DataNode: New storage id DS-2117780943-192.168.1.130-50010-1215723949510 is assigned to data-node 127.0.0.1:50010
2008-07-10 23:05:49,586 INFO org.apache.hadoop.dfs.DataNode: 127.0.0.1:50010In DataNode.run, data = FSDataset{dirpath='c:\hadoop\tmp\hadoop-sshd_server\dfs\data\current'}
2008-07-10 23:05:49,586 INFO org.apache.hadoop.dfs.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 60000msec
2008-07-10 23:06:04,636 INFO org.apache.hadoop.dfs.DataNode: BlockReport of 0 blocks got processed in 11 msecs
2008-07-10 23:19:54,512 ERROR org.apache.hadoop.dfs.DataNode: 127.0.0.1:50010:DataXceiver: java.io.IOException: Version Mismatch
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:961)
        at java.lang.Thread.run(Thread.java:619)


When compiling my own jar from the 0.17-1, I see that the distributed version has the revision number compiled into version number, instead of using the one from the source code (26738 vs 9). Skipping this check triggers another exception:

2008-07-17 17:28:51,268 ERROR org.apache.hadoop.dfs.DataNode: 127.0.0.1:50010:DataXceiver: java.io.IOException: Unknown opcode 112 in data stream
	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1002)
	at java.lang.Thread.run(Thread.java:619)


What do I do different from a hadoop application accessing hdfs?
"
Remove code related to conversion of name-node and data-node storage directories to the format introduced in hadoop 0.13,HDFS-293,,,,,"Hadoop 0.18 does not support direct HDFS upgrades from versions 0.13 or earlier as stated in HADOOP-2797.
A 2 step upgrade is required in this case first from 0.x <= 0.13 to one of version 0.14 through 0.17 and then to 0.18.
This implies that current hdfs does not need to support code related to conversions of the old (pre 0.13) storage layout to the current one introduced in 0.13 (see. HADOOP-702).
"
Name collision for AccessControlException.,HDFS-337,,,,,"There is a name collision in org.apache.hadoop.fs.permission.AccessControlException and java.security.AccessControlException.
Since  java.security.AccessControlException is not an IOException we cannot throw it directly as we do with FileNotFoundException.
Therefore, the only choice is to rename the hadoop AccessControlException to e.g., PermissionException (or AccessDeniedException).
To provide compatibility we can inherit PermissionException from AccessControlException, and deprecate the latter.
"
Explore usage of the sendfile api via java.nio.channels.FileChannel.transfer{To|From} for i/o in datanodes,HDFS-281,,,,,"We could potentially gain a lot of performance by using the *sendfile* system call:

$ man sendfile
{noformat}
DESCRIPTION
       This  call  copies  data between one file descriptor and another.  Either or both of these file descriptors may refer to a socket (but see below).
       in_fd should be a file descriptor opened for reading and out_fd should be a descriptor opened for writing.  offset is  a  pointer  to  a  variable
       holding  the input file pointer position from which sendfile() will start reading data.  When sendfile() returns, this variable will be set to the
       offset of the byte following the last byte that was read.  count is the number of bytes to copy between file descriptors.

       Because this copying is done within the kernel, sendfile() does not need to spend time transferring data to and from user space.
{noformat}

The nio package offers this via the java.nio.channels.FileChannel.transfer{To|From} apis:
http://java.sun.com/j2se/1.5.0/docs/api/java/nio/channels/FileChannel.html#transferFrom(java.nio.channels.ReadableByteChannel,%20long,%20long)
http://java.sun.com/j2se/1.5.0/docs/api/java/nio/channels/FileChannel.html#transferTo(long,%20long,%20java.nio.channels.WritableByteChannel)

From the javadocs:
{noformat}
     This method is potentially much more efficient than a simple loop that reads from this channel and writes to the target channel. Many operating systems can transfer bytes directly from the filesystem cache to the target channel without actually copying them.
{noformat}

----

Hence, this could well-worth exploring for doing io at the datanodes..."
DFS Upgrade should process dfs.data.dirs in parallel,HDFS-270,datanode,,,,"I just upgraded from 0.14.2 to 0.15.0, and things went very smoothly, if a little slowly.

The main reason the upgrade took so long was the block upgrades on the datanodes. Each of our datanodes has 3 drives listed for the dfs.data.dir parameter. From looking at the logs, it is fairly clear that the upgrade procedure does not attempt to upgrade all listed dfs.data.dir's in parallel.

I think even if all of your dfs.data.dir's are on the same physical device, there would still be an advantage to performing the upgrade process in parallel. The less downtime, the better: especially if it is potentially 20 minutes versus 60 minutes."
"DFS web UI does should have ""TAIL this block"" option",HDFS-188,,,,,"It should be available either in addition or instead ""TAIL this file"" as it covers it.
In case of 1 block files, the two are the same.
"
AuthenticationToken will be ignored if the cookie value contains '@',HDFS-6548,,,,,"if the cookie value is something like ""email=xyz@abc.com"", HDFS will ignore the AuthenticationToken and reject the request.

2014-06-05 19:12:40,654 WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: AuthenticationToken ignored: org.apache.hadoop.security.authentication.util.SignerException: Invalid signed text: u

This is caused by fix for HADOOP-10379 Protect authentication cookies with the HttpOnly and Secure flags
it constructs cookie header manually instead of using Cookie class so the value is not double quoted."
TestWebHdfsWithMultipleNameNodes failed with ConcurrentModificationException,HDFS-6428,webhdfs,,,,"TestWebHdfsWithMultipleNameNodes failed as follows:

{code}
Running org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.643 sec <<< FAILURE! - in org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes  Time elapsed: 3.771 sec  <<< ERROR!
java.util.ConcurrentModificationException: null
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
        at java.util.HashMap$EntryIterator.next(HashMap.java:934)
        at java.util.HashMap$EntryIterator.next(HashMap.java:932)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:251)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.shutdown(FsVolumeList.java:249)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:1389)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1304)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1555)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1530)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1514)
        at org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes.shutdownCluster(TestWebHdfsWithMultipleNameNodes.java:99)
{code}


"
Mavenize hdfs contribs,HDFS-2097,,,,,Same as HADOOP-6671 for hdfs contribs
Various syntax and style cleanups,HDFS-6319,,,,,"Fix various style issues like if(, while(, [i.e. lack of a space after the keyword],
Extra whitespace and newlines
if (...) return ... [lack of {}'s]
"
"Branch 0.23 Patch for ""Block Replication Policy Implementation May Skip Higher-Priority Blocks for Lower-Priority Blocks""",HDFS-4696,,,,,This JIRA tracks the solution to HDFS-4366 for the 0.23 branch.
"ch{mod,own,grp} -R to do recursion at the name node",HDFS-241,,,,,Performance. No need to maintain {{distch}}.
Create options to search files/dirs in OfflineImageViewer,HDFS-5990,tools,,,,"Add some query options to liststatus operation in WebImageViewer to search files/dirs in a fsimage.
An example query is as follows:
{code}
curl -i http://localhost:5978/?op=liststatus&owner=root&group=supergroup&minsize=1&maxsize=1048576&recursive=true
{code}
"
JobTracker blocked on TIMED_WAITING DFSOutputStream.waitForAckedSeqno() running,HDFS-6139,hdfs-client,,,,"We're using CDH 4.2.1.  The following is a part of threaddump on our JobTracker.

{code}
""IPC Server handler 249 on 8021"" daemon prio=10 tid=0x00007fce80e2c000 nid=0x718e in Object.wait() [0x00007fc92afe6000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00007fc95f5ffba8> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1708)
        - locked <0x00007fc95f5ffba8> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:1694)
        at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1778)
        - locked <0x00007fc95f5ff898> (a org.apache.hadoop.hdfs.DFSOutputStream)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:66)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:99)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3562)
        - locked <0x00007fc9652787e0> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3475)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker.call(WritableRpcEngine.java:474)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1695)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1691)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1689)
... ...
""Thread-9489990"" daemon prio=10 tid=0x00007fce6c01b000 nid=0x14e1 runnable [0x00007fc8f38f7000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x00007fc9631201d0> (a sun.nio.ch.Util$1)
        - locked <0x00007fc9631201e8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00007fc963120158> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:336)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:158)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:156)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:129)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:117)
        at java.io.FilterInputStream.read(FilterInputStream.java:66)
        at java.io.FilterInputStream.read(FilterInputStream.java:66)
        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:169)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1105)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1039)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:487)
{code}"
DataNode exceptions reading local disk,HDFS-193,,,,,"We get 100s of exceptions at WARN level per day indicating errors while trying to read local blocks.  When this occurs, I've checked on the local box's dfs.data.dir and the block is not present.  Here is a relevant snippet from the logs regarding the missing block.  It *looks* like the DataNode deletes the block and then tries to read it again later.

NOTE: this is for the jar file as up to 8 hosts have this exception for one block and our data repl factor is only 3.
"
ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(1,HDFS-2076,datanode,,,,"see sir
datanode log socket and datasteam problem unable to upload text file to DFS i deleted tmp folders dfs and mapred again i formated ""hadoop namenode -format""
start-all.sh done then
dfs folder contains:
data node ,name node,secondarynamenode
mapred: empty
about space:-----------------
linux-8ysi:/etc/hadoop/hadoop-0.20.2 # df -h
Filesystem Size Used Avail Use% Mounted on
/dev/sda5 25G 16G 7.4G 69% /
udev 987M 212K 986M 1% /dev
/dev/sda7 42G 5.5G 34G 14% /home
-------------------------------------------
http://localhost:50070/dfshealth.jsp------------------

NameNode 'localhost:54310'
Started: Wed Jun 15 04:13:14 IST 2011
Version: 0.20.2, r911707
Compiled: Fri Feb 19 08:07:34 UTC 2010 by chrisdo
Upgrades: There are no upgrades in progress.

Browse the filesystem
Namenode Logs
Cluster Summary
10 files and directories, 0 blocks = 10 total. Heap Size is 15.5 MB / 966.69 MB (1%)
Configured Capacity : 24.61 GB
DFS Used : 24 KB
Non DFS Used : 17.23 GB
DFS Remaining : 7.38 GB
DFS Used% : 0 %
DFS Remaining% : 29.99 %
Live Nodes : 1
Dead Nodes : 0

NameNode Storage:
Storage Directory Type State
/tmp/Testinghadoop/dfs/name IMAGE_AND_EDITS Active

Hadoop, 2011.
----------------------------------------
core-site.xml
---------------------------------
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
<name>hadoop.tmp.dir</name>
<value>/tmp/Testinghadoop/</value>
<description>A base for other temporary directories.</description>
</property>

<property>
<name>fs.default.name</name>
<value>hdfs://localhost:54310</value>
<description>The name of the default file system. A URI whose
scheme and authority determine the FileSystem implementation. The
uri's scheme determines the config property (fs.SCHEME.impl) naming
the FileSystem implementation class. The uri's authority is used to
determine the host, port, etc. for a filesystem.</description>
</property>

</configuration>
------------------------------------------------
hdfs-site.xml
----------------------------------
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
<name>dfs.permissions</name>
<value>true</value>
<description>
If ""true"", enable permission checking in HDFS.
If ""false"", permission checking is turned off,
but all other behavior is unchanged.
Switching from one parameter value to the other does not change the mode,
owner, or group of files or directories.
</description>
</property>

<property>
<name>dfs.replication</name>
<value>1</value>
<description>Default block replication.
The actual number of replications can be specified when the file is created.
The default is used if replication is not specified in create time.
</description>
</property>

</configuration>
---------------------------------------
mapred-site.xml
----------------------------------
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
<name>mapred.job.tracker</name>
<value>localhost:54311</value>
<description>The host and port that the MapReduce job tracker runs
at. If ""local"", then jobs are run in-process as a single map
and reduce task.
</description>
</property>
</configuration>
----------------------------------------------------------------------------------

please give suggetions about this error:
------------------------------------------------------------------------------------------------------------------
linux-8ysi:/etc/hadoop/hadoop-0.20.2/conf # hadoop fsck /
RUN_JAVA
/usr/java/jre1.6.0_25/bin/java
.Status: HEALTHY
Total size: 0 B
Total dirs: 7
Total files: 1 (Files currently being written: 1)
Total blocks (validated): 0
Minimally replicated blocks: 0
Over-replicated blocks: 0
Under-replicated blocks: 0
Mis-replicated blocks: 0
Default replication factor: 1
Average block replication: 0.0
Corrupt blocks: 0
Missing replicas: 0
Number of data-nodes: 1
Number of racks: 1


The filesystem under path '/' is HEALTHY

linux-8ysi:/etc/hadoop/hadoop-0.20.2/conf # hadoop dfsadmin -report
RUN_JAVA
/usr/java/jre1.6.0_25/bin/java
Configured Capacity: 26425618432 (24.61 GB)
Present Capacity: 7923564544 (7.38 GB)
DFS Remaining: 7923539968 (7.38 GB)
DFS Used: 24576 (24 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (1 total, 0 dead)

Name: 127.0.0.1:50010
Decommission Status : Normal
Configured Capacity: 26425618432 (24.61 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 18502053888 (17.23 GB)
DFS Remaining: 7923539968(7.38 GB)
DFS Used%: 0%
DFS Remaining%: 29.98%
Last contact: Wed Jun 15 05:54:00 IST 2011

i got this error:
----------------------------

linux-8ysi:/etc/hadoop/hadoop-0.20.2 # hadoop dfs -put spo.txt In
RUN_JAVA
/usr/java/jre1.6.0_25/bin/java
11/06/15 04:50:18 WARN hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)
at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Unknown Source)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

at org.apache.hadoop.ipc.Client.call(Client.java:740)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
at $Proxy0.addBlock(Unknown Source)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
at $Proxy0.addBlock(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2937)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2819)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

11/06/15 04:50:18 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null
11/06/15 04:50:18 WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/root/In/spo.txt"" - Aborting...
put: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
11/06/15 04:50:18 ERROR hdfs.DFSClient: Exception closing file /user/root/In/spo.txt : org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)
at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Unknown Source)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)
at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Unknown Source)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

at org.apache.hadoop.ipc.Client.call(Client.java:740)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
at $Proxy0.addBlock(Unknown Source)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
at $Proxy0.addBlock(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2937)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2819)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

regards
Ranga Swamy
8904524975 "
"File not being replicated, even when #of DNs >0",HDFS-675,namenode,,,,"One of my tests is now failing, possibly a race condition: 
java.io.IOException: File /test-filename could only be replicated to 0 nodes, instead of 1. ( there are currently 1 live data nodes in the cluster)"
Rolling upgrae exception,HDFS-6113,,,,,"I've a hadoop-2.3 running non-securable on the cluster. then I built a trunk instance, also non securable.

NN1 - active
NN2 - standby
DN1 - datanode 
DN2 - datanode
JN1,JN2,JN3 - Journal and ZK

then on the NN2:
{code}
hadoop-dameon.sh stop namenode
hadoop-dameon.sh stop zkfc
{code}

then:
change the environment variables to the new hadoop.(trunk version)

then:

{code}
hadoop-dameon.sh start namenode
{code}

NN2 throws exception:
{code}
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Could not journal CTime for one more JournalNodes. 1 exceptions thrown:
10.100.91.33:8485: Failed on local exception: java.io.EOFException; Host Details : local host is: ""10-204-8-136/10.204.8.136""; destination host is: ""jn33.com"":8485;
        at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)
        at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223)
        at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.getJournalCTime(QuorumJournalManager.java:631)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.getSharedLogCTime(FSEditLog.java:1383)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog(FSImage.java:738)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:600)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:360)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:258)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:444)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:500)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:656)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:641)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1294)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)
{code}


JN throws Exception:
{code}
2014-03-18 12:19:01,960 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8485: readAndProcess threw exception java.io.IOException: Unable to read authentication method from client 10.204.8.136. Count of bytes read: 0
java.io.IOException: Unable to read authentication method
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1344)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:761)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:560)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:535)
2014-03-18 12:19:01,960 DEBUG org.apache.hadoop.ipc.Server: IPC Server listener on 8485: disconnecting client 10.204.8.136:39063. Number of active connections: 1
{code}
"
Configurable DataXceiver thread stack size,HDFS-4814,datanode,,,,"By default, Java creates a thread with 512kB stack size.  In Datanode, we may have 4096 or more DataXceiver threads.  These threads do not require such large stack size and unnecessarily occupy a large amount of memory."
TestFsLimits#testDefaultMaxComponentLength Fails on branch-2,HDFS-6104,,,,,"testDefaultMaxComponentLength fails intermittently with the following error
{noformat}
java.lang.AssertionError: expected:<0> but was:<255>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hdfs.server.namenode.TestFsLimits.testDefaultMaxComponentLength(TestFsLimits.java:90)
{noformat}

On doing some research, I found that this is actually a JDK7 issue.
The test always fails when it runs after any test that runs addChildWithName() method"
Mismatch in number of bytes already moved and number of bytes being moved in balancer report,HDFS-3295,balancer & mover,,,,"Scenario:
Replication factor = 1,fs.defaultFS=hdfs://namenodeip:port,dfs.namenode.rpc-address=namenodeip:port.
step 1: started DN1.Pumped 4.67GB of data
step 2: started DN2.
step 3: issued the balancer cmd(./start-balancer.sh -threshold 1)

Totally  848.41 MB  has been moved to 2DN and took 4 iterations to move the blocks to 2DN.

But in balancer output in all the iterations the number of bytes already moved is alway 0KB  and there is mismatch in the Bytes being moved with the bytes left to move

Balancer output for 2nd Iteration
=================================
{noformat}
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
Apr 18, 2012 12:41:20 PM          0                 0 KB           848.41 MB          151.18 MB
Apr 18, 2012 1:06:46 PM           1                 0 KB           646.02 MB          151.18 MB
Apr 18, 2012 1:10:28 PM           2                 0 KB           417.43 MB          151.18 MB
Apr 18, 2012 1:14:04 PM           3                 0 KB           223.84 MB          151.18 MB
Apr 18, 2012 1:18:10 PM           4                 0 KB            30.34 MB          151.18 MB{noformat}

In the above balancer output

鈥ytes already moved is 0KB
鈥?In 3 rd iterarion Bytes left to move is 223.84 MB but Bytes being moved is 151.18 MB . So in next iteration the bytes left to move should be 223.84 MB - 151.18 MB 
"
Consider supporting a mechanism to allow datanodes to drain outstanding work during rolling upgrade,HDFS-5446,datanode,,,,"Rebuilding write pipelines is expensive and this can happen many times during a rolling restart of datanodes (i.e. during a rolling upgrade). It seems like it might help if datanodes could be told to drain current work while rejecting new requests - possibly with a new response indicating the node is temporarily unavailable (it's not broken, it's just going through a maintenance phase where it shouldn't accept new work). 

Waiting just a few seconds is normally enough to clear up a good percentage of the open requests without error, thus reducing the overhead associated with restarting lots of datanodes in rapid succession.

Obviously would need a timeout to make sure the datanode doesn't wait forever.
"
TestDNFencingWithReplication fails on branch2,HDFS-5829,,,,,"{noformat}
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.097 sec <<< FAILURE!
testFencingStress(org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication)  Time elapsed: 6 sec  <<< ERROR!
java.lang.ExceptionInInitializerError
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication.<clinit>(TestDNFencingWithReplication.java:49)
	... 28 more
{noformat}"
"When SSL is enabled , the Namenode WEBUI redirects to Infosecport, which could be 0",HDFS-5660,,,,,"(case 1) When SSL is enabled by setting ""hadoop.ssl.enabled"", SSL will be enabled on the regular port (infoport) on Datanode. 

 (case 2) When SSL on HDFS is enabled by setting ""dfs.https.enable"", SSL will be enabled on a separate port (infoSecurePort)  on Datanode. 

if SSL is enabled , the Namenode always redirects to infoSecurePort. ""infoSecurePort"" will be 0 in case 1 above.

This breaks the file browsing via web.
"
Remove compression support from FSImage,HDFS-5725,,,,,"As proposed in HDFS-5722, this jira removes the support of compression in the FSImage format."
HDFS file append failing in multinode cluster,HDFS-4600,,,,,"NOTE: the following only happens in a fully distributed setup (core-site.xml and hdfs-site.xml are attached)

Steps to reproduce:

{noformat}
$ javac -cp /usr/lib/hadoop/client/\* X.java
$ echo aaaaa > a.txt
$ hadoop fs -ls /tmp/a.txt
ls: `/tmp/a.txt': No such file or directory
$ HADOOP_CLASSPATH=`pwd` hadoop X /tmp/a.txt
13/03/13 16:05:14 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.10.37.16:50010, 10.80.134.126:50010], original=[10.10.37.16:50010, 10.80.134.126:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:793)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:858)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:964)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:470)
Exception in thread ""main"" java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.10.37.16:50010, 10.80.134.126:50010], original=[10.10.37.16:50010, 10.80.134.126:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:793)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:858)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:964)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:470)
13/03/13 16:05:14 ERROR hdfs.DFSClient: Failed to close file /tmp/a.txt
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.10.37.16:50010, 10.80.134.126:50010], original=[10.10.37.16:50010, 10.80.134.126:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:793)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:858)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:964)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:470)
{noformat}

Given that the file actually does get created:
{noformat}
$ hadoop fs -ls /tmp/a.txt
Found 1 items
-rw-r--r--   3 root hadoop          6 2013-03-13 16:05 /tmp/a.txt
{noformat}

this feels like a regression in APPEND's functionality.

"
Datanode should have compatibility mode for sending combined block reports,HDFS-5200,datanode,,,,We may want to consider adding a compatibility mode to the Datanode wherein it can send a combined block report (as is done today). HDFS-4988 will modify this behavior so that the Datanode will send one block report per Storage Directory. We will look at this when we get to compatibility/upgrade testing for the feature.
NameNode: change startup progress to track loading INode ACL Map.,HDFS-5622,namenode,,,,Define a new startup progress {{StepType}} for loading INode ACL Map entries and use it to track progress during {{Phase#LOADING_FSIMAGE}}.
NameNode: implement AclManager as abstraction over INode ACL Map.,HDFS-5595,namenode,,,,Complete an initial implementation of {{AclManager}} to enable further development tasks.  This will be a basic implementation using the INode ACL Map to track associations between inodes and ACLs.  This will not fully implement all of the optimizations discussed in the design doc.  Further optimization work will be tracked in separate tasks.
HA namenode with QJM created from org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider should implement Closeable,HDFS-5566,,,,,"When using hbase-0.96 with hadoop-2.2.0, stopping master/regionserver node will result in {{Cannot close proxy - is not Closeable or does not provide closeable invocation}}.

[Mail Archive|https://drive.google.com/file/d/0B22pkxoqCdvWSGFIaEpfR3lnT2M/edit?usp=sharing]

My hadoop-2.2.0 configured as HA namenode with QJM, the configuration is like this:
{code:xml}
  <property>
    <name>dfs.nameservices</name>
    <value>hadoopdev</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.hadoopdev</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.hadoopdev.nn1</name>
    <value>fphd9.ctpilot1.com:9000</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.hadoopdev.nn1</name>
    <value>fphd9.ctpilot1.com:50070</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.hadoopdev.nn2</name>
    <value>fphd10.ctpilot1.com:9000</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.hadoopdev.nn2</name>
    <value>fphd10.ctpilot1.com:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://fphd8.ctpilot1.com:8485;fphd9.ctpilot1.com:8485;fphd10.ctpilot1.com:8485/hadoopdev</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.hadoopdev</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>shell(/bin/true)</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/data/hadoop/hadoop-data-2/journal</value>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>fphd1.ctpilot1.com:2222</value>
  </property>
{code}

I traced the code and found out that when stopping the hbase master node, it will try invoke method ""close"" on namenode, but the instance that created from {{org.apache.hadoop.hdfs.NameNodeProxies.createProxy}} with failoverProxyProviderClass {{org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider}} do not have the Closeable interface.

If we use the Non-HA case, the created instance will be {{org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB}} that implement Closeable.

TL;DR;
With hbase connecting to hadoop HA namenode, when stopping the hbase master or regionserver, it couldn't find the {{close}} method to gracefully close namenode session."
WebHDFS: add support for recursive flag in ACL operations.,HDFS-5611,webhdfs,,,,Implement and test handling of recursive flag for all ACL operations in WebHDFS.
libHDFS: add support for recursive flag in ACL functions.,HDFS-5607,libhdfs,,,,Implement and test handling of recursive flag for all ACL functions in libHDFS.
DistributedFileSystem: add support for recursive flag in ACL methods.,HDFS-5599,hdfs-client,namenode,,,Implement and test handling of recursive flag for all ACL methods in {{DistributedFileSystem}}.
HftpFileSystem should try both KSSL and SPNEGO when authentication is required,HDFS-3699,,,,,"See discussion in HDFS-2617 (Replaced Kerberized SSL for image transfer and fsck with SPNEGO-based solution).

To handle the transition from Hadoop1.0 systems running KSSL authentication to Hadoop systems running SPNEGO, it would be good to fix the client in both 1 and 2 to try SPNEGO and then fall back to try KSSL.  

This will allow organizations that are running a lot of Hadoop 1.0 to gradually transition over, without needing to convert all clusters at the same time.  They would first need to update their 1.0 HFTP clients (and 2.0/0.23 if they are already running those) and then they could copy data between clusters without needing to move all clusters to SPNEGO in a big bang.

"
Incorrect exit code when copying a file bigger than given quota,HDFS-4891,,,,,"Exit code is incorrect for hdfs command.

===Repro step===
1. Set quota on a directory in HDFS.
2. Get a file of which the size is bigger than given quota
3. Do $ hdfs fs -copyFromLocal command to copy the file to HDFS
4. There will be an exception. That is expected error message.
   Exit code will be zero. This is incorrect."
fuse_dfs: ERROR: could not connect open file fuse_impls_open.c:54,HDFS-5072,fuse-dfs,,,,"Here are some command lines on CentOS 6.4
sudo ./fuse_dfs_wrapper.sh dfs://172.16.0.80:9000 /mnt/hdfs
sudo -u hadoop bin/hadoop dfs -mkdir /test
sudo -u hadoop bin/hadoop dfs -chown -R root:root /test

I can create file and directories from following command lines
sudo bin/hadoop dfs -copyFromLocal /tmp/vod/* /test
sudo touch /mnt/hdfs/test/test.txt

And then I created samba share \\172.16.0.80\hdfs for /mnt/hdfs,
On window system, go to the share folder \\172.16.0.80\hdfs\test via root user,
I can create directory, copy files from samba, also can rename file on the samba,
but when I copy file into samba, it popup one window and said I/O error.

I checked the /var/log/messages, found

fuse_dfs: ERROR: could not connect open file fuse_impls_open.c:54

I'm guess it's a bad build, but wondering if there might be another cause."
Convert snapshot user guide to APT from XDOC,HDFS-4747,documentation,,,,"To be consistent with the rest of the HDFS docs, the snapshots user guide should use APT instead of XDOC."
NameSystem.addStoredBlock: addStoredBlock request received for blk_-8546297170266610178_1223147 on 192.168.10.44:40010 size 134217728 but was rejected: Block not in blockMap with any generation stamp,HDFS-5057,,,,,"In some cases, the following symptoms occur?

---------------------------------------------------------------------------









2013-08-02 00:09:16,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_-8546297170266610178_1223147 on 192.168.10.44:40010 size 134217728 but was rejected: Block not in blockMap with any generation stamp
2013-08-02 00:09:16,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-8546297170266610178 to 192.168.10.44:40010
2013-08-02 00:09:16,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_-8546297170266610178_1223147 on 192.168.10.23:40010 size 134217728 but was rejected: Block not in blockMap with any generation stamp
2013-08-02 00:09:16,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-8546297170266610178 to 192.168.10.23:40010
2013-08-02 00:09:16,468 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfsuser cause:org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hdfsroot/20130802/1214110.0 File does not exist. [Lease.  Holder: DFSClient_1545724836, pendingcreates: 4]
2013-08-02 00:09:16,468 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 40000, call addBlock(/hdfsroot/20130802/1214110.0, DFSClient_1545724836, null) from 192.168.10.23:60071: error: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hdfsroot/20130802/1214110.0 File does not exist. [Lease.  Holder: DFSClient_1545724836, pendingcreates: 4]
org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hdfsroot/20130802/1214110.0 File does not exist. [Lease.  Holder: DFSClient_1545724836, pendingcreates: 4]
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1720)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1711)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1619)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:736)
        at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)
		
		
		
		"
Error while running custom writable code  - DFSClient_NONMAPREDUCE_549327626_1 doesnot have any open file,HDFS-5015,hdfs-client,,,,"hi , 
We are facing an below error while running custom writable code, our driver code is using below code . logwritable is the name of custom writable interface.

--Error message at the code execution is as follows
DFSClient_NONMAPREDUCE_549327626_1 doesnot have any open file

--Driver code - 
job.setMapOutputKeyClass(logwritable.class);
job.setMapOutputValueClass(logwritable.class);
job.setNumReducerTasks(0);

--logwritable class implements Writable interface
--Definition of mapper is as follows

public class MAPPER extends Mapper<Text,IntWritable,Logwritable, Logwritabe>{ 
....
contaxt.write(new logwritable(), new logwritable());
}

Any help to resolve above issue will be great help
Thanks
Allan "
HDFSSeek API fails to seek to position when file is opened in write mode.,HDFS-4991,libhdfs,,,,"Hi,

hdfsSeek API fails to seek to position when file is opened in write mode. I studied in documentation that hdfsSeek is only supported when file is opened in read mode.

We have a requirement of replacing the file resided on hadoop environment.

Is there any possibility of having HDFSSeek to be supported when file is opened in write mode?

Regards,
Dayakar"
Add class to manage JournalList,HDFS-3182,ha,namenode,,,See the comment for details of the JournalList ZooKeeper znode.
HDFS permission check is incorrect,HDFS-4918,hdfs-client,namenode,,,"HDFS permisson check is incorrect, even if dfs.permissions is set false. it does look like this was caused by snap shot.
"
Access to HDFS,HDFS-4844,hdfs-client,,,,"Hello,

I'm currently working on the hadoop framework.

I'm trying to publish the statistics from the stored data (in hdfs) in my web interface.
I want to know if there is a possibility to access the filesystem hdfs by a script Perl ?
Is there a perl lib to download ?


"
Encapsulate arguments to BlockReaderFactory in a class,HDFS-4352,hdfs-client,,,,Encapsulate the arguments to BlockReaderFactory in a class to avoid having to pass around 10+ arguments to a few different functions.
Per directory trash settings / trash override,HDFS-4683,,,,,"With the migration of trash settings to server side, it becomes more complicated for applications built on top of HDFS to properly deal with their trash. Applications like HBase and Accumulo already have a fair amount of trash management, adding the HDFS Trash will simply put more stress on DFS. But fully disabling the trash is overkill, as there still may be use for it in other uses of hadoop.

I would like to request either:
A. per directory or user trash settings, so that applications which work in a specific directory or use a specific user can continue to ignore the trash.
B. An updated DistributedFileSystem delete() call which allows you to force ignoring the trash. I'm not sure how feasible this is due to the FileSystem API, but it may be possible."
Create a fsckraid tool to verify the consistency of erasure codes for HDFS-503,HDFS-582,contrib/raid,,,,"HDFS-503 should also have a tool to test the consistency of the parity files generated, so that data corruption can be detected and treated."
"Add a lifecycle interface for Hadoop components: namenodes, job clients, etc.",HDFS-326,,,,,"I'd like to propose we have a standard interface for hadoop components, the things that get started or stopped when you bring up a namenode. currently, some of these classes have a stop() or shutdown() method, with no standard name/interface, but no way of seeing if they are live, checking their health of shutting them down reliably. Indeed, there is a tendency for the spawned threads to not want to die; to require the entire process to be killed to stop the workers. 

Having a standard interface would make it easier for 
 * management tools to manage the different things
 * monitoring the state of things
 * subclassing

The latter is interesting as right now TaskTracker and JobTracker start up threads in their constructor; that's very dangerous as subclasses may have their methods called before they are full initialised. Adding this interface would be the right time to clean up the startup process so that subclassing is less risky."
Wrong server principal is used for rpc calls to namenode if HA is enabled,HDFS-4713,ha,namenode,,,"When various components are connecting to a namenode in a HA-enabled environment, a wrong server principal may be picked up.  This result in SASL failure, since the client-side used a wrong service ticket for the connection.
"
Loading data from HDFS to tape,HDFS-4731,,,,,"I want to load my HDFS data directly to a tape or external storage device.

Please let me know if there is any way to do this."
Permission checker should not expose resolved paths,HDFS-4703,namenode,,,,"Namenode currently prints inode information in AccessControlException. When path provided by user is different from the actual path is resolved to a different path in namenode (some examples as in snapshot paths, HDFS-4434), this might expose information about resolved path.

In this jira, in permission checker, I propose adding a separate field for path to print in AccessControlException."
getting error while configuring the hadoop,HDFS-4682,namenode,,,,"i try to configuring hadoop in windows but i am getting error ... i followed the address .....
http://blog.sqltrainer.com/2012/01/installing-and-configuring-apache.html"
Update the groupId property in the pom.xml to use ASF org instead of kr.ac.korea.dbserver,HDFS-4644,,,,,"As part of incubating process in ASF, we need to change the groupId to reflect the ASF org."
eclipse plugin for hadoop 2.0.0-alpha,HDFS-4624,federation,,,,Is there an eclipse plug in available for hadoop 2.0.0-alpha? i am currently working on a project to device a solution for small files problem and i am using hdfs federation. I want to integrate our web server with hdfs. So I need eclipse plugin for this version. Please help me out.
Enable replicating and pinning files to a data node,HDFS-2004,balancer & mover,,,,Some HDFS applications require that a given file is on the local DataNode.  The functionality created here will allow pinning the file to any DataNode.
Namenode HA using Backup Namenode as Hot Standby,HDFS-2124,datanode,hdfs-client,namenode,,"This is to share our experiences on building Automatic, Hot High Availability for Namenode, leveraging the existing Backup Namenode."
"Namenode supports Active and Standby modes; Standby Namenode should be able to able to take up the Active role dynamically, without losing any data.",HDFS-2164,datanode,hdfs-client,namenode,,"Namenode should have the capability to play dual roles, either as Active or Standby, as HA Agent decides. The Standby Namenode should contain the instance of Backup Namenode inside. When HA Agent forces a switch over, the Backup Namenode instance should be turned to a normal Namenode instance."
GetImage failed,HDFS-4501,,,,,"fsimage and editslog are not updating 
Following are the logs 

SNN Logs:
----------------
2013-02-14 17:29:56,975 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0
2013-02-14 17:29:57,039 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Downloaded file fsimage size 10181 bytes.
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Downloaded file edits size 521 bytes.
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: VM type       = 64-bit
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: 2% max memory = 17.77875 MB
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: capacity      = 2^21 = 2097152 entries
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: recommended=2097152, actual=2097152
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hduser
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=supergroup
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=true
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.block.invalidate.limit=100
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2013-02-14 17:29:57,045 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files = 89
2013-02-14 17:29:57,059 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files under construction = 0
2013-02-14 17:29:57,061 INFO org.apache.hadoop.hdfs.server.common.Storage: Edits file /app/hadoop/tmp/dfs/namesecondary/current/edits of size 521 edits # 7 loaded in 0 seconds.
2013-02-14 17:29:57,061 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0
2013-02-14 17:29:57,121 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 10181 saved in 0 seconds.
2013-02-14 17:29:57,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 10181 saved in 0 seconds.
2013-02-14 17:29:58,121 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Posted URL ramesh:50070putimage=1&port=50090&machine=0.0.0.0&token=-32:1989419481:0:1360842594000:1360842284984
2013-02-14 17:29:58,128 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint:
2013-02-14 17:29:58,129 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.FileNotFoundException: http://ramesh:50070/getimage?putimage=1&port=50090&machine=0.0.0.0&token=-32:1989419481:0:1360842594000:1360842284984
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1613)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:160)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.putFSImage(SecondaryNameNode.java:377)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:418)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:312)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:275)
        at java.lang.Thread.run(Thread.java:722)


NN Logs:
-----------

2013-02-14 18:15:08,127 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hduser cause:java.net.ConnectException: Connection refused
2013-02-14 18:15:08,128 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hduser cause:java.net.ConnectException: Connection refused
2013-02-14 18:15:08,129 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:198)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:378)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:473)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:203)
	at sun.net.www.http.HttpClient.New(HttpClient.java:290)
	at sun.net.www.http.HttpClient.New(HttpClient.java:306)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:995)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:931)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:849)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1299)
	at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:160)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:88)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:85)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:85)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:70)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(GetImageServlet.java:70)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:835)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)


Please help...."
"When storageID of dfs.data.dir of being inconsistent, restart datanode will be failure.",HDFS-4343,datanode,,,,"A datanode has multiple storage directories configured using dfs.data.dir. When the storageID in the VERSION files in these directories, the datanode fails to startup. Consider a scenario, when old data in a storage directory is not cleared, the storage ID from it will not match with storage ID of in other storage storage directories. In this situation, the DataNode will quit and restart fails."
DFSInputStream.reportCheckSumFailure do not report all corrupted blocks,HDFS-4263,,,,,"DFSInputStream.reportCheckSumFailure do not report all corrupted blocks, it seems the code forgets to loop through the corruptedBlockMap."
Add support for scheduled automatic snapshots,HDFS-4166,namenode,,,,This jira will track the work related to supporting automatic scheduled snapshots. 
?Formatting HDFS running into errors :( - Many thanks ,HDFS-4109,hdfs-client,,,,"Hi,

I am trying to format the Hadoop file system with:

bin/hadoop namenode -format

But I received this error in Cygwin:

/home/anjames/bin/../conf/hadoop-env.sh: line 8: $鈥橽r鈥? command not found
/home/anjames/bin/../conf/hadoop-env.sh: line 14: $鈥橽r鈥? command not found
/home/anjames/bin/../conf/hadoop-env.sh: line 17: $鈥橽r鈥? command not found
/home/anjames/bin/../conf/hadoop-env.sh: line 25: $鈥橽r鈥? command not found
/bin/java; No  such file or directoryjre7
/bin/java; No  such file or directoryjre7
/bin/java; cannot execute: No such file or directory

I had previous modified the following conf files the cygwin/home/anjames directory
1. core-site.xml 
2. mapred-site.xml 
3. hdfs-site.xml 

4. hadoop-env.sh

-I updated this file using the instructions: ""uncomment the JAVA_HOME export command, and set the path to your Java home (typically C:/Program Files/Java/{java-home}""

i.e. In the ""hadoop-env.sh"" file, I took out the ""#"" infront of JAVA_HOME comment and changed the path as follows:

export JAVA_HOME=C:\Progra~1\Java\jre7



The hadoop-env.sh file is now:

----------------------------------------------------------------

# Set Hadoop-specific environment variables here.


# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.


# The java implementation to use.  
export JAVA_HOME=C:\Progra~1\Java\jre7 ###<-----uncommented and revised code

# Extra Java CLASSPATH elements.  Optional.

# export HADOOP_CLASSPATH=


# The maximum amount of heap to use, in MB. Default is 1000.

# export HADOOP_HEAPSIZE=2000

# Extra Java runtime options.  Empty by default.
# export HADOOP_OPTS=-server


# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_NAMENODE_OPTS""
export HADOOP_SECONDARYNAMENODE_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_SECONDARYNAMENODE_OPTS""
export HADOOP_DATANODE_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_DATANODE_OPTS""
export HADOOP_BALANCER_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_BALANCER_OPTS""
export JAVA_HOME=C:\Progra~1\Java\jre7
HADOOP_JOBTRACKER_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_JOBTRACKER_OPTS""
# export HADOOP_TASKTRACKER_OPTS=
# The following applies to multiple commands (fs, dfs, fsck, distcp etc)
# export HADOOP_CLIENT_OPTS

# Extra ssh options.  Empty by default.
# export HADOOP_SSH_OPTS=""-o ConnectTimeout=1 -o SendEnv=HADOOP_CONF_DIR""

# Where log files are stored.  $HADOOP_HOME/logs by default.
# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs

# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.
# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves

# host:path where hadoop code should be rsync'd from.  Unset by default.
# export HADOOP_MASTER=master:/home/$USER/src/hadoop

# Seconds to sleep between slave commands.  Unset by default.  This
# can be useful in large clusters, where, e.g., slave rsyncs can
# otherwise arrive faster than the master can service them.
# export HADOOP_SLAVE_SLEEP=0.1

# The directory where pid files are stored. /tmp by default.
# export HADOOP_PID_DIR=/var/hadoop/pids

# A string representing this instance of hadoop. $USER by default.
# export HADOOP_IDENT_STRING=$USER

# The scheduling priority for daemon processes.  See 'man nice'.
# export HADOOP_NICENESS=10


------------------------------

I'm trying to get back in the programming swing with a Big Data Analytics course, so any help is much appreciated its been a while, many thanks. 

"
TestJspHelper#testGetUgi fails with NPE,HDFS-3654,,,,,Looks like my recent change in HDFS-3639 can occasionally cause this test to fail. 
Incompatible change between hadoop-1 and hadoop-2 when the dfs.hosts and dfs.hosts.exclude files are not present,HDFS-3977,,,,,"While testing hadoop-1 and hadoop-2 the following was noticed

if the files in the properties dfs.hosts and dfs.hosts.exclude do not exist

in hadoop-1 namenode format and start went through successfully.

in hadoop-2 we get a file not found exception and both the format and the namenode start commands fail.


We should be logging a warning in the case when the file is not found so that we are compatible with hadoop-1"
NN is getting shutdown by throwing IllegalArgumentException ,HDFS-3467,namenode,,,,"Scenario:
=========
Configure fs.default.name
Format Namenode
Start NameNode.

Here it is getting shutdown ..like following..

{noformat}
2012-05-25 21:09:30,810 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:295)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:283)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:336)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:388)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:400)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:551)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1114)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1170)
2012-05-25 21:09:30,812 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
{noformat}

This is happening in hadoop-2.0.1 But it's not like that in trunk."
Gracefully handle OutOfMemoryErrors,HDFS-2911,datanode,namenode,,,"We should gracefully handle j.l.OutOfMemoryError exceptions in the NN or DN. We should catch them in a high-level handler, cleanly fail the RPC (vs sending back the OOM stackrace) or background thread, and shutdown the NN or DN. Currently the process is left in a not well-test tested state (continuously fails RPCs and internal threads, may or may not recover and doesn't shutdown gracefully)."
Remove name.node.address servlet attribute,HDFS-3437,namenode,,,,Per HDFS-3434 we should be able to get rid of NAMENODE_ADDRESS_ATTRIBUTE_KEY since we always call DfsServlet#createNameNodeProxy within the NN.
QJM: Add a few missing imports in TestEditLog and TestFSEditLogLoader,HDFS-3820,test,,,,Both of these test files are missing a few imports required to make the compile.
TestNameNodeMetrics fails intermittently,HDFS-540,,,,,TestNameNodeMetrics has strict timing constraint that relies on block management functionality and can fail intermittently.
Enable the trash feature by default,HDFS-2740,hdfs-client,namenode,,,"Currently trash is disabled out of box. I do not think it'd be of high surprise to anyone (but surely a relief when *hit happens) to have trash enabled by default, with the usually recommended periods of 1-day.

Thoughts?"
XML-based metrics as JSP servlet for NameNode,HDFS-453,namenode,,,,"In HADOOP-4559, a general REST API for reporting metrics was proposed but work seems to have stalled. In the interim, we have a simple XML translation of the existing NameNode status page which provides the same metrics as the human-readable page. This is a relatively lightweight addition to provide some machine-understandable metrics reporting."
Add concat to FsShell,HDFS-950,tools,,,,Would be nice if concat (HDFS-222) was exposed up to FsShell so users don't have to use hadoop jar.
webhdfs able to set access and mod times of paths to a value in future,HDFS-2435,webhdfs,,,,I am able to use the settimes api and set the access and modification time of paths to a value in future. Should that be allowed?
webhdfs shows wrong message when content is deleted,HDFS-2406,webhdfs,,,,
Wget retrieval of fsimage fails,HDFS-3588,namenode,,,,"jlord$ wget http://localhost:50070/getimage?getimage=1 
--2012-06-28 17:45:48-- http://localhost:50070/getimage?getimage=1 
Resolving localhost... 127.0.0.1, ::1, fe80::1 
Connecting to localhost|127.0.0.1|:50070... connected. 
HTTP request sent, awaiting response... 410 GetImage failed. java.io.IOException: Invalid request has no txid parameter \tat org.apache.hadoop.hdfs.server.namenode.GetImageServlet$GetImageParams.parseLongParam(GetImageServlet.java:414) \tat org.apache.hadoop.hdfs.server.namenode.GetImageServlet$GetImageParams.<init>(GetImageServlet.java:338) \tat org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(GetImageServlet.java:85) \tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707) \tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820) \tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) \tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) \tat org.apache.hadoop.http.HttpServe"
Update DatanodeManager to resolve network location with new API from HADOOP-8304 (DNSToSwitchMapping should add interface to resolve individual host besides a list of host),HDFS-3324,datanode,,,,HADOOP-8304 (DNSToSwitchMapping should add interface to resolve individual host besides a list of host) will induce a new API to resolve individual host rather than a list of host. Here is update on HDFS part to use new API.
While Balancing more than 10 Blocks are being moved from one DN even though the maximum number of blocks to be moved in an iterations is hard coded to 5,HDFS-3377,balancer & mover,,,,"Replication factor= 1,block size is default value
Step 1: Start NN,DN1
Step 2: Pump 5 GB of data.
Step 3: Start DN2 and issue balancer with threshold value 1

In the balancer report and the NN logs displays that more than 8 blocks are being moved from DN1 to DN2 in one iterations But MAX_NUM_CONCURRENT_MOVES in one iterations is hard coded to 5.
Balancer report for 1st iteration:
=================================
{noformat}
HOST-XX-XX-XX-XX:/home/Andreina/NewHadoop2nd/hadoop-2.0.0-SNAPSHOT/bin # ./hdfs balancer -threshold 1
12/05/03 17:31:28 INFO balancer.Balancer: Using a threshold of 1.0
12/05/03 17:31:28 INFO balancer.Balancer: namenodes = [hdfs://HOST-XX-XX-XX-XX:9002]
12/05/03 17:31:28 INFO balancer.Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=1.0]
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
12/05/03 17:31:30 INFO net.NetworkTopology: Adding a new node: /datacenter1/rack1/YY.YY.YY.YY:50176
12/05/03 17:31:30 INFO net.NetworkTopology: Adding a new node: /datacenter1/rack1/XX.XX.XX.XX:50076
12/05/03 17:31:30 INFO balancer.Balancer: 1 over-utilized: [Source[XX.XX.XX.XX:50076, utilization=5.018416429773605]]
12/05/03 17:31:30 INFO balancer.Balancer: 1 underutilized: [BalancerDatanode[YY.YY.YY.YY:50176, utilization=3.272819804269012E-5]]
12/05/03 17:31:30 INFO balancer.Balancer: Need to move 1.06 GB to make the cluster balanced.
12/05/03 17:31:30 INFO balancer.Balancer: Decided to move 716.13 MB bytes from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176
12/05/03 17:31:30 INFO balancer.Balancer: Will move 716.13 MB in this iteration
May 3, 2012 5:31:30 PM            0                 0 KB             1.06 GB          716.13 MB
12/05/03 17:35:29 INFO balancer.Balancer: Moving block -5275260117334749945 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:36:31 INFO balancer.Balancer: Moving block -8079758341763366944 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:37:12 INFO balancer.Balancer: Moving block -7395554712490186313 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:37:45 INFO balancer.Balancer: Moving block 7805443002654525130 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:38:15 INFO balancer.Balancer: Moving block 1864290085256894184 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:40:30 INFO balancer.Balancer: Moving block 23322655230037442 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:41:24 INFO balancer.Balancer: Moving block -8839566903692469634 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:43:03 INFO balancer.Balancer: Moving block 7304385435779271887 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:43:48 INFO balancer.Balancer: Moving block -7242009026552182303 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:44:06 INFO balancer.Balancer: Moving block -2449309138254106767 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:44:55 INFO balancer.Balancer: Moving block 500930296233438046 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:45:04 INFO balancer.Balancer: Moving block 2642725820310610865 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.{noformat}"
"When a file is deleted, its blocks remain in the blocksmap till the next block report from Datanode",HDFS-140,,,,,"When a file is deleted, the namenode sends out block deletions messages to the appropriate datanodes. However, the namenode does not delete these blocks from the blocksmap. Instead, the processing of the next block report from the datanode causes these blocks to get removed from the blocksmap.

If we desire to make block report processing less frequent, this issue needs to be addressed. Also, this introduces indeterministic behaviout to a a few unit tests. Another factor to consider is to ensure that duplicate block detection is not compromised.
"
"dfsadmin -refreshServiceAcl fails Kerb authentication with valid Kerb ticket, other subcommands succeed",HDFS-3001,hdfs-client,,,,"With a valid hdfs kerberos ticket, the dfsadmin subcommand '-refreshServiceAcl' still fails on Kerb authentication. Please see the comment for more details.
"
Why open method in class DFSClient would compare old LocatedBlocks and new LocatedBlocks?,HDFS-404,,,,,"This is in the package of org.apache.hadoop.hdfs, DFSClient.openInfo():
if (locatedBlocks != null) {
        Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();
        Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();
        while (oldIter.hasNext() && newIter.hasNext()) {
          if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {
            throw new IOException(""Blocklist for "" + src + "" has changed!"");
          }
        }
      }
Why we need compare old LocatedBlocks and new LocatedBlocks, and in what case it happen?
Why not ""this.locatedBlocks = newInfo"" directly?"
BookKeeper Journal Manager is not retrying to connect to BK when BookKeeper is not available for write.,HDFS-3392,,,,,"Scenario:

1. Start 3 bookKeeper and 3 zookeeper.
2. Start one NN as active & second NN as standby.
3. Write some file.
4. Stop all BookKeepers.

Issue:
Bookkeeper Journal Manager is not retrying to connect to BK when Bookkeeper is not available for write and Active namenode is shutdown.
"
Start-all.sh Error ,HDFS-3430,datanode,hdfs-client,namenode,,"Hi,
m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error please help me the Error is as follow :- 

2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.0.2-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'root' on Wed May 16 12:30:17 IST 2012
************************************************************/
2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://sra_hadoop:9000
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)

2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62
"
fix FSEditLog#verifyEndOfLog,HDFS-3340,,,,,FSEditLog#verifyEndOfLog may incorrectly report corruption in some cases.  It should be fixed.
DN metrics should include per-disk utilization,HDFS-2999,datanode,,,,We should have per-dfs.data.dir metrics in the DN's metrics report.
Httpfs: Cannot build because of wrong pom file,HDFS-3311,,,,,"On httpfs master 43f595d77d9d42ade5220bfba55ec13e558afb7a
{code}
hoop]$ mvn clean package site assembly:single
...
[WARNING] Unable to create Maven project from repository.
org.apache.maven.project.InvalidProjectModelException: 1 problem was encountered while building the effective model for com.sun.jersey:jersey-server:1.4
[FATAL] Non-parseable POM /home/yuki/.m2/repository/com/sun/jersey/jersey-project/1.4/jersey-project-1.4.pom: end tag name </body> must match start tag name <hr> from line 5 (position: TEXT seen ...</center>\r\n</body>... @6:8)  @ /home/yuki/.m2/repository/com/sun/jersey/jersey-project/1.4/jersey-project-1.4.pom, line 6, column 8
 for project com.sun.jersey:jersey-server:1.4 for project com.sun.jersey:jersey-server:1.4
	at org.apache.maven.project.DefaultMavenProjectBuilder.transformError(DefaultMavenProjectBuilder.java:193)
	at 
...
{code}
and
{code}
hoop]$ cat /home/yuki/.m2/repository/com/sun/jersey/jersey-project/1.4/jersey-project-1.4.pom

<html>
<head><title>301 Moved Permanently</title></head>
<body bgcolor=""white"">
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx/0.6.39</center>
</body>
</html>

{code}
"
Add GETMERGE operation to httpfs,HDFS-2833,,,,,"Add to a convenience operation GETMERGE to httpfs. 

This will simplify for external system accessing over HTTP to consume the output of an MR job a single stream.

It would have the same semantics as the 'hadoop fs -getmerge' command."
Hadoop-1.0.2 is taking the wrong class path during setup ,HDFS-3239,,,,,"/usr/libexec/../bin/hadoop: line 321: /usr/lib/jvm/java-6-sun/bin/java: No such file or directory
/usr/libexec/../bin/hadoop: line 387: /usr/lib/jvm/java-6-sun/bin/java: No such file or directory
"
clean cache and can't start hadoop,HDFS-3117,,,,,"i use command cache >3 /proc/sys/vm/drop_caches to clean cache
Now i can't start hadoop.
thanks"
Build contrib,HDFS-3112,build,,,,"I build capacity-shechuder contrib in src/contrib/capacity-scheduler
> ant package

but error is:

   [javac]                          ^
    [javac] /setup/hadoop-1.0.0/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacitySchedulerQueue.java:754: cannot find symbol
    [javac] symbol  : class JobInProgress
    [javac] location: class org.apache.hadoop.mapred.CapacitySchedulerQueue.UserInfo
    [javac]     public void jobAdded(JobSchedulingInfo jobSchedInfo, JobInProgress job) {
    [javac]                                                          ^
    [javac] /setup/hadoop-1.0.0/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacitySchedulerQueue.java:758: cannot find symbol
    [javac] symbol  : class JobSchedulingInfo
    [javac] location: class org.apache.hadoop.mapred.CapacitySchedulerQueue.User

   ........

   [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 100 errors

BUILD FAILED
/setup/hadoop-1.0.0/src/contrib/build.xml:30: The following error occurred while executing this line:
/setup/hadoop-1.0.0/src/contrib/build-contrib.xml:185: Compile failed; see the compiler error output for details.

HADOOP_CLASSPATH=/setup/hadoop-1.0.0/lib/

thanks
"
Append: The condition is incorrect for checking whether the last block is full,HDFS-1624,,,,,"When the last block is full, the free space should be 0 but not equal to block size.
{code}
//In DFSOutputStream.DataStreamer.DataStreamer(..),
      if (freeInLastBlock == blockSize) {
        throw new IOException(""The last block for file "" + 
            src + "" is full."");
      }
{code}"
HA: enable hadoop security authorization for haadmin / protocols,HDFS-2926,ha,,,,Per HDFS-2917 we can enable Acls on haadmin via adding Acl support to the protocols.
Define a DFSClient interface for wire compatibility,HDFS-2155,hdfs-client,,,,"Define a DFSClient interface so that different versions of client can be loaded for talking to the corresponding version of server.  For more details, see HADOOP-7347."
"Move OfflineImageViewer, OfflineEditsViewer into the same package as the NameNode",HDFS-2990,,,,,"Since OfflineImageviewer and OfflineEditsVeiwer are in different namespaces than the NameNode, we can't ruuse a lot of the code from the NameNode without making things public that we probably don't want to make public.  Let's move them into the NameNode namespace to avoid this problem.  These tools will always be tightly tied to the NameNode anyway (they are parsing the same on-disk structures, after all), so that is where they belong."
some improvements to the manual NN metadata recovery tools,HDFS-2971,libhdfs,,,,"Some improvements to the manual NN metadata recovery tools.

Specifically, we want the Offline Edit Viewer (oev) tool to prints out the highest generation stamp that was encountered when processing the edit log.

We also want OEV to look for large gaps in the generation stamp, as these can indicate corruption.  The minimum gap to look for should be configurable with -G or --genStampGap."
"Add service lifecycle to the HDFS classes: NameNode, Datanode, etc",HDFS-545,datanode,namenode,,,This is the HDFS portion of the service lifecycle changes: integrating the HDFS services: Namenode (and subclasses) and the Datanode with the Service base class. 
HA: TestDFSUtil is failing,HDFS-2960,ha,test,,,"TestDFSUtil is failing.

{noformat}
org.junit.ComparisonFailure: expected:<ns1-nn1.example.com[]:8020> but was:<ns1-nn1.example.com[/220.250.64.24]:8020>
	at org.junit.Assert.assertEquals(Assert.java:123)
	at org.junit.Assert.assertEquals(Assert.java:145)
	at org.apache.hadoop.hdfs.TestDFSUtil.testHANameNodesWithFederation(TestDFSUtil.java:411)
{noformat}

"
Allow NNThroughputBenchmark to stress IPC layer,HDFS-2945,benchmarks,,,,"Currently, the NNThroughputBenchmark acts only as a benchmark of the NN itself. It accesses the NN directly rather than going via IPC. It would be nice to allow it to run in another mode where all access goes through the IPC layer, so that changes in IPC serialization performance can be repeatably/easily measured."
hudson ignored a test failure while generating the junit test report.,HDFS-502,test,,,,"http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/25/console
console test logs show a test failure.

exec] [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 3.745 sec
[exec] [junit] Test org.apache.hadoop.hdfs.server.datanode.TestInterDatanodeProtocol FAILED

TestInterDatanodeProtocol test failed for some reason; and hudson while parsing the xml results didn't consider the test failure.
http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/25/testReport/

I'm not sure if this is something to do with hudson junit plugin or  with hudson itself.

"
"Standby namenode gets a ""cannot lock storage"" exception during startup",HDFS-2865,ha,namenode,,,"Standby NN is restarted. This is a follow-on to hdfs-2863. In this setup, dfs.edits.dir is different from dfs.shared.edits.dir. During startup, standby NN fails to acquire lock on the dfs.edits.dir. If standby NN is restarted again, it seems to work fine."
HA: Client should fail if a failover occurs which switches block pool ID,HDFS-2811,ha,hdfs-client,namenode,,Making sure that the client is talking to an NN with the same block pool ID as the one it was previously talking to seems like a good sanity check.
HA : An alternative approach to clients handling  Namenode failover.,HDFS-2713,ha,hdfs-client,,,"This is the approach for client failover which we adopted when we developed HA for Hadoop. I would like to propose thia approach for others to review & include in the HA implementation, if found useful.

This is similar to the ConfiguredProxyProvider in the sense that the it takes the address of both the Namenodes as the input. The major differences I can see from the current implementation are
1) During failover, user threads can be controlled very accurately about *the time they wait for active namenode* to be available, awaiting the retry. Beyond this, the threads will not be made to wait; DFS Client will throw an Exception indicating that the operation has failed.
2) Failover happens in a seperate thread, not in the client application threads. The thread will keep trying to find the Active Namenode until it succeeds. 
3) This also means that irrespective of whether the operation's RetryAction is RETRY_FAILOVER or FAIL, the user thread can trigger the client's failover. "
Add option to fsck that checks the crc,HDFS-319,,,,,"If I'm not mistaken the fsck command doesn't actually check the crc of the blocks.
I just had a problem where a few blocks managed to get the size 0, fsck didn't report this but
my hadoop programs started failing because some of the input data was corrupted.

An option in the fsck program to actually check the crc would be nice, even though it would take a while to run."
HA: support 2NN with SBN,HDFS-2736,ha,,,,"HDFS-2291 adds support for making the SBN capable of checkpointing, seems like we may also need to support the 2NN checkpointing as well. Eg if we fail over to the SBN does it continue to checkpoint? If not the log grows unbounded until the old primary comes back, if so does that create performance problems since the primary wasn't previously checkpointing?"
Add support for the standby in the bin scripts,HDFS-2732,ha,,,,"We need to update the bin scripts to support SBNs. Two ideas:

Modify start-dfs.sh to start another copy of the NN if HA is configured. We could introduce a file similar to masters (2NN hosts) called standbys which lists the SBN hosts, and start-dfs.sh would automatically make the NN it starts active (and leave the NNs listed in standby as is).

Or simpler, we could just provide a start-namenode.sh script that a user can run to start the SBN on another host themselves. The user would manually tell the other NN to be active via HAAdmin (or start-dfs.sh could do that automatically, ie assume the NN it starts should be the primary)."
Streaming task stuck in MapTask$DirectMapOutputCollector.close,HDFS-105,,,,,Observed a streaming task stuck in MapTask$DirectMapOutputCollector.close
Regression: TestInjectionForSimulatedStorage fails with IllegalMonitorStateException,HDFS-146,,,,,"org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection fails with IllegalMonitorStateException

Stacktrace
java.lang.IllegalMonitorStateException
	at java.lang.Object.notifyAll(Native Method)
	at org.apache.hadoop.ipc.Server.stop(Server.java:1110)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:574)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:569)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:553)
	at org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection(TestInjectionForSimulatedStorage.java:195)

No errors show up in the standard output, but there are a few warnings.
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/749/testReport/org.apache.hadoop.hdfs/TestInjectionForSimulatedStorage/testInjection/"
Unit test failed: TestInjectionForSimulatedStorage,HDFS-44,,,,,"Unit test failed: TestInjectionForSimulatedStorage failed in the nightly build with a timeout:

tail from the console:

[junit] 2007-12-12 12:02:18,674 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] 2007-12-12 12:02:19,184 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] 2007-12-12 12:02:19,694 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] 2007-12-12 12:02:20,204 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestInjectionForSimulatedStorage FAILED (timeout)

Complete console log:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/330/console"
TestInjectionForSimulatedStorage fails once in a while,HDFS-104,,,,,"TestInjectionForSimulatedStorage fails once in a while.
"
high cpu usage in ReplicationMonitor thread ,HDFS-102,,,,,"We had a namenode stuck in CPU 99% and it  was showing a slow response time.
(dfs.namenode.handler.count was still set to 10.)

ReplicationMonitor thread was using the most CPU time.
Jstack showed,

""org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1c7b0f4d"" daemon prio=10 tid=0x0000002d90690800 nid=0x4855 runnable [0x0000000041941000..0x0000000041941b30]
   java.lang.Thread.State: RUNNABLE
  at java.util.AbstractList$Itr.remove(AbstractList.java:360)
  at org.apache.hadoop.dfs.FSNamesystem.blocksToInvalidate(FSNamesystem.java:2475)
  - locked <0x0000002a9f522038> (a org.apache.hadoop.dfs.FSNamesystem)
  at org.apache.hadoop.dfs.FSNamesystem.computeDatanodeWork(FSNamesystem.java:1775)
  at org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor.run(FSNamesystem.java:1713)
  at java.lang.Thread.run(Thread.java:619)
"
NameNode should not serve up a bad edits log,HDFS-93,,,,,"A NameNode disk failure (apparently) resulted in the NameNode serving a bad edits log to the Secondary NameNode. The SNN observed the problem (good!), but had no alternative but to ask again for the log, and again get the same bad replica.

1. The NN could/should have observed the same fault as the SNN.
2. If a replica is known to be bad, the NN should serve a different replica, if available.
3. The SNN should have a way to report replica failure to the NN."
Change all references of dfs to hdfs in configs,HDFS-55,,,,,"After code restructuring dfs has been changed to hdfs, but I see config variables with dfs.<something> eg dfs.http.address. Should we change everything to hdfs?"
DistributedFileSystem.listPaths with some paths causes directory to be cleared,HDFS-58,,,,,"I am currently writing a Ruby wrapper to the Java DFS client libraries via JNI. While attempting to test the listPaths method of the FileSystem class, I discovered that passing a Path URI like ""hdfs://tf11:7276/user/rapleaf"" results in the /user/rapleaf directory being cleared of all contents. A path URI like ""hdfs://tf11:7276/user/rapleaf/*"" will list the contents of the directory without damage. 

I have verified this by creating directories and listing via the bin/hadoop dfs -ls command. 

Obviously passing an incorrectly formatted string a method that should be read-only should not have destructive effects. Also, the actual required path syntax for listings should be recorded in the documentation."
Datanode shutdown is called multiple times ,HDFS-61,,,,,"- When DataNode gets {{IncorrectVersionException}} in {{DataNode.offerService()}} {{DataNode.shutdown()}} is called
- In {{DataNode.processCommand()}} when DataNode gets DNA_SHUTDOWN, {{DataNode.shutdown()}} is called

{{DataNode.shutdown()}} is again cal"
Unhandled exceptions in DFSClient,HDFS-19,,,,,"DFSOutputStream.handleSocketException() does not handle exceptions thrown inside it
by abandonBlock(). I'd propose to retry abandonBlock() in case of timeout.
In case of DFSOutputStream.close() the exception in handleSocketException() will result in
calling abandonFileInProgress().
In a similar case of DFSOutputStream.flush() the file will not be abandoned.
Exceptions thrown by abandonFileInProgress() are not handled either.

Feels like we need a general mechanism for handling all these things."
Check that network topology is updated when new data-nodes are joining the cluster,HDFS-5,,,,,There is a suspicion that network topology is not updated if new racks are added to the cluster. We should investigate and either confirm or rule out this.
DFS logging in NameSystem.pendingTransfer consumes all disk space,HDFS-10,,,,,"Sometimes the namenode goes crazy.  I see this in my logs:

2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.243:50010 to replicate blk_-9064654741761822118 to datanode(s) x.y.z.247:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.243:50010 to replicate blk_-8996500637974689840 to datanode(s) x.y.yz.225:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.227:50010 to replicate blk_-8870980160272831217 to datanode(s) x.y.z.244:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.227:50010 to replicate blk_-8721101562083234290 to datanode(s) x.y.z.250:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.250:50010 to replicate blk_-9044741671491162229 to datanode(s) x.y.z.244:50010

There are on the order of 10k/sec until the machine runs out of disk space.

I notice that in FSNamesystem.java, about 10 lines above this line is logged, there is a comment:

        //
        // Move the block-replication into a ""pending"" state.
        // The reason we use 'pending' is so we can retry
        // replications that fail after an appropriate amount of time.
        // (REMIND - mjc - this timer is not yet implemented.)
        //
"
unresponsive namenode because of not finding places to replicate,HDFS-21,,,,,"We have a 80 node cluster where many nodes started to fail such it went down to 59 live nodes. Originally we had our set of applications 60 times replicated. The cluster size went below the required replication number, and started to become increasingly less responsive, spewing out the following messages at a high rate:

WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2

"
test-contrib target fails on hdfsproxy tests,HDFS-667,contrib/hdfsproxy,,,,"hdfsproxy test fails only on hadoop-20 branch.
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-20-Build/33/console"
TestDFSShell is failing on trunk,HDFS-1819,test,,,,"The commit of HADOOP-7202 now requires that classes that extend {{FsCommand}} implement the {{void run(PathData)}} method. The {{Count}} class was changed to extend {{FsCommand}}, but renamed the {{run}} method and did not provide a replacement."
IOE in org.apache.hadoop.hdfs.server.namenode.GetImageServlet,HDFS-2549,,,,,"Cluster is running but i see in logs this exception:

2011-11-10 01:57:32,851 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of transactions: 9 Total time for transactions(ms): 1Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 103                                                                                                 
2011-11-10 01:57:34,403 INFO org.apache.hadoop.hdfs.server.namenode.GetImageServlet: Downloaded new fsimage with checksum: 59f76fc93e2ab11ff9bb966e37bbb696                                                                                                                                                                    
2011-11-10 01:57:34,414 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.io.IOException: Actual checksum of transferred fsimage: 59f76fc93e2ab11ff9bb966e37bbb696 does not match expected checksum: null                                                                                            
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:102)                                                                                                                                                                                                                            
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:95)                                                                                                                                                                                                                             
        at java.security.AccessController.doPrivileged(Native Method)                                                                                                                                                                                                                                                          
        at javax.security.auth.Subject.doAs(Subject.java:396)                                                                                                                                                                                                                                                                  
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)                                                                                                                                                                                                                                
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:95)                                                                                                                                                                                                                               
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:78)                                                                                                                                                                                                                               
        at java.security.AccessController.doPrivileged(Native Method)                                                                                                                                                                                                                                                          
        at javax.security.auth.Subject.doAs(Subject.java:396)                                                                                                                                                                                                                                                                  
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)                                                                                                                                                                                                                                
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(GetImageServlet.java:78)                                                                                                                                                                                                                               
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)                                                                                                                                                                                                                                                        
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                                                                                                                        
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)                                                                                                                                                                                                                                              
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)                                                                                                                                                                                                                             
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:835)                                                                                                                                                                                                                                  
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)                                                                                                                                                                                                                             
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)                                                                                                                                                                                                                                         
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)                                                                                                                                                                                                                                               
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)                                                                                                                                                                                                                        
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.Server.handle(Server.java:326)                                                                                                                                                                                                                                                                    
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)                                                                                                                                                                                                                                             
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)                                                                                                                                                                                                                             
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)                                                                                                                                                                                                                                                         
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)                                                                                                                                                                                                                                                    
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)                                                                                                                                                                                                                                                    
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)                                                                                                                                                                                                                                        
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)                                                                                                                                                                    "
TSocket timed out reading 4 bytes error against HadoopThriftServer from Hadoop-0.20.2 in Perl,HDFS-2550,hdfs-client,,,,"For the past few weeks I have randomly been receiving errors via the Perl binding of ThriftFS TSocket cannot read 4 bytes.  I thought that it was a symptom of too many clients against the server (say 16 doing a mix of file read and write, as well as listStatus), but in the past couple of days, I have started getting them all the time, even with only 1 client trying to read.  The Perl client error is:

{noformat}
$VAR1 = bless( {
                 'code' => 0,
                 'message' => 'TSocket: timed out reading 4 bytes from bigwws001:9090'
               }, 'Thrift::TException' );
{noformat}

This typically happens in conjunction with errors in other clients, thus initially leading me to believe that it was really a timeout issue.  But after it started occurring with only 1 client running and well within the 10 sec timeout I had set on the TSocket within my Perl client, I checked on the exceptions in the ThriftFS server log:

{noformat}
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.checkBounds(String.java:397)
        at java.lang.String.<init>(String.java:442)
        at org.apache.hadoop.thriftfs.HadoopThriftServer$HadoopThriftHandler.read(HadoopThriftServer.java:307)
        at org.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem$Processor$read.process(Unknown Source)
        at org.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem$Processor.process(Unknown Source)
        at com.facebook.thrift.server.TThreadPoolServer$WorkerProcess.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}


This change seems to have ""solved"" the problem, but this is very much a hack since I do not know the code (maybe throwing an IOException is the right thing to do, and then let it turn into a ThriftIOException?).

{noformat}
--- src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java-orig       2011-11-11 09:18:44.000000000 -0600
+++ src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java    2011-11-11 09:00:47.000000000 -0600
@@ -303,8 +303,9 @@
         }
         byte[] tmp = new byte[length];
         int numbytes = in.read(offset, tmp, 0, length);
-        HadoopThriftHandler.LOG.debug(""read done: "" + tout.id);
-        return new String(tmp, 0, numbytes, ""UTF-8"");
+        HadoopThriftHandler.LOG.debug(""read done: "" + tout.id +
+                                     "" numbytes: "" + numbytes);
+        return new String(tmp, 0, numbytes > 0 ? numbytes : 0, ""UTF-8"");
       } catch (IOException e) {
         throw new ThriftIOException(e.getMessage());
       }

{noformat}
"
with hadoop.security.group.mapping settings two extra INFO with every hadoop command,HDFS-2446,hdfs-client,,,,"If we add following config parameter in core-site.xml in all the compute nodes and client side, we get extra two INFO message in STDOUT for every hadoop command

core-site.xml
=============
...
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping</value>
    <description>
    Class for user to group mapping (get groups for a given user) for ACL
    </description>
  </property>
....

$hadoop dfs -lsr /tmp
11/10/12 19:04:58 INFO util.NativeCodeLoader: Loaded the native-hadoop library
11/10/12 19:04:58 INFO security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
drwx------   - hadoopqa hdfs          0 2011-10-12 19:04 /tmp/file1
drwx------   - hadoopqa hdfs          0 2011-10-12 19:04 /tmp/file2
$hadoop dfs -rmr -skipTrash /tmp/file2
11/10/12 19:04:54 INFO util.NativeCodeLoader: Loaded the native-hadoop library
11/10/12 19:04:54 INFO security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
Deleted hdfs://<NN Hostname>/tmp/file2"
Fix the 2 release audit warnings in trunk.,HDFS-2448,,,,,"/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/META-INF/services/org.apache.hadoop.security.token.TokenRenewer
/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/META-INF/services/org.apache.hadoop.security.token.TokenRenewer
Lines that start with ????? in the release audit report indicate files that do not have an Apache license header.
So, we can include them in rat configuration item in pom.xml to exclude from release audit warnings.

"
"The elephant should remember names, not numbers.",HDFS-34,,,,,"The name node and the data node should not cache the resolution of host names, as doing so prevents the use of DNS CNAMEs for any sort of fail over capability."
Post users:  need admin-only access to HDFS,HDFS-226,,,,,"When user support gets added to HDFS, administrators are going to need to be able to set the namenode such that it only allows connections/interactions from the administrative user.  This is particularly important after upgrades and for other administrative work that may require the changing of user/group ownership, permissions, location of files within the HDFS, etc."
Name node doesn't always properly recognize health of data node,HDFS-577,,,,,The one-way communication (data node -> name node) for node health does not guarantee that the data node is actually healthy.
Name node will exit safe mode w/0 blocks even if data nodes are broken,HDFS-580,,,,,"If one brings up a freshly formatted name node against older data nodes with an incompatible storage id (such that the datanodes fail with Directory /mnt/u001/dfs-data is in an inconsistent state: is incompatible with others.), the name node will still come out of safe mode.  Writes will partially succeed--entries are created, but all are zero length.
"
HDFS should support SNMP,HDFS-607,,,,,HDFS should provide key statistics over a standard protocol such as SNMP.  This would allow for much easier integration into common software packages that are already established in the industry.
Topology is permanently cached,HDFS-870,,,,,Replacing the topology script requires a namenode bounce because the NN caches the information permanently.  It should really either expire it periodically or expire on -refreshNodes.
utility to list all files less than X replication,HDFS-1049,tools,,,,"It would be great to have a utility that lists all files that have a replication less than X.  While fsck provides this output and it isn't that tricky to parse, it would still be nice if Hadoop had this functionality out of the box."
Bouncing the namenode causes JMX deadnode count to drop to 0,HDFS-2275,namenode,,,,The namenode JMX metrics and the namenode web UI disagree on the number of dead nodes.  See comments.
hftp: not able to delete content using hftp url,HDFS-2374,,,,,"issue command 

bin/hadoop dfs -rmr hftp://nn:port/path

Command fails with the following exception

11/09/27 03:53:43 WARN fs.Trash: Can't create trash directory: hftp://nn:port/user/some_user/.Trash/Current/path
Problem with Trash.Not supported. Consider using -skipTrash option"
app master configuration web UI link under the Job menu opens up application menu,HDFS-2357,,,,,"If you go to the app master web UI for a particular job.  The job menu on the left side displays links for overview, counters, configuration, etc..

If you click on the configuration one, it closes the job menu and opens the application menu on that left side.  It shouldn't do this.  It should leave the job menu open."
Improve metrics for measuring NN startup costs.,HDFS-1729,namenode,,,,"Current logging and metrics are insufficient to diagnose latency problems in cluster startup.  Add:
1. better logs in both Datanode and Namenode for Initial Block Report processing, to help distinguish between block
report processing problems and RPC/queuing problems;
2. new logs to measure cost of scanning all blocks for over/under/invalid replicas, which occurs in Namenode just
before exiting safe mode;
3. new logs to measure cost of processing the under/invalid replica queues (created by the above mentioned scan), which
occurs just after exiting safe mode, and is said to take 100% of CPU."
HDFS logs not being rotated,HDFS-2302,,,,,"In commit c5edca2b15eca7c0bd568a0017f699ac91b8aebf, the logs for the namenode, datanode and secondarynamenode are being written to .out files and are not being rotated after one day. IMHO rotation of logs is important"
Call to localhost/127.0.0.1:54310 failed on connection exception: Connection refused,HDFS-2295,datanode,hdfs-client,namenode,,"when I try it
dm@master:/usr/local/hadoop-0.20.2$ bin/hadoop dfs -ls hdfs://localhost:54310/
It cast these exceptions
11/08/29 11:34:29 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 0 time(s).
11/08/29 11:34:30 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 1 time(s).
11/08/29 11:34:31 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 2 time(s).
11/08/29 11:34:32 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 3 time(s).
11/08/29 11:34:33 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 4 time(s).
11/08/29 11:34:34 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 5 time(s).
11/08/29 11:34:35 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 6 time(s).
11/08/29 11:34:36 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 7 time(s).
11/08/29 11:34:37 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 8 time(s).
11/08/29 11:34:38 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 9 time(s).
ls: Call to localhost/127.0.0.1:54310 failed on connection exception: java.net.ConnectException: Connection refused
Our namenode is listening to the 54310 port.
<name>fs.default.name</name>
<value>hdfs://master:54310</value>

"
Add the ability to set dfs.block.size for a given file from the command line,HDFS-2293,hdfs-client,,,,"See the thread [here|http://www.mail-archive.com/hdfs-user@hadoop.apache.org/msg01459.html], in which Allen correctly points out that since dfs.block.size is a client-side config, one could just set a different value in a custom HADOOP_CONF_DIR. We might as well make that easy."
Accelerate chmod task in ant files,HDFS-1834,build,,,,"The chmod tasks have a parallel=""false"" attribute attached to them.  This causes ant to shell out for each and every item in the fileset which slows down parts of the build."
dfs.blocksize and dfs.block.size are ignored,HDFS-2216,datanode,,,,"Hi everybody, 
I have a big problem with the blocksize configuration I tried different configurations at the hdfs-site config (datanodes and namenodes have the same configuration)

i tried 
<property>
  <name>dfs.block.size</name>
  <value>128m</value>
  <final>true</final>
</property>
-----------------------
<property>
  <name>dfs.blocksize</name>
  <value>128m</value>
  <final>true</final>
</property>
<property>
  <name>dfs.block.size</name>
  <value>134217728</value>
  <final>true</final>
</property>
-----------------------
<property>
  <name>dfs.blocksize</name>
  <value>134217728</value>
  <final>true</final>
</property>

But in all cases when I run a map-reduce job i found that the amount of slots is proportional to 64M blocks and if a run a du -hs in all datanode a found that the block files are 65M

For example 

65M	blk_720821373677199742
520K	blk_720821373677199742_13833.meta
65M	blk_-7294849724164540020
520K	blk_-7294849724164540020_7314.meta
65M	blk_7468624346905314857
520K	blk_7468624346905314857_7312.meta
65M	blk_7638666943543421576
520K	blk_7638666943543421576_7312.meta
65M	blk_7830551307355288414
520K	blk_7830551307355288414_7314.meta
65M	blk_7844142950685471855
520K	blk_7844142950685471855_7312.meta
65M	blk_7978753697206960302
520K	blk_7978753697206960302_7312.meta
65M	blk_-7997715050017508513
520K	blk_-7997715050017508513_7313.meta
65M	blk_-8085168141075809653
520K	blk_-8085168141075809653_7314.meta
65M	blk_8250324684742742886
520K	blk_8250324684742742886_7314.meta
65M	blk_839132493383742510
520K	blk_839132493383742510_7312.meta
65M	blk_847712434829366950
520K	blk_847712434829366950_7314.meta
65M	blk_-8735461258629196142

but if I run ""hadoop fs -stat %o file"" I get 134217728

Do yo know if I am doing same wrong? or It's souns a bug

Thanks"
Add simple HTTP GET support for single cluster case and for both Jetty and Tomcat based proxy ,HDFS-474,contrib/hdfsproxy,,,,This is a follow-up to HADOOP-5366. Currently simple HTTP GET support only works for tomcat-based proxy and only when it is set up to proxy for multiple HDFS clusters (see HADOOP-5363). This feature supports users using commonly available HTTP clients like wget or curl to fetch files from proxy. It would be good to port this feature to both Jetty and Tomcat based proxies in cases where the proxy is set up to proxy for a single HDFS cluster. Note that currently Jetty-based proxy can't be set up to proxy for multiple HDFS clusters.
TestProxyUtil failing test-patch builds,HDFS-936,contrib/hdfsproxy,test,,,"TestProxy has been consistently failing HDFS test-patch builds the last day or two:

junit.framework.AssertionFailedError: null
	at org.apache.hadoop.hdfsproxy.TestProxyUtil.testSendCommand(TestProxyUtil.java:43)"
test-cactus fails with timeout error on trunk,HDFS-911,contrib/hdfsproxy,,,,"test-cactus target is failing on trunk:
{noformat}
test-cactus:
     [echo]  Free Ports: startup-17053 / http-17054 / https-17055
     [echo] Please take a deep breath while Cargo gets the Tomcat for running the servlet tests...
     [copy] Copying 1 file to /private/tmp/zok/hadoop-hdfs/build/contrib/hdfsproxy/target/tomcat-config/conf
   [cactus] -----------------------------------------------------------------
   [cactus] Running tests against Tomcat 5.x @ http://localhost:17054
   [cactus] -----------------------------------------------------------------
   [cactus] Deploying [/private/tmp/zok/hadoop-hdfs/build/contrib/hdfsproxy/target/test.war] to [/private/tmp/zok/hadoop-hdfs/build/contrib/hdfsproxy/target/tomcat-config/
webapps]...
   [cactus] Tomcat 5.x starting...
   [cactus] Tomcat 5.x started on port [17054]

BUILD FAILED
/private/tmp/zok/hadoop-hdfs/src/contrib/hdfsproxy/build.xml:292: Failed to start the container after more than [180000] ms. Trying to connect to the [http://localhost:170
54/test/ServletRedirector?Cactus_Service=RUN_TEST] test URL yielded a [-1] error code. Please run in debug mode for more details about the error.{noformat}"
Restarting the namenode when the secondary namenode is checkpointing seems to remove everything from /,HDFS-2177,,,,,This was again discovered by Arpit Gupta! Restarting the namenode when the secondary namenode is checkpointing seems to remove everything from /
Using the hadoop-deamon.sh script to start nodes leads to a depricated warning ,HDFS-2122,,,,,hadoop-daemon.sh calls common/bin/hadoop for hdfs/bin/hdfs tasks and so common/bin/hadoop complains its deprecated for those uses.
HDFS FileStatus.getPermission() should return 777 when dfs.permissions=false,HDFS-17,,,,,"Generic permission checking code should still work correctly when dfs.permissions=false.  Currently FileStatus#getPermission() returns the actual permission when dfs.permissions=false on the namenode, which is incorrect, since all accesses are permitted in this case."
Web UI fails with non-default port,HDFS-1196,,,,,"On trunk using hdfs://localhost:9000 for fs.default.name going to the web UI http://localhost:9000 fails with the following stack trace: 

10/06/08 23:07:19 INFO namenode.NameNode: NameNode up at: localhost/127.0.0.1:9000
10/06/08 23:08:45 INFO ipc.Server: IPC Server listener on 9000: readAndProcess threw exception java.io.IOException: Unable to read authentication method. Count of bytes read: 0
java.io.IOException: Unable to read authentication method
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1092)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:525)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:332)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

Use port 50070 works (but says ""NameNode 'localhost:9000'"").
"
Hadoop does not scale as expected,HDFS-2091,,,,,"The more nodes I add to this application, the slower it goes. This is the app's map,

 public void map(IntWritable linearPos, FloatWritable heat, Context context
                            ) throws IOException, InterruptedException {

       int myLinearPos = linearPos.get();
       //Distribute my value to the previous and the next
       linearPos.set(myLinearPos - 1);
       context.write(linearPos, heat);
       linearPos.set(myLinearPos + 1);
       context.write(linearPos, heat);
       //Distribute my value to the cells above and below
       linearPos.set(myLinearPos - MatrixData.Length());
       context.write(linearPos, heat);
       linearPos.set(myLinearPos + MatrixData.Length());
       context.write(linearPos, heat);
    }//end map

and this is the reduce,

public void reduce(IntWritable linearPos, Iterable<FloatWritable> fwValues,
                     Context context) throws IOException, InterruptedException {

       //Handle first and last ""cold"" boundaries
       if(linearPos.get()<0 || linearPos.get()>MatrixData.LinearSize()){
          return;
       }

       if(linearPos.get()==MatrixData.HeatSourceLinearPos()){
          context.write(linearPos, new FloatWritable(MatrixData.HeatSourceTemperature()));
          return;
       }

       float result = 0.0f;
       //Add all the values
       for(FloatWritable heat : fwValues) {
          result += heat.get();
       }

      context.write(linearPos, new FloatWritable(result/4) );
}

For example, with 6 nodes I get a running time of 15minutes, and with 4 nodes I get a running time of 8minutes!.
This is how I generated the input,

 public static void main(String[] args) throws IOException {
     //Write file in the local dir
     String uri = ""/home/beto/mySeq"";

     Configuration conf = new Configuration();
     FileSystem fs = FileSystem.get(URI.create(uri), conf);
     Path path = new Path(uri);

     IntWritable key = new IntWritable();
     FloatWritable value = new FloatWritable(0.0f);

     SequenceFile.Writer writer = null;
     try {
       writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());

     int step = MatrixData.LinearSize()/10;
     int limit = step;
     for (int i = 0; i <= MatrixData.LinearSize(); i++) {
        key.set(i);
        if(i>limit){
             System.out.println(""*"");
             limit +=step;
        }
          if(i==MatrixData.HeatSourceLinearPos()) {
            writer.append(key, new FloatWritable(MatrixData.HeatSourceTemperature()));
            continue;
          }

        writer.append(key, value);

      }
    } finally {
      IOUtils.closeStream(writer);
    }
  }


I'm basically solving a heat transfer problem in a squared section. Pretty simple. The input data is being stored as a (key, value) pairs, read in this way, processed, and written again in the same format.
Any thoughts?

Alberto.
"
Can't read binary data off HDFS via thrift API,HDFS-1169,,,,,"Trying to access binary data stored in HDFS (in my case, TypedByte files generated by Dumbo) via thrift talking to org.apache.hadoop.thriftfs.HadoopThriftServer, the data I get back is mangled. For example, when I read a file which contains the value 0xa2, it's coming back as 0xef 0xbf 0xbd, also known as the Unicode replacement character.

I think this is because the read method in HadoopThriftServer.java is trying to convert the data read from HDFS into UTF-8 via the String() constructor. 

This essentially makes the HDFS thrift API useless for me :-(.

Not being an expert on Thrift, but would it be possible to modify the API so that it uses the binary type listed on http://wiki.apache.org/thrift/ThriftTypes?"
thriftfs jar cleanup,HDFS-1484,build,,,,"Thriftfs has some jars checked into the source directory (src/contrib/thriftfs/lib). It contains libthrift.jar, which we should get via ivy,  and hadoopthriftapi.jar which is a checked in (out of date?) binary of the generated java code. This file could probably be removed entirely. "
Add Thrift interface to JobTracker/TaskTracker,HDFS-419,,,,,"We currently have Thrift interfaces for accessing the NN and the DFS, but no access to the Mapred system. I'm currently working on instrumenting the JT with a Thrift plugin. If anyone has any thoughts in this area, please comment on this JIRA.

Open questions:
* Is job submission a practical goal to accomplish via Thrift? My thought is that this might be a goal for a second JIRA after basic monitoring/reporting is working.
* Does this belong in contrib/thriftfs? I propose renaming contrib/thriftfs to contrib/thrift, as it will no longer be FS-specific."
Move new per-daemon Thrift contrib into a new contrib project/package layout,HDFS-418,,,,,The discussion in HADOOP-4707 has moved away from the original intent of that issue. Opening this one to continue the discussion of this rename/move.
Add conf to classpath in start_thrift_server.sh,HDFS-789,,,,,"In the current script start_thrift_server.sh, the conf folder is not in classpath, so when user start the thrift server, it actually use the local file system.
So I create this issue to put the hdfs configuration file to classpath."
Hadoop Namenode not starting up.,HDFS-1864,,,,,"1. Checked to make sure hadoop was running properly. Discovered that we suppose to run 'jps' and make sure there is a namenode process. 

2. Documentation said, if namenode does not exist - then run 

/etc/init.d/hadoop-0.20-namenode start 

/etc/init.d/hadoop-0.20-namenode status - namenode process fails 

 EQX hdfs@hadoop-master:/usr/lib/hadoop/bin$ /etc/init.d/hadoop-0.20-namenode status 
namenode dead but pid file exists 


3. Searched for pid files. We deleted pid files. 

4. All over stats fell off. As direct of result, looking at the process list - and there appeared to be a stalled process that was killed. 

kill -9 

for the following process: 

EQX root@hadoop-master:/etc/init.d# ps aux | grep namenode 
hdfs 5038 0.2 1.0 3617440 526704 ? Sl Mar31 74:02 /usr/java/default/bin/java -Dproc_namenode -Xmx3000m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dhadoop.log.dir=/usr/lib/hadoop/logs -Dhadoop.log.file=hadoop-hdfs-namenode-hadoop-master.rockyou.com.log -Dhadoop.home.dir=/usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,DRFA -Djava.library.path=/usr/lib/hadoop/lib/native/Linux-amd64-64 -Dhadoop.policy.file=hadoop-policy.xml -classpath /usr/lib/hadoop/conf:/usr/java/default/lib/tools.jar:/usr/lib/hadoop:/usr/lib/hadoop/hadoop-core-0.20.2+737.jar:/usr/lib/hadoop/lib/aspectjrt-1.6.5.jar:/usr/lib/hadoop/lib/aspectjtools-1.6.5.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-daemon-1.0.1.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.0.4.jar:/usr/lib/hadoop/lib/commons-logging-api-1.0.4.jar:/usr/lib/hadoop/lib/commons-net-1.4.1.jar:/usr/lib/hadoop/lib/core-3.1.1.jar:/usr/lib/hadoop/lib/hadoop-fairscheduler-0.20.2+737.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.8.jar:/usr/lib/hadoop/lib/hadoop-lzo.jar:/usr/lib/hadoop/lib/hsqldb-1.8.0.10.jar:/usr/lib/hadoop/lib/hue-plugins-1.1.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.5.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.5.2.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.12.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.12.jar:/usr/lib/hadoop/lib/jets3t-0.6.1.jar:/usr/lib/hadoop/lib/jetty-6.1.14.jar:/usr/lib/hadoop/lib/jetty-util-6.1.14.jar:/usr/lib/hadoop/lib/junit-4.5.jar:/usr/lib/hadoop/lib/kfs-0.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.15.jar:/usr/lib/hadoop/lib/mockito-all-1.8.2.jar:/usr/lib/hadoop/lib/mysql-connector-java-5.0.8-bin.jar:/usr/lib/hadoop/lib/oro-2.0.8.jar:/usr/lib/hadoop/lib/servlet-api-2.5-6.1.14.jar:/usr/lib/hadoop/lib/slf4j-api-1.4.3.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.4.3.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsp-2.1/jsp-2.1.jar:/usr/lib/hadoop/lib/jsp-2.1/jsp-api-2.1.jar::/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar org.apache.hadoop.hdfs.server.namenode.NameNode 
root 16449 0.0 0.0 61136 744 pts/4 S+ 16:29 0:00 grep namenode 
EQX root@hadoop-master:/etc/init.d# kill -9 5038



 We starting looking at log output - we discovered the namenode startup process is throwing a null pointer exception. 

STARTUP_MSG: build = -r 98c55c28258aa6f42250569bd7fa431ac657bdbd; compiled by 'root' on Mon Oct 11 13:14:05 EDT 2010 
************************************************************/ 
2011-04-25 21:16:47,841 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null 
2011-04-25 21:16:47,949 INFO org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.ganglia.GangliaContext31 
2011-04-25 21:16:47,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hdfs 
2011-04-25 21:16:47,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=root 
2011-04-25 21:16:47,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=true 
2011-04-25 21:16:47,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s) 
2011-04-25 21:16:48,301 INFO org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics: Initializing FSNamesystemMetrics using context object:org.apache.hadoop.metrics.ganglia.GangliaContext31 
2011-04-25 21:16:48,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemStatusMBean 
2011-04-25 21:16:48,328 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files = 237791 
2011-04-25 21:16:51,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files under construction = 0 
2011-04-25 21:16:51,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 42758182 loaded in 3 seconds. 
2011-04-25 21:16:51,701 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.NullPointerException 
at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1088) 

at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1100) 
at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedMkdir(FSDirectory.java:987) 

at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedMkdir(FSDirectory.java:974) 

at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:718) 

at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1034) 
at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:845) 

at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:379) 
at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:99) 

at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:343) 
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:317) 

at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:214) 
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:394) 

at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1148) 

at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1157) 
"
hdfsproxy tests fails test patch builds. ,HDFS-471,contrib/hdfsproxy,,,,"org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface

Failing for the past 19 builds (Since #469 ) 
Took 25 sec.
Error Message

Stacktrace
java.lang.NullPointerException
	at org.apache.commons.cli.GnuParser.flatten(GnuParser.java:110)
	at org.apache.commons.cli.Parser.parse(Parser.java:143)
	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:374)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:138)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1314)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:414)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:119)
	at org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface(TestHdfsProxy.java:209)

Link: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/487/testReport/org.apache.hadoop.hdfsproxy/TestHdfsProxy/testHdfsProxyInterface/"
CHANGES.txt in the last three branches diverged,HDFS-1082,documentation,,,,"Particularly, CHANGES.txt in hdfs trunk and 0.21 don't reflect that 0.20.2 has been released, there is no section for 0.20.3, and the diff on the fixed issues is not uniform."
"In secure mode, Datanodes should shutdown if they come up on non-privileged ports",HDFS-1589,,,,,"It's a security hole, need to fix it asap."
Number of Under-Replicated Blocks information posted on WebUI is  inconsistent with CLI Fsck report. ,HDFS-810,namenode,,,,Number of Under-Replicated Blocks show on WebUI is inconsistent with Under replicated blocks shown on Fsck.
Failed to execute fsck with -move option,HDFS-110,,,,,"I received the following error when running fsck with -move option. The dfs was started by a user while the fsck was ran by a different user that does not have the write access to the hadoop dfs data directory.

- moving to /lost+found: /data.txt
java.io.FileNotFoundException: hadoop-dfs-data-dir/tmp/client-8234960199756230677 (Permission denied)
at java.io.FileOutputStream.open(Native Method)
at java.io.FileOutputStream.<init>(FileOutputStream.java:179)
at java.io.FileOutputStream.<init>(FileOutputStream.java:131)
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:546)
at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:99)
at org.apache.hadoop.dfs.DFSck.lostFoundMove(DFSck.java:222)
at org.apache.hadoop.dfs.DFSck.check(DFSck.java:178)
at org.apache.hadoop.dfs.DFSck.check(DFSck.java:124)
at org.apache.hadoop.dfs.DFSck.fsck(DFSck.java:112)
at org.apache.hadoop.dfs.DFSck.main(DFSck.java:433)
Failed to move /data.txt to /lost+found: hadoop-dfs-data-dir/tmp/client-8234960199756230677 (Permission denied)
"
"create(file, true) appears to be violating atomicity",HDFS-1563,namenode,,,,"Will upload a unittest to reveal this bug.

In a word, when a thread is doing create(file, true) on existing file, there are chances that another thread will get 'false' for exists(file) during the period."
TestFileAppend4 fails,HDFS-1306,,,,,"Following tests are failing on trunk:
TestFileAppend4.testRecoverFinalizedBlock 
TestFileAppend4.testCompleteOtherLeaseHoldersFile 
"
Make FSVolumeSet in FSDataSet pluggable,HDFS-1405,datanode,,,,"I am trying to submit a patch for HDFS-1362, which enable online add or remove a volume of a serving DataNode.

I managed to create the patch  avoiding modify exist class, but FSVolumeSet is an exception, the member ""volumes"" is an array, while I need a List to facilitate the hotplug procedure. Thus I have 2 possible solutions:
* Provide a abstract class as its super class, with the definition of all its methods. OR
* Modify the volumes' definition in FSVolumeSet, as a Gereral Type Collection<>, which may use different implementation in subclasses.

I will supply a patch for the first method for review"
Reduce the time required for a checkpoint,HDFS-1162,namenode,,,,The checkpoint time increases linearly with the number of files in the cluster. This is a problem with large clusters.
RaidNode should fix missing blocks directly on Data Node,HDFS-1171,contrib/raid,,,,"RaidNode currently does not fix missing blocks. The missing blocks have to be fixed manually.

This task proposes that recovery be more automated:
1. RaidNode periodically fetches a list of corrupt files from the NameNode
2. If the corrupt files has a RAID parity file, RaidNode identifies missing block(s) in the file and recomputes the block(s) using the parity file and other good blocks
3. RaidNode sends the generated block contents to a DataNode
   a. RaidNode chooses a DataNode with the most available space to send the block. "
Need a command line option in RaidShell to fix blocks using raid,HDFS-1453,contrib/raid,,,,"RaidShell currently has an option to recover a file and return the path to the recovered file. The administrator can then rename the recovered file to the damaged file.

The problem with this is that the file metadata is altered, specifically the modification time. Instead we need a way to just repair the damaged blocks and send the fixed blocks to a data node.

Once this is done, we can put automation around it."
Group name is not properly set in inodes,HDFS-1415,,,,,In NameNode.create() and NameNode.mkdirs() do not pass the group name. So it is not properly set.
creating a file in hdfs should not automatically create the parent directories,HDFS-98,,,,,"I think it would be better if HDFS didn't automatically create directories for the user. In particular, in clean up code, it would be nice if deleting a directory couldn't be undone by mistake by a process that hasn't been killed yet creating a new file."
Client logic for 1st phase and 2nd phase failover are different,HDFS-1237,hdfs-client,,,,"- Setup:
number of datanodes = 4
replication factor = 2 (2 datanodes in the pipeline)
number of failure injected = 2
failure type: crash
Where/When failures happen: There are two scenarios: First, is when two datanodes crash at the same time in the first "
Hadoop distcp tool fails if file path contains special characters + & !,HDFS-31,tools,,,,"Copying folders containing + & ! characters between hdfs (using hftp) does not work in distcp

For example: 
Copying  folder ""string1+string2""  at ""namenode.address.com"", hftp port myport to ""/myotherhome/folder"" on ""myothermachine"" does not work 

myothermachine prompt>>> hadoop --config ~/mycluster/ distcp  ""hftp://namenode.address.com:myport/myhome/dir/string1+string2""  /myotherhome/folder/
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Error results for hadoop job1:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
08/07/16 00:27:39 INFO tools.DistCp: srcPaths=[hftp://namenode.address.com:myport/myhome/dir/string1+string2]
08/07/16 00:27:39 INFO tools.DistCp: destPath=/myotherhome/folder/
08/07/16 00:27:41 INFO tools.DistCp: srcCount=2
08/07/16 00:27:42 INFO mapred.JobClient: Running job: job1
08/07/16 00:27:43 INFO mapred.JobClient:  map 0% reduce 0%
08/07/16 00:27:58 INFO mapred.JobClient: Task Id : attempt_1_m_000000_0, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:538)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)

08/07/16 00:28:14 INFO mapred.JobClient: Task Id : attempt_1_m_000000_1, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:538)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)

08/07/16 00:28:28 INFO mapred.JobClient: Task Id : attempt_1_m_000000_2, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:538)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)

With failures, global counters are inaccurate; consider running with -i
Copy failed: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1053)
        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:615)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:764)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:784)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Error log for the map task which failed
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
INFO org.apache.hadoop.tools.DistCp: FAIL string1+string2/myjobtrackermachine.com-joblog.tar.gz : java.io.IOException: Server returned HTTP response code: 500 for URL: http://mymachine.com:myport/streamFile?filename=/myhome/dir/string1+string2/myjobtrackermachine.com-joblog.tar.gz&ugi=myid,mygroup
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1241)
	at org.apache.hadoop.dfs.HftpFileSystem.open(HftpFileSystem.java:117)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:371)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:377)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:504)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:279)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
Make it possible for BlockPlacementPolicy to return null,HDFS-1351,namenode,,,,"The idea is to modify FSNamesystem.chooseExcessReplicates code, so it can accept a null return from chooseReplicaToDelete which will indicate that NameNode should not be deleting extra replicas.

One possible usecase - if there are nodes being added to the cluster that might have corrupt replicas on them you do not want to delete other replicas until the block scanner finished scanning every block on the datanode.

This will require additional work on the implementation of the BlockPlacementPolicy, but with this JIRA I just wanted to create a basis for future improvements."
Corrupted blocks get deleted but not replicated,HDFS-86,,,,,"When I test the patch to HADOOP-1345 on a two node dfs cluster, I see that dfs correctly delete the corrupted replica and successfully retry reading from the other correct replica, but the block does not get replicated. The block remains with only 1 replica until the next block report comes in.

In my testcase, since the dfs cluster has only 2 datanodes, the target of replication is the same as the target of block invalidation.  After poking the logs, I found out that the namenode sent the replication request before the block invalidation request. 

This is because the namenode does not invalidate a block well. In FSNamesystem.invalidateBlock, it first puts the invalidate request in a queue and then immediately removes the replica from its state, which triggers the choosing a target for the block. When requests are sent back to the target datanode as a reply to a heartbeat message, the replication requests have higher priority than the invalidate requests.

This problem could be solved if a namenode removes an invalidated replica from its state only after the invalidate request is sent to the datanode."
NameNode.getBlockLocations throws NPE when offset > filesize and file is not empty,HDFS-513,namenode,,,,"in BlockManager.getBlockLocations, if the offset is past the end of a non-empty file, it returns null. In FSNamesystem.getBlockLocationsInternal, this null is passed through to inode.createLocatedBlocks, so it ends up with a LocatedBlocks instance whose .blocks is null. This is then iterated over in FSNamesystem.getBlockLocations, and throws an NPE.

Instead, I think BlockManager.getBlockLocations should return Collections.emptyList in the past-EOF case. This would result in an empty list response from NN.getBlockLocations which matches the behavior of an empty file. If this sounds like the appropriate fix I""ll attach the patch."
Namenode should return lease recovery request with other requests,HDFS-320,,,,,"HADOOP-5034 modified NN to return both replication and deletion requests to DN in one reply to a heartbeat. However, the lease recovery request is still sent separately by itself. Is there a reason for this? If not, I suggest we combine them together. This will make it less confusing when adding new types of requests, which are combinable as well."
 FileChecksumServlets.RedirectServlet doesn't carry forward the delegation token,HDFS-1136,,,,," FileChecksumServlets.RedirectServlet doesn't carry forward the delegation token in the redircted URL when redirecting to a random data node to get checksum
from there"
DFSClient incorrectly asks for new block if primary crashes during first recoverBlock,HDFS-1229,hdfs-client,,,,"Setup:
--------
+ # available datanodes = 2
+ # disks / datanode = 1
+ # failures = 1
+ failure type = crash
+ When/where failure happens = during primary's recoverBlock
 
Details:
----------
Say client is appending to block X1 in 2 datanodes: dn1 and dn2.
First it needs to make sure both dn1 and dn2  agree on the new GS of the block.
1) Client first creates DFSOutputStream by calling
 
>OutputStream result = new DFSOutputStream(src, buffersize, progress,
>                                            lastBlock, stat, conf.getInt(""io.bytes.per.checksum"", 512));
 
in DFSClient.append()
 
2) The above DFSOutputStream constructor in turn calls processDataNodeError(true, true)
(i.e, hasError = true, isAppend = true), and starts the DataStreammer
 
> processDatanodeError(true, true);  /* let's call this PDNE 1 */
> streamer.start();
 
Note that DataStreammer.run() also calls processDatanodeError()
> while (!closed && clientRunning) {
>  ...
>      boolean doSleep = processDatanodeError(hasError, false); /let's call this PDNE 2*/
 
3) Now in the PDNE 1, we have following code:
 
> blockStream = null;
> blockReplyStream = null;
> ...
> while (!success && clientRunning) {
> ...
>    try {
>         primary = createClientDatanodeProtocolProxy(primaryNode, conf);
>         newBlock = primary.recoverBlock(block, isAppend, newnodes); /*exception here*/
>         ...
>    catch (IOException e) {
>         ...
>         if (recoveryErrorCount > maxRecoveryErrorCount) { 
>         // this condition is false
>         }
>         ...
>         return true;
>    } // end catch
>    finally {...}
>    
>    this.hasError = false;
>    lastException = null;
>    errorIndex = 0;
>    success = createBlockOutputStream(nodes, clientName, true);
>    }
>    ...
 
Because dn1 crashes during client call to recoverBlock, we have an exception.
Hence, go to the catch block, in which processDatanodeError returns true
before setting hasError to false. Also, because createBlockOutputStream() is not called
(due to an early return), blockStream is still null.
 
4) Now PDNE 1 has finished, we come to streamer.start(), which calls PDNE 2.
Because hasError = false, PDNE 2 returns false immediately without doing anything

> if (!hasError) { return false; }
 
5) still in the DataStreamer.run(), after returning false from PDNE 2, we still have
blockStream = null, hence the following code is executed:

if (blockStream == null) {
   nodes = nextBlockOutputStream(src);
   this.setName(""DataStreamer for file "" + src + "" block "" + block);
   response = new ResponseProcessor(nodes);
   response.start();
}
 
nextBlockOutputStream which asks namenode to allocate new Block is called.
(This is not good, because we are appending, not writing).
Namenode gives it new Block ID and a set of datanodes, including crashed dn1.
this leads to createOutputStream() fails because it tries to contact the dn1 first.
(which has crashed). The client retries 5 times without any success,
because every time, it asks namenode for new block! Again we see
that the retry logic at client is weird!

*This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)*"
Impact in NameNode scalability because heartbeat processing acquires the global lock,HDFS-81,,,,,"The heartbeat processing code recently got rearranged via HADOOP-3254. This caused the NameNode heartbeat processing code to acquire the FSNamesystem global lock for every heartbeat processing. This could impact scalability of the namenode.

This problem is present in 0.17.x and 0.18.x release only. It is not present in 0.16.x and on trunk.
"
All datanodes are bad in 2nd phase,HDFS-1239,hdfs-client,,,,"- Setups:
number of datanodes = 2
replication factor = 2
Type of failure: transient fault (a java i/o call throws an exception or return false)
Number of failures = 2
when/where failures happen = during the 2nd phase of the pipeline, each happens at each "
NameNode startup failed,HDFS-18,,,,,"After bouncing the cluster namenode refuses to start and gives the error: FSNamesystem initialization failed. Also says: saveLeases found path /tmp/temp623789763/tmp659456056/_temporary/_attempt_200904211331_0010_r_000002_0/part-00002 but no matching entry in namespace. Recovery from checkpoint resulted in wide spread corruption which made it necessary to format the dfs.
This is opened as a result from these threads: http://www.mail-archive.com/core-user@hadoop.apache.org/msg09397.html, http://www.mail-archive.com/core-user@hadoop.apache.org/msg09663.html
"
Client uselessly retries recoverBlock 5 times,HDFS-1236,hdfs-client,,,,"Summary:
Client uselessly retries recoverBlock 5 times
The same behavior is also seen in append protocol (HDFS-1229)

The setup:
+ # available datanodes = 4
+ Replication factor = 2 (hence there are 2 datanodes in the pipeline)
+ Failure type = Bad disk at datanode (not crashes)
+ # failures = 2
+ # disks / datanode = 1
+ Where/when the failures happen: This is a scenario where each disk of the two datanodes in the pipeline go bad at the same time during the 2nd phase of the pipeline (the data transfer phase).
 
Details:
 
In this case, the client will call processDatanodeError
which will call datanode.recoverBlock to those two datanodes.
But since these two datanodes have bad disks (although they're still alive),
then recoverBlock() will fail.
For this one, the client's retry logic ends when streamer is closed (close == true).
But before this happen, the client will retry 5 times
(maxRecoveryErrorCount) and will fail all the time, until
it finishes.  What is interesting is that
during each retry, there is a wait of 1 second in
DataStreamer.run (i.e. dataQueue.wait(1000)).
So it will be a 5-second total wait before declaring it fails.
 
This is a different bug than HDFS-1235, where the client retries
3 times for 6 seconds (resulting in 25 seconds wait time).
In this experiment, what we get for the total wait time is only
12 seconds (not sure why it is 12). So the DFSClient quits without
contacting the namenode again (say to ask for a new set of
two datanodes).
So interestingly we find another
bug that shows client retry logic is complex and not deterministic
depending on where and when failures happen.

This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and
Haryadi Gunawi (haryadi@eecs.berkeley.edu)"
Bad retry logic at DFSClient,HDFS-1233,hdfs-client,,,,"- Summary: failover bug, bad retry logic at DFSClient, cannot failover to the 2nd disk
 
- Setups:
+ # available datanodes = 1
+ # disks / datanode = 2
+ # failures = 1
+ failure type = bad disk
+ When/where failure happens = (see below)
 
- Details:

The"
Incorrect hadoop dependency in 0.21,HDFS-1180,build,,,,"The  build is broken due to a dependency on ""hadoop-common 0.21"". In version 0.21 the ""common"" artifact is called ""hadoop-core."""
Deprecate dfs.permissions.superusergroup in favor of hadoop.cluster.administrators,HDFS-1008,security,,,,"HADOOP-6568 added the configuration {{hadoop.cluster.administrators}} through which admins can configure who the superusers/supergroups for the cluster are. HDFS itself already has {{dfs.permissions.superusergroup}} (which is just a single group). As agreed upon at HADOOP-6568, this should be deprecated in favor of {{hadoop.cluster.administrators}}."
Fix Javadoc for DistributedCache usage,HDFS-1098,documentation,,,,"The Javadoc for using DistributedCache isn't up-to-date with code:
 *     JobConf job = new JobConf();
 *     DistributedCache.addCacheFile(new URI(""/myapp/lookup.dat#lookup.dat""), 
 *                                   job);

This is the new API:
  public static void addCacheFile(URI uri, Configuration conf) {

Javadoc should (partially) reflect the actual usage from TaskRunner:
            p[i] = DistributedCache.getLocalCache(files[i], conf,
                                                  new Path(baseDir),
                                                  fileStatus,
                                                  false, Long.parseLong(
                                                           fileTimestamps[i]),
                                                  new Path(workDir.
                                                        getAbsolutePath()),
                                                  false);
          }
          DistributedCache.setLocalFiles(conf, stringifyPathArray(p));"
Block report processing should compare gneration stamp,HDFS-168,,,,,"If a reported block has a different generation stamp then the one stored in the NameNode, the reported block will be considered as invalid.  This is incorrect since blocks with larger generation stamp are valid."
TestHDFSCLI.refreshServiceAcl failed,HDFS-848,,,,,"{noformat}
                     Test ID: [597]
            Test Description: [refreshServiceAcl: refreshing security authorization policy for namenode]
 
               Test Commands: [-fs NAMENODE -refreshServiceAcl ]
 
 
                  Comparator: [ExactComparator]
          Comparision result:   [fail]
             Expected output:   []
               Actual output:   [refreshServiceAcl: org.apache.hadoop.security.authorize.AuthorizationException: java.security.AccessControlException: access denied ConnectionPermission(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol)
]
{noformat}
I did ""ant veryclean"" before running ""ant run-test-hdfs -Dtestcase=TestHDFSCLI""."
Secondary name node won't start,HDFS-109,,,,,"The secondary name node was unable to start. 

Early investigation suggests that the secondary node was not provisioned the same as the primary node, and entered a swapping regime. 

This report can be dismissed if provisioning is the complete explanation."
TestOfflineImageViewer fails,HDFS-683,tools,,,,"{noformat}
Failed reading valid file: No image processor to read version -21 is available.
{noformat}
See [build #56|http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/56/testReport/org.apache.hadoop.hdfs.tools.offlineImageViewer/TestOfflineImageViewer/testOIV/] for more details."
Simplify the codes in the replica related classes,HDFS-628,datanode,,,,"In the replica related classes (e.g. ReplicaBeingWritten, ReplicaInPipeline, etc.), there are too many constructors and unnecessary parameter passing."
"dfs startup error, 0 datanodes in ",HDFS-100,,,,," Web site shows:
NameNode 'master.cloud:9000'
Started:  Thu Dec 18 17:10:35 CST 2008  
Version:  0.17.2.1, r684969  
Compiled:  Wed Aug 20 22:29:32 UTC 2008 by oom  
Upgrades:  There are no upgrades in progress.  


Browse the filesystem 
--------------------------------------------------------------------------------

Cluster Summary
Safe mode is ON. The ratio of reported blocks 0.0000 has not reached the threshold 0.9990. Safe mode will be turned off automatically.
21 files and directories, 6 blocks = 27 total. Heap Size is 4.94 MB / 992.31 MB (0%) 

Capacity : 0 KB 
DFS Remaining : 0 KB 
DFS Used : 0 KB 
DFS Used% : 0 % 
Live Nodes  : 0 
Dead Nodes  : 0 




--------------------------------------------------------------------------------
There are no datanodes in the cluster 

On blog of namenode, it shows:
2008-12-18 17:10:35,204 INFO org.apache.hadoop.dfs.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master.cloud/10.100.4.226
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.17.2.1
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 684969; compiled by 'oom' on Wed Aug 20 22:29:32 UTC 2008
************************************************************/
2008-12-18 17:10:35,337 INFO org.apache.hadoop.ipc.metrics.RpcMetrics: Initializing RPC Metrics with hostName=NameNode, port=9000
2008-12-18 17:10:35,344 INFO org.apache.hadoop.dfs.NameNode: Namenode up at: master.cloud/10.100.4.226:9000
2008-12-18 17:10:35,348 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null
2008-12-18 17:10:35,351 INFO org.apache.hadoop.dfs.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext
2008-12-18 17:10:35,436 INFO org.apache.hadoop.fs.FSNamesystem: fsOwner=user,users,ftp,sshd
2008-12-18 17:10:35,437 INFO org.apache.hadoop.fs.FSNamesystem: supergroup=supergroup
2008-12-18 17:10:35,437 INFO org.apache.hadoop.fs.FSNamesystem: isPermissionEnabled=true
2008-12-18 17:10:35,576 INFO org.apache.hadoop.fs.FSNamesystem: Finished loading FSImage in 181 msecs
2008-12-18 17:10:35,585 INFO org.apache.hadoop.dfs.StateChange: STATE* Safe mode ON. 
The ratio of reported blocks 0.0000 has not reached the threshold 0.9990. Safe mode will be turned off automatically.
2008-12-18 17:10:35,595 INFO org.apache.hadoop.fs.FSNamesystem: Registered FSNamesystemStatusMBean
2008-12-18 17:10:35,727 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-12-18 17:10:35,870 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-12-18 17:10:35,871 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-12-18 17:10:35,871 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-12-18 17:10:36,260 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@b60b93
2008-12-18 17:10:36,307 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-12-18 17:10:36,309 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:50070
2008-12-18 17:10:36,310 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@1bd7848
2008-12-18 17:10:36,310 INFO org.apache.hadoop.fs.FSNamesystem: Web-server up at: 0.0.0.0:50070
2008-12-18 17:10:36,310 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2008-12-18 17:10:36,312 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2008-12-18 17:10:36,316 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000: starting
2008-12-18 17:10:36,317 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000: starting
2008-12-18 17:10:36,317 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000: starting
2008-12-18 17:10:36,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000: starting
2008-12-18 17:10:36,322 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000: starting
2008-12-18 17:10:36,374 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000: starting


and in the slaves blog, i find a strange thing. 

************************************************************/
2008-12-18 17:11:47,627 INFO org.apache.hadoop.dfs.DataNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = slave3.cloud/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.17.2.1
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 684969; compiled by 'oom' on Wed Aug 20 22:29:32 UTC 2008
************************************************************/
2008-12-18 17:11:48,267 ERROR org.apache.hadoop.dfs.DataNode: java.io.IOException: Incompatible namespaceIDs in /home/user/hadoop/tmp/dfs/data: namenode namespaceID = 1098832880; datanode namespaceID = 464592288
        at org.apache.hadoop.dfs.DataStorage.doTransition(DataStorage.java:298)
        at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:142)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:258)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:176)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:2795)
        at org.apache.hadoop.dfs.DataNode.instantiateDataNode(DataNode.java:2750)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:2758)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:2880)

2008-12-18 17:11:48,269 INFO org.apache.hadoop.dfs.DataNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at slave3.cloud/127.0.0.1
************************************************************/
"
files disappearing on dfs,HDFS-33,,,,,"We have rare instances where we see files disappearing after they have been renamed.

Because of the rarity, it is hard to reproduce. In the latest instance, the destination files (900 of them) existed before the renaming operation with the same filenames and got deleted just before the new files got renamed. It is possible, but not sure, that all instances of file-disappearing are of this nature."
"On a busy cluster, it is possible for the client to believe it cannot fetch a block when the client or datanodes are running slowly",HDFS-262,,,,,"On a heavily loaded node, the communication between a DFSClient can time out or fail leading DFSClient to believe the datanode is non-responsive even though the datanode is, in fact, healthy. It may run through all the retries for that datanode leading DFSClient to mark the datanode ""dead"".  

This can continue as DFSClient iterates through the other datanodes for the block it is looking for, and then DFSClient will declare that it can't find any servers for that block (even though all n (where n = replication factor) datanodes are healthy (but slow) and have valid copies of the block.

It is also possible that the process running the DFSClient is too slow and misses (or times out) responses from the data node, resulting in the DFSClient believing that the datanode is dead.

Another possibility is that the block has been moved from one or more datanodes since DFSClient$DFSInputStream.chooseDataNode() found the locations of the block.

When the retries for each datanode and all datanodes are exhausted, DFSClient$DFSInputStream.chooseDataNode() issues the warning:

{code}
          if (nodes == null || nodes.length == 0) {
            LOG.info(""No node available for block: "" + blockInfo);
          }
          LOG.info(""Could not obtain block "" + block.getBlock() + "" from any node:  "" + ie);
{code}

It would be an improvement, and not impact performance under normal conditions if  when DFSClient decides that it cannot find the block anywhere, for it to retry finding the block by calling 

{code}
private static LocatedBlocks callGetBlockLocations()
{code}
 
*once* , to attempt to recover from machine(s) being too busy, or the block being relocated since the initial call to callGetBlockLocations(). If the second attempt to find the block based on what the namenode told DFSClient,  then issue the messages and give up by throwing the exception it does today.
"
Strange behavior with bin/hadoop dfs -mv,HDFS-643,,,,,"I had a very strange experience today with the bin/hadoop dfs -mv command. I accidentally passed 3 parameters, like so:

{code}
bin/hadoop dfs -mv /1 /2 /3
{code}

My intent was to mv /1 to /3, but mis-pasted some stuff. However, to my surprise, the result I got was that /1 had been moved to /2, and /2 had then been moved to /3! This seems like totally confusing semantics - I would have expected a 3-parameter move to either ignore the 3rd parameter or error out altogether. Needless to say, a solid 10 minutes of confused scrambling commenced."
data node startup problem,HDFS-407,,,,,"Hi,
I have set up a cluster using dom0( a.k.a Host OS ) and one domU ( a.k.a Guest OS ) both are running on Fedora -8 32-bit .
Para Virtualization used: XEN Version 3.1.0-13

HostOS host name is hadoop10 configured as name-node/job tracker and it will acts as data-node/task tracker as well in order to spread the data storage and GuestOS name is hadoop11 it is configured as data-node/task tracker

i'm facing new problem...trying to start HDFS daemons but i'm getting following errors:
1) Permission denied error in line number [3] below
2) hadoop11: no datanode to stop at line number [8]

but i can able to SSH to hadoop11( GuestOS/slave )
---------------------------------------------------------------------

[hadoop@hadoop10 hadoop-0.18.0]$ bin/start-dfs.sh
[1] starting namenode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-namenode-hadoop10.out
[2] hadoop11: starting datanode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-datanode-hadoop11.out
[3] hadoop11: /home/hadoop/hadoop-0.18.0/bin/hadoop-daemon.sh: line 118: /tmp/hadoop-hadoop-datanode.pid: Permission denied
[4] hadoop10: starting datanode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-datanode-hadoop10.out
[5] hadoop10: starting secondarynamenode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-secondarynamenode-hadoop10.out
[6] [hadoop@hadoop10 hadoop-0.18.0]$ bin/stop-dfs.sh
[7] stopping namenode
[8] hadoop11: no datanode to stop
[9] hadoop10: stopping datanode
[10] hadoop10: stopping secondarynamenode
[11] [hadoop@hadoop10 hadoop-0.18.0]$ ssh hadoop11    ----------- |
[12] Last login: Tue Oct  7 18:07:20 2008 from hadoop10    -----------|---------> I can able to SSH to slave ( hadoop11 ) from master ( hadoop10)
[13] [hadoop@hadoop11 ~]$                              --------------------------------|

------------------------------------------------------------------------------------------------

Whats going wrong over here??  
"
What will we encounter if we add a lot of nodes into the current cluster?,HDFS-541,,,,,"Our cluster has 200 nodes now. In order to improve its ability, we hope to add 60 nodes into the current cluster. However, we all don't know what will happen if we add so many nodes at the same time. Could you give me some tips and notes? During the process, which part shall we pay much attention on?
Thank you!
"
Upgrade build to use findbugs-1.3.8,HDFS-507,build,,,,"Current findbugs reports a false warning:

{quote}
OBL     Method  org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.readHeader(File) may fail to clean up stream or resource of type java.io.InputStream
{quote}

Suggested version findbugs-1.3.8 does not report this false positive."
Convert the file including 10 minutes run's tests into the test harness test suite,HDFS-505,build,test,,,"This issue is created to track the conversion of external testlist for 10 minutes tests into JUnit test suite.

It would be consistent through the bigger Hadoop project to have 10 minutes tests to be collected in a JUnit's test suite instead of an external file (as of now). 

The reason is simple: with the file we'd have two points of maintenance: tests source code and one an auxiliary text files. Besides, a general approach is to use JUnit's suites for tagging purposes, thus having a suite here would be more uniform.
"
error : too many fetch failures,HDFS-486,,,,,"i configured the hadoop cluster environment with one physical mahine as data node and 7(2physical and 5 virtual machines) as namenode
when i submitted the sort job(from hadoop-core-example)
it finished with following error, can you explain why this is happening and how to solve this?


output from terminal ************************************************************

[root@hadoop1 hadoop-0.18.3]# bin/hadoop jar hadoop-0.18.3-examples.jar sort input13 output1
Running on 7 nodes to sort from hdfs://hadoop1:8022/user/root/input13 into hdfs://hadoop1:8022/user/root/output1 with 12 reduces.
Job started: Fri Jul 10 14:00:19 IST 2009
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.JobClient: Running job: job_200907101344_0002
09/07/10 14:00:20 INFO mapred.JobClient:  map 0% reduce 0%
09/07/10 14:00:24 INFO mapred.JobClient:  map 6% reduce 0%
09/07/10 14:00:25 INFO mapred.JobClient:  map 12% reduce 0%
09/07/10 14:00:28 INFO mapred.JobClient:  map 31% reduce 0%
09/07/10 14:00:29 INFO mapred.JobClient:  map 50% reduce 0%
09/07/10 14:00:33 INFO mapred.JobClient:  map 66% reduce 0%
09/07/10 14:00:34 INFO mapred.JobClient:  map 72% reduce 0%
09/07/10 14:00:35 INFO mapred.JobClient:  map 75% reduce 0%
09/07/10 14:00:37 INFO mapred.JobClient:  map 75% reduce 1%
09/07/10 14:00:38 INFO mapred.JobClient:  map 78% reduce 5%
09/07/10 14:00:39 INFO mapred.JobClient:  map 89% reduce 10%
09/07/10 14:00:40 INFO mapred.JobClient:  map 89% reduce 11%
09/07/10 14:00:41 INFO mapred.JobClient:  map 90% reduce 11%
09/07/10 14:00:42 INFO mapred.JobClient:  map 99% reduce 14%
09/07/10 14:00:43 INFO mapred.JobClient:  map 99% reduce 16%
09/07/10 14:00:44 INFO mapred.JobClient:  map 99% reduce 18%
09/07/10 14:00:45 INFO mapred.JobClient:  map 99% reduce 19%
09/07/10 14:00:47 INFO mapred.JobClient:  map 99% reduce 22%
09/07/10 14:00:48 INFO mapred.JobClient:  map 100% reduce 22%
09/07/10 14:00:50 INFO mapred.JobClient:  map 100% reduce 24%
09/07/10 14:00:52 INFO mapred.JobClient:  map 100% reduce 25%
09/07/10 14:00:53 INFO mapred.JobClient:  map 100% reduce 26%
09/07/10 14:00:54 INFO mapred.JobClient:  map 100% reduce 27%
09/07/10 14:00:58 INFO mapred.JobClient:  map 100% reduce 33%
09/07/10 14:01:00 INFO mapred.JobClient:  map 100% reduce 34%
09/07/10 14:01:03 INFO mapred.JobClient:  map 100% reduce 39%
09/07/10 14:03:29 INFO mapred.JobClient:  map 100% reduce 40%
09/07/10 14:03:42 INFO mapred.JobClient:  map 100% reduce 41%
09/07/10 14:03:50 INFO mapred.JobClient:  map 100% reduce 47%
09/07/10 14:03:51 INFO mapred.JobClient:  map 100% reduce 50%
09/07/10 14:03:52 INFO mapred.JobClient:  map 100% reduce 56%
09/07/10 14:03:57 INFO mapred.JobClient:  map 100% reduce 57%
09/07/10 14:04:00 INFO mapred.JobClient:  map 100% reduce 58%
09/07/10 14:04:05 INFO mapred.JobClient:  map 100% reduce 59%
09/07/10 14:04:07 INFO mapred.JobClient:  map 100% reduce 60%
09/07/10 14:04:09 INFO mapred.JobClient:  map 100% reduce 66%
09/07/10 14:04:10 INFO mapred.JobClient:  map 100% reduce 67%
09/07/10 14:04:12 INFO mapred.JobClient:  map 100% reduce 68%
09/07/10 14:04:13 INFO mapred.JobClient:  map 100% reduce 69%
09/07/10 14:04:14 INFO mapred.JobClient:  map 100% reduce 70%
09/07/10 14:04:20 INFO mapred.JobClient:  map 100% reduce 79%
09/07/10 14:04:21 INFO mapred.JobClient:  map 100% reduce 80%
09/07/10 14:04:22 INFO mapred.JobClient:  map 100% reduce 81%
09/07/10 14:04:23 INFO mapred.JobClient:  map 100% reduce 82%
09/07/10 14:04:33 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:04:42 INFO mapred.JobClient: Task Id : attempt_200907101344_0002_m_000013_0, Status : FAILED
Too many fetch-failures
09/07/10 14:04:44 INFO mapred.JobClient:  map 93% reduce 87%
09/07/10 14:04:50 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:05:06 INFO mapred.JobClient:  map 100% reduce 93%
09/07/10 14:05:36 INFO mapred.JobClient:  map 100% reduce 94%
09/07/10 14:06:17 INFO mapred.JobClient: Job complete: job_200907101344_0002
09/07/10 14:06:17 INFO mapred.JobClient: Counters: 17
09/07/10 14:06:17 INFO mapred.JobClient:   File Systems
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes read=1077612760
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes written=1077285377
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes read=1083539214
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes written=2167083496
09/07/10 14:06:17 INFO mapred.JobClient:   Job Counters 
09/07/10 14:06:17 INFO mapred.JobClient:     Launched reduce tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Rack-local map tasks=2
09/07/10 14:06:17 INFO mapred.JobClient:     Launched map tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Data-local map tasks=15
09/07/10 14:06:17 INFO mapred.JobClient:   Map-Reduce Framework
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input groups=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Combine output records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map input records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Map output bytes=1074564177
09/07/10 14:06:17 INFO mapred.JobClient:     Map input bytes=1077284885
09/07/10 14:06:17 INFO mapred.JobClient:     Combine input records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input records=102341
Job ended: Fri Jul 10 14:06:17 IST 2009
The job took 357 seconds.

"
error : too many fetch failures,HDFS-485,datanode,,,,"i have a hadoop cluster configured with 1 physcical machine as name node and 7 data nodes(2 physcical+5 virtual).
When a sort job (hadoop-core-examples) is submitted it completes with the following error:can anyone tell me why and how to solve this issue.

hadoop version:0.18.3

O/P from terminal********************************
[root@hadoop1 hadoop-0.18.3]# bin/hadoop jar hadoop-0.18.3-examples.jar sort input13 output1
Running on 7 nodes to sort from hdfs://hadoop1:8022/user/root/input13 into hdfs://hadoop1:8022/user/root/output1 with 12 reduces.
Job started: Fri Jul 10 14:00:19 IST 2009
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.JobClient: Running job: job_200907101344_0002
09/07/10 14:00:20 INFO mapred.JobClient:  map 0% reduce 0%
09/07/10 14:00:24 INFO mapred.JobClient:  map 6% reduce 0%
09/07/10 14:00:25 INFO mapred.JobClient:  map 12% reduce 0%
09/07/10 14:00:28 INFO mapred.JobClient:  map 31% reduce 0%
09/07/10 14:00:29 INFO mapred.JobClient:  map 50% reduce 0%
09/07/10 14:00:33 INFO mapred.JobClient:  map 66% reduce 0%
09/07/10 14:00:34 INFO mapred.JobClient:  map 72% reduce 0%
09/07/10 14:00:35 INFO mapred.JobClient:  map 75% reduce 0%
09/07/10 14:00:37 INFO mapred.JobClient:  map 75% reduce 1%
09/07/10 14:00:38 INFO mapred.JobClient:  map 78% reduce 5%
09/07/10 14:00:39 INFO mapred.JobClient:  map 89% reduce 10%
09/07/10 14:00:40 INFO mapred.JobClient:  map 89% reduce 11%
09/07/10 14:00:41 INFO mapred.JobClient:  map 90% reduce 11%
09/07/10 14:00:42 INFO mapred.JobClient:  map 99% reduce 14%
09/07/10 14:00:43 INFO mapred.JobClient:  map 99% reduce 16%
09/07/10 14:00:44 INFO mapred.JobClient:  map 99% reduce 18%
09/07/10 14:00:45 INFO mapred.JobClient:  map 99% reduce 19%
09/07/10 14:00:47 INFO mapred.JobClient:  map 99% reduce 22%
09/07/10 14:00:48 INFO mapred.JobClient:  map 100% reduce 22%
09/07/10 14:00:50 INFO mapred.JobClient:  map 100% reduce 24%
09/07/10 14:00:52 INFO mapred.JobClient:  map 100% reduce 25%
09/07/10 14:00:53 INFO mapred.JobClient:  map 100% reduce 26%
09/07/10 14:00:54 INFO mapred.JobClient:  map 100% reduce 27%
09/07/10 14:00:58 INFO mapred.JobClient:  map 100% reduce 33%
09/07/10 14:01:00 INFO mapred.JobClient:  map 100% reduce 34%
09/07/10 14:01:03 INFO mapred.JobClient:  map 100% reduce 39%
09/07/10 14:03:29 INFO mapred.JobClient:  map 100% reduce 40%
09/07/10 14:03:42 INFO mapred.JobClient:  map 100% reduce 41%
09/07/10 14:03:50 INFO mapred.JobClient:  map 100% reduce 47%
09/07/10 14:03:51 INFO mapred.JobClient:  map 100% reduce 50%
09/07/10 14:03:52 INFO mapred.JobClient:  map 100% reduce 56%
09/07/10 14:03:57 INFO mapred.JobClient:  map 100% reduce 57%
09/07/10 14:04:00 INFO mapred.JobClient:  map 100% reduce 58%
09/07/10 14:04:05 INFO mapred.JobClient:  map 100% reduce 59%
09/07/10 14:04:07 INFO mapred.JobClient:  map 100% reduce 60%
09/07/10 14:04:09 INFO mapred.JobClient:  map 100% reduce 66%
09/07/10 14:04:10 INFO mapred.JobClient:  map 100% reduce 67%
09/07/10 14:04:12 INFO mapred.JobClient:  map 100% reduce 68%
09/07/10 14:04:13 INFO mapred.JobClient:  map 100% reduce 69%
09/07/10 14:04:14 INFO mapred.JobClient:  map 100% reduce 70%
09/07/10 14:04:20 INFO mapred.JobClient:  map 100% reduce 79%
09/07/10 14:04:21 INFO mapred.JobClient:  map 100% reduce 80%
09/07/10 14:04:22 INFO mapred.JobClient:  map 100% reduce 81%
09/07/10 14:04:23 INFO mapred.JobClient:  map 100% reduce 82%
09/07/10 14:04:33 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:04:42 INFO mapred.JobClient: Task Id : attempt_200907101344_0002_m_000013_0, Status : FAILED
Too many fetch-failures
09/07/10 14:04:44 INFO mapred.JobClient:  map 93% reduce 87%
09/07/10 14:04:50 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:05:06 INFO mapred.JobClient:  map 100% reduce 93%
09/07/10 14:05:36 INFO mapred.JobClient:  map 100% reduce 94%
09/07/10 14:06:17 INFO mapred.JobClient: Job complete: job_200907101344_0002
09/07/10 14:06:17 INFO mapred.JobClient: Counters: 17
09/07/10 14:06:17 INFO mapred.JobClient:   File Systems
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes read=1077612760
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes written=1077285377
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes read=1083539214
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes written=2167083496
09/07/10 14:06:17 INFO mapred.JobClient:   Job Counters 
09/07/10 14:06:17 INFO mapred.JobClient:     Launched reduce tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Rack-local map tasks=2
09/07/10 14:06:17 INFO mapred.JobClient:     Launched map tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Data-local map tasks=15
09/07/10 14:06:17 INFO mapred.JobClient:   Map-Reduce Framework
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input groups=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Combine output records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map input records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Map output bytes=1074564177
09/07/10 14:06:17 INFO mapred.JobClient:     Map input bytes=1077284885
09/07/10 14:06:17 INFO mapred.JobClient:     Combine input records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input records=102341
Job ended: Fri Jul 10 14:06:17 IST 2009
The job took 357 seconds.
"
Block Report Optimization: Queue block reports,HDFS-393,,,,,Use queuing to improve efficiency of block reports.
Add query whether storage has been initialized to DN's JMXBean,HDFS-14030,hdfs,,,,"During the short period while the DN is starting and the storage is not yet available we get the following errors:
{noformat}
ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException: Storage not yet initialized
{noformat}
I suggest to add an option to the DataNodeMXBean to query whether the storage has been initialized or not to surpass the exception."
TestDataNodeUUID#testUUIDRegeneration times out on Windows,HDFS-13568,,,,,"{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID{color}
{color:#d04437}[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.059 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID{color}
{color:#d04437}[ERROR] testUUIDRegeneration(org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID) Time elapsed: 10.006 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 10000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.net.DNS.resolveLocalHostname(DNS.java:284){color}
{color:#d04437} at org.apache.hadoop.net.DNS.<clinit>(DNS.java:61){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newBlockPoolID(NNStorage.java:989){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newNamespaceInfo(NNStorage.java:599){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:168){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1172){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:403){color}
{color:#d04437} at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:234){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1080){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration(TestDataNodeUUID.java:90){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestDataNodeUUID.testUUIDRegeneration:90 鈺?test timed out after 10000 millise...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0{color}

聽

{color:#333333}[Windows daily build|https://builds.apache.org/job/hadoop-trunk-win/467/testReport/org.apache.hadoop.hdfs.server.datanode/TestDataNodeUUID/testUUIDRegeneration/] also聽times out on this test.{color}"
TestPipelinesFailover times out on Windows,HDFS-13562,,,,,"testCompleteFileAfterCrashFailover times out, causing other tests to fail because they cannot start cluster:

{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover{color}
{color:#d04437}[ERROR] Tests run: 8, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 30.813 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover{color}
{color:#d04437}[ERROR] testCompleteFileAfterCrashFailover(org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover) Time elapsed: 30.009 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 30000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getLocalHostName(SecurityUtil.java:256){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.replacePattern(SecurityUtil.java:224){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getServerPrincipal(SecurityUtil.java:179){color}
{color:#d04437} at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:90){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:521){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:511){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:400){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:115){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:336){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:131){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:962){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1370){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:495){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2695){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2598){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1554){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:904){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.doWriteOverFailoverTest(TestPipelinesFailover.java:143){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testCompleteFileAfterCrashFailover(TestPipelinesFailover.java:128){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] testWriteOverGracefulFailoverWithDnFail(org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover) Time elapsed: 0.055 s <<< ERROR!{color}
{color:#d04437}java.io.IOException: Could not fully delete D:\OSS\hadoop-branch-2\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\name1{color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1047){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.doTestWriteOverFailoverWithDnFail(TestPipelinesFailover.java:221){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail(TestPipelinesFailover.java:203){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}...{color}

聽

{color:#d04437}[INFO]{color}

{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testAllocateBlockAfterCrashFailover:122->doWriteOverFailoverTest:143 鈺?IO{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testCompleteFileAfterCrashFailover:128->doWriteOverFailoverTest:143 鈺梴color}
{color:#d04437}[ERROR] TestPipelinesFailover.testFailoverRightBeforeCommitSynchronization:338 鈺?IO Co...{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testLeaseRecoveryAfterFailover:283 鈺?IO Could not fully ...{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testPipelineRecoveryStress:455 鈺?IO Could not fully dele...{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testWriteOverCrashFailoverWithDnFail:208->doTestWriteOverFailoverWithDnFail:221 鈺?IO{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testWriteOverGracefulFailover:116->doWriteOverFailoverTest:143 鈺?IO{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail:203->doTestWriteOverFailoverWithDnFail:221 鈺?IO{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 8, Failures: 0, Errors: 8, Skipped: 0{color}"
"TestFileAppend.testAppendCorruptedBlock,TestFileAppend.testConcurrentAppendRead time out on Windows",HDFS-13552,,,,,"{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.TestFileAppend{color}
{color:#d04437}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 20.073 s <<< FAILURE! - in org.apache.hadoop.hdfs.TestFileAppend{color}
{color:#d04437}[ERROR] testConcurrentAppendRead(org.apache.hadoop.hdfs.TestFileAppend) Time elapsed: 10.005 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 10000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.net.DNS.resolveLocalHostname(DNS.java:284){color}
{color:#d04437} at org.apache.hadoop.net.DNS.<clinit>(DNS.java:61){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newBlockPoolID(NNStorage.java:989){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newNamespaceInfo(NNStorage.java:599){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:168){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1172){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:403){color}
{color:#d04437} at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:234){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1080){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.TestFileAppend.testConcurrentAppendRead(TestFileAppend.java:701){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] testAppendCorruptedBlock(org.apache.hadoop.hdfs.TestFileAppend) Time elapsed: 10.001 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 10000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getLocalHostName(SecurityUtil.java:256){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.replacePattern(SecurityUtil.java:224){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getServerPrincipal(SecurityUtil.java:179){color}
{color:#d04437} at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:90){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:521){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:511){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:400){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:115){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:336){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:162){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:889){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:725){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1215){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1090){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.TestFileAppend.testAppendCorruptedBlock(TestFileAppend.java:674){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestFileAppend.testAppendCorruptedBlock:674 鈺?test timed out after 10000 mill...{color}
{color:#d04437}[ERROR] TestFileAppend.testConcurrentAppendRead:701 鈺?test timed out after 10000 mill...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0{color}"
TestDataNodeMultipleRegistrations#testClusterIdMismatchAtStartupWithHA times out intermittently on Windows,HDFS-13569,,,,,"On Windows,聽TestDataNodeMultipleRegistrations#testClusterIdMismatchAtStartupWithHA may time out and cause subsequent tests to fail.

{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations{color}
{color:#d04437}[ERROR] Tests run: 6, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 65.082 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations{color}
{color:#d04437}[ERROR] testClusterIdMismatchAtStartupWithHA(org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations) Time elapsed: 20.003 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 20000 milliseconds{color}
{color:#d04437} at java.net.NetworkInterface.getAll(Native Method){color}
{color:#d04437} at java.net.NetworkInterface.getNetworkInterfaces(NetworkInterface.java:343){color}
{color:#d04437} at org.apache.htrace.core.TracerId.getBestIpString(TracerId.java:179){color}
{color:#d04437} at org.apache.htrace.core.TracerId.processShellVar(TracerId.java:145){color}
{color:#d04437} at org.apache.htrace.core.TracerId.<init>(TracerId.java:116){color}
{color:#d04437} at org.apache.htrace.core.Tracer$Builder.build(Tracer.java:159){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:940){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1215){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1090){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.testClusterIdMismatchAtStartupWithHA(TestDataNodeMultipleRegistrations.java:248){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] test2NNRegistration(org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations) Time elapsed: 0.029 s <<< ERROR!{color}
{color:#d04437}java.io.IOException: Could not fully delete E:\OSS\hadoop-branch-2\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\name1{color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1047){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.test2NNRegistration(TestDataNodeMultipleRegistrations.java:69){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26){color}
{color:#d04437} at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271){color}
{color:#d04437} at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70){color}
{color:#d04437} at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50){color}
{color:#d04437} at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238){color}
{color:#d04437} at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63){color}
{color:#d04437} at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236){color}
{color:#d04437} at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53){color}
{color:#d04437} at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229){color}
{color:#d04437} at org.junit.runners.ParentRunner.run(ParentRunner.java:309){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413){color}

聽

{color:#d04437}...{color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.test2NNRegistration:69 鈺?IO Could not fully ...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testClusterIdMismatch:204 鈺?IO Could not ful...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testClusterIdMismatchAtStartupWithHA:248 鈺?...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testDNWithInvalidStorageWithHA:273 鈺?IO Coul...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testMiniDFSClusterWithMultipleNN:314 鈺?IO Co...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 6, Failures: 0, Errors: 5, Skipped: 0{color}"
TestTransferFsImage#testGetImageTimeout times out intermittently on Windows,HDFS-13561,,,,,"{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage{color}
{color:#d04437}[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 32.695 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage{color}
{color:#d04437}[ERROR] testGetImageTimeout(org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage) Time elapsed: 5.002 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 5000 milliseconds{color}
{color:#d04437} at java.net.SocketInputStream.socketRead0(Native Method){color}
{color:#d04437} at java.net.SocketInputStream.socketRead(SocketInputStream.java:116){color}
{color:#d04437} at java.net.SocketInputStream.read(SocketInputStream.java:171){color}
{color:#d04437} at java.net.SocketInputStream.read(SocketInputStream.java:141){color}
{color:#d04437} at java.io.BufferedInputStream.fill(BufferedInputStream.java:246){color}
{color:#d04437} at java.io.BufferedInputStream.read1(BufferedInputStream.java:286){color}
{color:#d04437} at java.io.BufferedInputStream.read(BufferedInputStream.java:345){color}
{color:#d04437} at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704){color}
{color:#d04437} at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647){color}
{color:#d04437} at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569){color}
{color:#d04437} at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474){color}
{color:#d04437} at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.doGetUrl(TransferFsImage.java:433){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:418){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage.testGetImageTimeout(TestTransferFsImage.java:133){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestTransferFsImage.testGetImageTimeout:133 鈺?test timed out after 5000 milli...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0{color}"
TestNetworkTopology#testInvalidNetworkTopologiesNotCachedInHdfs times out on Windows,HDFS-13555,,,,,"Although TestNetworkTopology#testInvalidNetworkTopologiesNotCachedInHdfs has 180s timeout, it is overridden by global timeout
{code:java}
@Rule
 public Timeout testTimeout = new Timeout(30000);{code}
{color:#d04437}[INFO] Running org.apache.hadoop.net.TestNetworkTopology{color}
{color:#d04437}[ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.741 s <<< FAILURE! - in org.apache.hadoop.net.TestNetworkTopology{color}
{color:#d04437}[ERROR] testInvalidNetworkTopologiesNotCachedInHdfs(org.apache.hadoop.net.TestNetworkTopology) Time elapsed: 30.009 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 30000 milliseconds{color}
{color:#d04437} at java.lang.Object.wait(Native Method){color}
{color:#d04437} at java.lang.Thread.join(Thread.java:1257){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:26){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestNetworkTopology>Object.wait:-2 鈺?test timed out after 30000 milliseconds{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0{color}"
Why we don't add a harder lease expiration limit.,HDFS-13288,namenode,,,,"Currently there exists a soft expire timeout(1 minutes by default) and hard expire timeout(60 minutes by default).聽

On our production environment. Some client began writing a file long time(more than one year) ago, when writing finished and tried to close the output stream, the client failed closing it (for some IOException. etc. ).聽 But the client process is a background service, it doesn't exit. So the lease doesn't released for more than one year.

The problem is that, the lease for the file is occupied, we have to call recover lease on the file when doing demission or appending operation.

聽

So I am聽wondering why we don't add a more harder lease expire timeout, when a lease lasts too long (maybe one month),聽 revoke it.聽

聽"
IllegalStateException: Unable to finalize edits file,HDFS-13206,,,,,"I noticed the following in hbase test output running against hadoop3:
{code}
2018-02-28 18:40:18,491 ERROR [Time-limited test] namenode.JournalSet(402): Error: finalize log segment 1, 658 failed for (journal JournalAndStream(mgr=FileJournalManager(root=/mnt/disk2/a/2-hbase/hbase-server/target/test-data/5670112c-31f1-43b0-af31-c1182e142e63/cluster_8f993609-c3a1-4fb4-8b3d-0e642261deb1/dfs/name-0-1), stream=null))
java.lang.IllegalStateException: Unable to finalize edits file /mnt/disk2/a/2-hbase/hbase-server/target/test-data/5670112c-31f1-43b0-af31-c1182e142e63/cluster_8f993609-c3a1-4fb4-8b3d-0e642261deb1/dfs/name-0-1/current/edits_inprogress_0000000000000000001
  at org.apache.hadoop.hdfs.server.namenode.FileJournalManager.finalizeLogSegment(FileJournalManager.java:153)
  at org.apache.hadoop.hdfs.server.namenode.JournalSet$2.apply(JournalSet.java:224)
  at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:385)
  at org.apache.hadoop.hdfs.server.namenode.JournalSet.finalizeLogSegment(JournalSet.java:219)
  at org.apache.hadoop.hdfs.server.namenode.FSEditLog.endCurrentLogSegment(FSEditLog.java:1427)
  at org.apache.hadoop.hdfs.server.namenode.FSEditLog.close(FSEditLog.java:398)
  at org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync.close(FSEditLogAsync.java:110)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1320)
  at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1909)
  at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
  at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1013)
  at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2047)
  at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1987)
  at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1958)
  at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1951)
  at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:767)
  at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:1109)
  at org.apache.hadoop.hbase.master.balancer.TestFavoredNodeTableImport.stopCluster(TestFavoredNodeTableImport.java:71)
{code}"
Allow dfs.data.transfer.protection default to hadoop.rpc.protection,HDFS-6859,security,,,,"Currently administrator needs to configure both _dfs.data.transfer.protection_ and _hadoop.rpc.protection_ to specify _QOP_ for rpc and data transfer protocols. In some cases, the values for these two properties will be same. In those cases, it may be easier to allow dfs.data.transfer.protection default to hadoop.rpc.protection.
This also ensures that an admin will get QOP as _Authentication_ if admin  does not specify either of those values.

Separate jiras  (HDFS-6858 and HDFS-6859) are created for dfs.data.transfer.saslproperties.resolver.class and dfs.data.transfer.protection respectively.
"
The DFSUsed value bigger than the Capacity,HDFS-13034,hdfs,,,,"||Node||Last contact||Admin State||Capacity||Used||Non DFS Used||Remaining||Blocks||Block pool used||Failed Volumes||Version||
|A|0|In Service|20.65 TB|18.26 TB|0 B|1.27 TB|24330|2.57 TB (12.42%)|0|2.7.1|
|B|2|In Service|5.47 TB|12.78 TB|0 B|1.46 TB|27657|2.65 TB (48.37%)|0|2.7.1|"
StreamCapability enums are not displayed in javadoc,HDFS-12604,hdfs,,,,"http://hadoop.apache.org/docs/r3.0.0-beta1/api/org/apache/hadoop/fs/StreamCapabilities.html

{{StreamCapability#HFLUSH}} and {{StreamCapability#HSYNC}} are not displayed in the doc."
backport HDFS-3568 (add security to fuse_dfs) to branch-1,HDFS-3638,,,,,"Backport HDFS-3568 to branch-1.  This will give fuse_dfs support for Kerberos authentication, allowing FUSE to be used in a secure cluster."
handling of corrupt blocks not suitable for commodity hardware,HDFS-12649,namenode,,,,"Hadoop's documentation tells me it's suitable for commodity hardware in the sense that hardware failures are expected to happen frequently. However, there is currently no automatic handling of corrupted blocks, which seems a bit contradictory to me.

See: https://stackoverflow.com/questions/19205057/how-to-fix-corrupt-hdfs-files

This is even problematic for data integrity as the redundancy is not kept at the desired level without manual intervention and therefore in a timely manner. If there is a corrupted block, I would at least expect that the namenode forces the creation of an additional good replica to keep up the redundancy level, ie. the redundancy level should never include corrupted data... which it currently does:

    ""UnderReplicatedBlocks"" : 0,
    ""CorruptBlocks"" : 2,

(namenode /jmx http dump)"
TestCheckpoint#testCheckpoint may fail due to Bad value assertion,HDFS-5834,,,,,"I saw the following when running test suite on Linux:
{code}
testCheckpoint(org.apache.hadoop.hdfs.server.namenode.TestCheckpoint)  Time elapsed: 3.058 sec  <<< FAILURE!
java.lang.AssertionError: Bad value for metric GetImageNumOps
Expected: gt(0)
     got: <0L>

        at org.junit.Assert.assertThat(Assert.java:780)
        at org.apache.hadoop.test.MetricsAsserts.assertCounterGt(MetricsAsserts.java:318)
        at org.apache.hadoop.hdfs.server.namenode.TestCheckpoint.testCheckpoint(TestCheckpoint.java:1058)
{code}"
TestHttpsFileSystem intermittently fails with Port in use error,HDFS-5718,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1634/testReport/junit/org.apache.hadoop.hdfs.web/TestHttpsFileSystem/org_apache_hadoop_hdfs_web_TestHttpsFileSystem/ :
{code}
java.net.BindException: Port in use: localhost:50475
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:383)
	at java.net.ServerSocket.bind(ServerSocket.java:328)
	at java.net.ServerSocket.<init>(ServerSocket.java:194)
	at javax.net.ssl.SSLServerSocket.<init>(SSLServerSocket.java:106)
	at com.sun.net.ssl.internal.ssl.SSLServerSocketImpl.<init>(SSLServerSocketImpl.java:108)
	at com.sun.net.ssl.internal.ssl.SSLServerSocketFactoryImpl.createServerSocket(SSLServerSocketFactoryImpl.java:72)
	at org.mortbay.jetty.security.SslSocketConnector.newServerSocket(SslSocketConnector.java:478)
	at org.mortbay.jetty.bio.SocketConnector.open(SocketConnector.java:73)
	at org.apache.hadoop.http.HttpServer.openListeners(HttpServer.java:973)
	at org.apache.hadoop.http.HttpServer.start(HttpServer.java:914)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:412)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:769)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:315)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1846)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1746)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1203)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:342)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:323)
	at org.apache.hadoop.hdfs.web.TestHttpsFileSystem.setUp(TestHttpsFileSystem.java:64)
{code}
This could have been caused by concurrent test(s)."
Ozone: TestBlockDeletingService#testBlockDeletionTimeout sometimes timeout,HDFS-12401,HDFS-7240,,,,"{code}
testBlockDeletionTimeout(org.apache.hadoop.ozone.container.common.TestBlockDeletingService)  Time elapsed: 100.383 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
{code}"
Block Storage : make the server address config more concise,HDFS-12041,hdfs,,,,"Currently there are a few places where the address are read from config like such 
{code}
    String cbmIPAddress = ozoneConf.get(
        DFS_CBLOCK_JSCSI_CBLOCK_SERVER_ADDRESS_KEY,
        DFS_CBLOCK_JSCSI_CBLOCK_SERVER_ADDRESS_DEFAULT
    );
    int cbmPort = ozoneConf.getInt(
        DFS_CBLOCK_JSCSI_PORT_KEY,
        DFS_CBLOCK_JSCSI_PORT_DEFAULT
    );
{code}
Similarly for jscsi address config. Maybe we should consider merge these to one single key config in form of host:port."
TestWebHdfsTimeouts fails due to null SocketTimeoutException,HDFS-11059,webhdfs,,,,"TestWebHdfsTimeouts expects SocketTimeoutException with ""connect timed out"" or ""Read timed out"" message but fails when encountering ""null"" message sometimes. Occurred 4 out of 100 tests.

{{SocksSocketImpl#remainingMillis}} may send null SocketTimeoutException:
{code}
    private static int remainingMillis(long deadlineMillis) throws IOException {
        if (deadlineMillis == 0L)
            return 0;

        final long remaining = deadlineMillis - System.currentTimeMillis();
        if (remaining > 0)
            return (int) remaining;

        throw new SocketTimeoutException();   <<<<==
    }
{code}"
Include event for AddBlock in Inotify Event Stream,HDFS-10608,,,,,"It would be nice to have an AddBlockEvent in the INotify pipeline.  Based on discussions from mailing list:

http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201607.mbox/%3C1467743792.4040080.657624289.7BE240AD%40webmail.messagingengine.com%3E"
Dead Code in DFS Util for DFSUtil#substituteForWildcardAddress,HDFS-8609,namenode,,,,"Dead code after JDK 1.4

{code}
    otherHttpAddr = DFSUtil.getInfoServerWithDefaultHost(
        otherIpcAddr.getHostName(), otherNode, scheme).toURL();
{code}

In {{DFSUtil#substituteForWildcardAddress}} 
{code}
 if (addr != null && addr.isAnyLocalAddress()) {
...
}
{code}

addr.isAnyLocalAddress() will always return false.

Always the url will be formed with address which is configured  in hdfs-site.xml .Same will affect bootStrap from NN and ssl certificate check"
combine FsShell.copyToLocal to ChecksumFileSystem.copyToLocalFile,HDFS-291,,,,,"Two methods provide similar functions

- ChecksumFileSystem.copyToLocalFile(Path src, Path dst, boolean copyCrc) is no longer used anywhere in the system

- It is better to use ChecksumFileSystem.getRawFileSystem() for copying crc in FsShell.copyToLocal

- FileSystem.isDirectory(Path) used in FsShell.copyToLocal is deprecated."
Remove deprecated FileSystem#getDefault* and getServerDefault methods that don't take a Path argument ,HDFS-11228,fs,,,,"FileSystem#getServerDefaults(), #getDefaultReplication, #getDefaultBlockSize were deprecated by HADOOP-8422 and the fix version is 2.0.2-alpha. They can be removed in Hadoop 3."
Improve BKJM documentation,HDFS-4613,documentation,,,,"The documentation tags BKJM as experimental, and I don't think it is experimental any longer. It is also missing a link of HDFS HA with BKJM on the left-hand side menu of the documentation."
FSDirAttrOp#setOwner throws ACE with misleading message,HDFS-10378,namenode,,,,"Calling {{setOwner}} as a non-super user does trigger {{AccessControlException}}, however, the message ""Permission denied. user=user1967821757 is not the owner of inode=child"" is wrong. Expect this message: ""Non-super user cannot change owner"".

Output of patched unit test {{TestPermission.testFilePermission}}:
{noformat}
2016-05-06 16:45:44,915 [main] INFO  security.TestPermission (TestPermission.java:testFilePermission(280)) - GOOD: got org.apache.hadoop.security.AccessControlException: Permission denied. user=user1967821757 is not the owner of inode=child1
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkOwner(FSPermissionChecker.java:273)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:250)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1642)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1626)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkOwner(FSDirectory.java:1595)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:88)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1717)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:835)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:481)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:665)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2423)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2419)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1755)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2417)
{noformat}

Will upload the unit test patch shortly."
Create accessor methods for DataNode#data and DataNode#isBlockTokenEnabled,HDFS-9028,,,,,Currently both DataNode#data and DataNode#isBlockTokenEnabled instance variables are package scoped with no accessor methods. This makes mocking in unit tests difficult. This jira is to make them private scoped with proper getters and setters.
DistributedFileSystem.listLocatedStatus() should return HdfsBlockLocation instead of BlockLocation,HDFS-10466,hdfs,,,,"https://issues.apache.org/jira/browse/HDFS-202 added a new API listLocatedStatus() to get all files' status with block locations for a directory. This is great that we don't need to call FileSystem.getFileBlockLocations() for each file. it's much faster (about 8-10 times).
However, the returned LocatedFileStatus only contains basic BlockLocation instead of HdfsBlockLocation, the LocatedBlock details are stripped out.

It should do the similar as DFSClient.getBlockLocations(), return HdfsBlockLocation which provide full block location details.

The implementation of DistributedFileSystem. listLocatedStatus() retrieves HdfsLocatedFileStatus which contains all information, but when convert it to LocatedFileStatus, it doesn't keep LocatedBlock data. It's a simple (and compatible) change to make to keep the LocatedBlock details."
FsShell get command does not support writing to stdout,HDFS-4869,,,,,"In FsShell the put command supports using ""\-"" in place of stdin, but this functionality (""\-"" in place of stdout) is broken in the get command."
"""hdfs dfs"" command usage still shows ""hadoop fs""",HDFS-3435,,,,,"""hdfs dfs"" command usage still shows ""hadoop fs"".
{code}
$ hdfs dfs
Usage: hadoop fs [generic options]
{code}

The command should be ""hdfs dfs""."
"Rename ""path.based"" caching configuration options",HDFS-5551,datanode,namenode,,,"Some configuration options still have the ""path.based"" moniker, missed during the big rename removing this naming convention."
libhdfs doesn't work with jamVM,HDFS-4387,libhdfs,,,,"Building and running tests on OpenJDK 7 on Ubuntu 12.10 fails with {{mvn test -Pnative}}.  The output is hard to decipher but the underlying issue is that {{test_libhdfs_native}} segfaults at startup.

{noformat}
(gdb) run
Starting program: /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/target/native/test_libhdfs_threaded
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".

Program received signal SIGSEGV, Segmentation fault.
0x00007ffff739a897 in attachJNIThread (name=0x0, is_daemon=is_daemon@entry=0 '\000', group=0x0) at thread.c:768
768 thread.c: No such file or directory.
(gdb) where
#0 0x00007ffff739a897 in attachJNIThread (name=0x0, is_daemon=is_daemon@entry=0 '\000', group=0x0) at thread.c:768
#1 0x00007ffff7395020 in attachCurrentThread (is_daemon=0, args=0x0, penv=0x7fffffffddb8) at jni.c:1454
#2 Jam_AttachCurrentThread (vm=<optimized out>, penv=0x7fffffffddb8, args=0x0) at jni.c:1466
#3 0x00007ffff7bcf979 in getGlobalJNIEnv () at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:527
#4 getJNIEnv () at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:585
#5 0x0000000000402512 in nmdCreate (conf=conf@entry=0x7fffffffdeb0) at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.c:49
#6 0x00000000004016e1 in main () at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:283
{noformat}"
Some usage of GetConf tool not compatible with federation,HDFS-4202,federation,scripts,,,"In the start/stop scripts, we use the ""getconf"" tool to look up certain config keys to discover hosts we need to start/stop daemons on. However, some of these configs are optionally suffixed by a nameservice or nameservice.namenode identifier. In these cases, the start/stop scripts wouldn't correctly see the configs, and therefore would fail to start all the necessary daemons for the federated cluster."
Hdfs audit shouldn't log mkdir operaton if the directory already exists.,HDFS-10305,namenode,,,,"Currently Hdfs audit logs mkdir operation even if the directory already exists.
This creates confusion while analyzing audit logs."
Is DataNode aware of the name of the file that it is going to store?,HDFS-10252,datanode,namenode,,,"I am going through the HDFS Namenode and Datanode code and I am trying to see if the DataNode is aware of the names of the files that are stored in it (and other metadata as well).

Assuming that we have the most simple case: 
1 NameNode
1 DataNode
1 single machine running HDFS with replication factor 1. 

and considering the way HDFS works a use case could be: 
A client requests to write a file from local to HDFS (for example: ""hdfs dfs -put file /file"")
He first communicates with NameNode and gets where this file should be stored.
Then, after receiving an answer, he requests to the DataNode to store that file.

(At that point I am going to be a little more specific about the code)
The DataNode has a DataXceiverServer class which runs and waits for requests. When a request comes, it starts a DataXceiver thread and try to serve that request. What I would like to know is, if at that specific point the DataNode knows the name of the file that it is going to store. I spent hours of debugging but I could not find it. Is it somewhere there, or only the NameNode knows the name of that file?"
Tests that use KeyStoreUtil must call KeyStoreUtil.cleanupSSLConfig(),HDFS-9309,,,,,"When KeyStoreUtil.setupSSLConfig() is called, several files are created (ssl-server.xml, ssl-client.xml, trustKS.jks, clientKS.jks, serverKS.jks). However, if they are not deleted upon exit, weird thing can happen to any subsequent tests.

For example, if ssl-client.xml is not delete, but trustKS.jks is deleted, TestWebHDFSOAuth2.listStatusReturnsAsExpected will fail with message:
{noformat}
java.io.IOException: Unable to load OAuth2 connection factory.
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:164)
	at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:81)
	at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:215)
	at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:131)
	at org.apache.hadoop.hdfs.web.URLConnectionFactory.newSslConnConfigurator(URLConnectionFactory.java:138)
	at org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:112)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:163)
	at org.apache.hadoop.hdfs.web.TestWebHDFSOAuth2.listStatusReturnsAsExpected(TestWebHDFSOAuth2.java:147)
{noformat}

There are currently several tests that do not clean up:

{noformat}

130 鉁?weichiu@weichiu ~/trunk (trunk) $ grep -rnw . -e 'KeyStoreTestUtil\.setupSSLConfig' | cut -d: -f1 |xargs grep -L ""KeyStoreTestUtil\.cleanupSSLConfig""
./hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/webapp/TestTimelineWebServicesWithSSL.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTokens.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferTestCase.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRespectsBindHostKeys.java
./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/TestHttpFSFWithSWebhdfsFileSystem.java
{noformat}

This JIRA is the effort to remove the bug."
SnapshotDiffReport$DiffReportEntry$hashCode should use type field,HDFS-9573,,,,,"DiffReportEntry.equals() uses field ""type"", but DiffReportEntry.hashCode() doesn't. This breaks the rules on equals and hashCode:
* if a class overrides equals, it must override hashCode
* when they are both overridden, equals and hashCode must use the same set of fields"
"jsvc should be listed as a dependency for ""package"" and ""bin-package""",HDFS-3316,build,,,,"The dependency on jsvc of targets ""package"" and ""bin-package"" would be much clearer if made explicit, as in the proposed patch.  (However, the larger issue of HADOOP-8364 should be addressed first.)"
Adding new target to build.xml to run test-core without compiling,HDFS-1494,build,,,,"While testing Apache Harmony Select (lightweight version of Harmony) with Hadoop hdfs we had to first build with Harmony and then test using Harmony Select using the test-core target. This was done in an effort to investigate any issues with Harmony Select in running common. However, the test-core target also compiles the classes which we are unable to do with Harmony Select. A new target is proposed that only runs the tests without compiling them."
Inconsistent log level practice,HDFS-8840,,,,,"In method ""checkLogsAvailableForRead()"" of class: hadoop-2.7.1-src\hadoop-hdfs-project\hadoop-hdfs\src\main\java\org\apache\hadoop\hdfs\server\namenode\ha\BootstrapStandby.java

The log level is not correct, after checking ""LOG.isDebugEnabled()"", we should use ""LOG.debug(msg, e);"", while now we use "" LOG.fatal(msg, e);"". Log level is inconsistent.

the source code of this method is:
private boolean checkLogsAvailableForRead(FSImage image, long imageTxId, long curTxIdOnOtherNode) {

  ...
    } catch (IOException e) {
   ...
      if (LOG.isDebugEnabled()) {
        LOG.fatal(msg, e);
      } else {
        LOG.fatal(msg);
      }
      return false;
    }
  }
"
Random VolumeChoosingPolicy,HDFS-9551,datanode,,,,Please find attached a new implementation of VolumeChoosingPolicy.  This implementation chooses volumes at random to place blocks.  It is thread-safe and un-synchronized so there is less thread contention.
When dfs.block.size is configured to 0 the block which is created in rbw is never deleted,HDFS-3356,namenode,,,,"dfs.block.size=0
step 1: start NN and DN
step 2: write a file ""a.txt""
The block is created in rbw and since the blocksize is 0 write fails and the file is not closed. DN sents in the block report , number of blocks as 1
Even after the DN has sent the block report and directory scan has been done , the block is not invalidated for ever.

But In earlier version when the block.size is configured to 0 default value will be taken and write will be successful.
NN logs:
========
{noformat}
2012-04-24 19:54:27,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* processReport: from DatanodeRegistration(.18.40.117, storageID=DS-452047493-xx.xx.xx.xx-50076-1335277451277, infoPort=50075, ipcPort=50077, storageInfo=lv=-40;cid=CID-742fda5f-68f7-40a5-9d52-a2a15facc6af;nsid=797082741;c=0), blocks: 0, processing time: 0 msecs
2012-04-24 19:54:29,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /1._COPYING_. BP-1612285678-xx.xx.xx.xx-1335277427136 blk_-262107679534121671_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[xx.xx.xx.xx:50076|RBW]]}
2012-04-24 19:54:30,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* processReport: from DatanodeRegistration(xx.xx.xx.xx, storageID=DS-452047493-xx.xx.xx.xx-50076-1335277451277, infoPort=50075, ipcPort=50077, storageInfo=lv=-40;cid=CID-742fda5f-68f7-40a5-9d52-a2a15facc6af;nsid=797082741;c=0), blocks: 1, processing time: 0 msecs{noformat}

Exception message while writing a file:
=======================================
{noformat}
./hdfs dfs -put hadoop /1
12/04/24 19:54:30 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: BlockSize 0 is smaller than data size.  Offset of packet in block 4745 Aborting file /1._COPYING_
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:467)
put: BlockSize 0 is smaller than data size.  Offset of packet in block 4745 Aborting file /1._COPYING_
12/04/24 19:54:30 ERROR hdfs.DFSClient: Failed to close file /1._COPYING_
java.io.IOException: BlockSize 0 is smaller than data size.  Offset of packet in block 4745 Aborting file /1._COPYING_
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:467){noformat}"
Implement a unix-like cat utility,HDFS-9272,hdfs-client,,,,"Implement the basic functionality of ""cat"" and have it build as a separate executable.

2 Reasons for this:
We don't have any real integration tests at the moment so something simple to verify that the library actually works against a real cluster is useful.

Eventually I'll make more utilities like stat, mkdir etc.  Once there are enough of them it will be simple to make a C++ implementation of the hadoop fs command line interface that doesn't take the latency hit of spinning up a JVM."
TestOfflineImageViewer.outputOfLSVisitor fails for certain usernames,HDFS-6540,,,,,"TestOfflineImageViewer.outputOfLSVisitor() fails if the username contains ""-"" (dash). A dash is a valid character in a username."
Non Authenticated Data node Allowed to Join HDFS,HDFS-8906,datanode,namenode,,,"An attacker with network access to a Hadoop cluster can create a spoof datanode that the namenode will accept into the cluster without authentication, allowing the attacker to run MapReduce jobs on the cluster in order to steal data.  The spoof datanode is created by adding the namenode RSA SSH public key to the known hosts directory, starting Hadoop services, setting the IP address to be the same as a legitimate node on the Hadoop cluster and sending the namenode a heartbeat message with an empty namespace ID.  This will cause the namenode to think that the spoof datanode is a node that had previously crashed and lost its data.  The namenode will then connect to the spoof datanode using its SSH credentials and start replicating data on the spoof datanode, incorporating the spoof datanode into the cluster.  Once incorporated, the spoof node can start issuing MapReduce jobs to retrieve cluster data."
DataNode should be marked as final to prevent subclassing,HDFS-190,,,,,"Reviewing the DataNode core, it starts a thread in its constructor calling back in to the Run() method. This is generally perceived as very dangerous, as if DataNode were ever subclassed, the subclass would start to be invoked in the run() method before its own constructor had finished working.

1. Consider splitting the constructor from the start() operation.
2. If this cannot be changed, mark DataNode as final so nobody can subclass it.  Though if the latter were done, it would be convenient to have a method to let external management components poll for the health of the node, and to pick up reasons for the node shutting down."
QUEUE_WITH_CORRUPT_BLOCKS is no longer needed,HDFS-9344,namenode,,,,"After the change of HDFS-9205, the {{QUEUE_WITH_CORRUPT_BLOCKS}} queue in {{UnderReplicatedBlocks}} is no longer needed."
Are there any official performance tests or reports using WebHDFS,HDFS-9212,webhdfs,,,,I'd like to know if there are any performance tests or reports when reading and writing files using WebHDFS rest api. Or any design-time numbers?
DataXceiver per accept seems to be a bottleneck in HBase/YCSB test,HDFS-2243,datanode,,,,"I am running the YCSB benchmark against HBase, sometimes against a single node, sometimes against a cluster of 6 systems. As the load increases into thousands of TPS, especially on the single node, I can see that the datanode runs very high system time and seems to be bottlenecked by how fast it can create the threads to handle the new connections in DataXceiverServer.run. By ""perf top"" I can see the process spends about 12% of all its time in pthread_create, and in hprof profiles I can see there are tens of thousands of threads created in just a few minutes of test execution.

Does anyone else observe this bottleneck? Is there a major challenge to using a thread pool of DataXceivers in this situation?
"
need log out an extra info in DFSOutputstream,HDFS-4770,hdfs-client,,,,need log out an extra info in DFSOutputstream
UserGroupInformation.loginUserFromKeytab will hang forever if keytab file length  is less than 6 byte.,HDFS-6674,security,,,,"The jstack is as follows:
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.available(Native Method)
	at java.io.BufferedInputStream.available(BufferedInputStream.java:399)
	- locked <0x0000000745585330> (a sun.security.krb5.internal.ktab.KeyTabInputStream)
	at sun.security.krb5.internal.ktab.KeyTab.load(KeyTab.java:257)
	at sun.security.krb5.internal.ktab.KeyTab.<init>(KeyTab.java:97)
	at sun.security.krb5.internal.ktab.KeyTab.getInstance0(KeyTab.java:124)
	- locked <0x0000000745586560> (a java.lang.Class for sun.security.krb5.internal.ktab.KeyTab)
	at sun.security.krb5.internal.ktab.KeyTab.getInstance(KeyTab.java:157)
	at javax.security.auth.kerberos.KeyTab.takeSnapshot(KeyTab.java:119)
	at javax.security.auth.kerberos.KeyTab.getEncryptionKeys(KeyTab.java:192)
	at javax.security.auth.kerberos.JavaxSecurityAuthKerberosAccessImpl.keyTabGetEncryptionKeys(JavaxSecurityAuthKerberosAccessImpl.java:36)
	at sun.security.jgss.krb5.Krb5Util.keysFromJavaxKeyTab(Krb5Util.java:381)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:701)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:784)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$5.run(LoginContext.java:721)
	at javax.security.auth.login.LoginContext$5.run(LoginContext.java:719)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokeCreatorPriv(LoginContext.java:718)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:590)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:679)"
"HDFS mover stuck in loop trying to move corrupt block with no other valid replicas, doesn't move rest of other data blocks",HDFS-8341,balancer & mover,,,,"HDFS mover gets stuck looping on a block that fails to move and doesn't migrate the rest of the blocks.

This is preventing recovery of data from a decomissioning external storage tier used for archive (we've had problems with that proprietary ""hyperscale"" storage product which is why a couple blocks here and there have checksum problems or premature eof as shown below), but this should not prevent moving all the other blocks to recover our data:
{code}hdfs mover -p /apps/hive/warehouse/<custom_scrubbed>
15/05/07 14:52:50 INFO mover.Mover: namenodes = {hdfs://nameservice1=[/apps/hive/warehouse/<custom_scrubbed>]}
15/05/07 14:52:51 INFO balancer.KeyManager: Block token params received from NN: update interval=10hrs, 0sec, token lifetime=10hrs, 0sec
15/05/07 14:52:51 INFO block.BlockTokenSecretManager: Setting block keys
15/05/07 14:52:51 INFO balancer.KeyManager: Update block keys every 2hrs, 30mins, 0sec
15/05/07 14:52:52 INFO block.BlockTokenSecretManager: Setting block keys
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 WARN balancer.Dispatcher: Failed to move blk_1075156654_1438349 with size=134217728 from <ip>:1019:ARCHIVE to <ip>:1019:DISK through <ip>:1019: block move is failed: opReplaceBlock BP-120244285-<ip>-1417023863606:blk_1075156654_1438349 received exception java.io.EOFException: Premature EOF: no length prefix available
<NOW IT STARTS LOOPING ON SAME BLOCK>
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 WARN balancer.Dispatcher: Failed to move blk_1075156654_1438349 with size=134217728 from <ip>:1019:ARCHIVE to <ip>:1019:DISK through <ip>:1019: block move is failed: opReplaceBlock BP-120244285-<ip>-1417023863606:blk_1075156654_1438349 received exception java.io.EOFException: Premature EOF: no length prefix available
...<repeat indefinitely>...
{code}"
The dncp_block_verification log can be compressed,HDFS-4224,datanode,,,,"On some systems, I noticed that when the scanner runs, the dncp_block_verification.log.curr file under the block pool gets quite large (several GBs). Although this is rolled away, we could also configure compression upon it (a codec that may work without natives, would be a good default) and save on I/O and space."
Race condition on yieldCount in FSDirectory.java,HDFS-9054,namenode,,,,"getContentSummaryInt only held read lock, and it called fsd.addYieldCount which may cause race condition:
{code}
  private static ContentSummary getContentSummaryInt(FSDirectory fsd,
      INodesInPath iip) throws IOException {
    fsd.readLock();
    try {
      INode targetNode = iip.getLastINode();
      if (targetNode == null) {
        throw new FileNotFoundException(""File does not exist: "" + iip.getPath());
      }
      else {
        // Make it relinquish locks everytime contentCountLimit entries are
        // processed. 0 means disabled. I.e. blocking for the entire duration.
        ContentSummaryComputationContext cscc =
            new ContentSummaryComputationContext(fsd, fsd.getFSNamesystem(),
                fsd.getContentCountLimit(), fsd.getContentSleepMicroSec());
        ContentSummary cs = targetNode.computeAndConvertContentSummary(cscc);
        fsd.addYieldCount(cscc.getYieldCount());
        return cs;
      }
    } finally {
      fsd.readUnlock();
    }
  }
{code}"
Small improvement for DatanodeManager#sortLocatedBlocks,HDFS-9032,,,,,"-This is a minor improvement: In sortLocatedBlocks, (in most cases) if locations of located block don't contain decommissioned/stale datanode, no need to call Arrays.sort. Also we can make the comparator as the class variable.-"
File is not closed in OfflineImageViewerPB#run(),HDFS-6290,tools,,,,"{code}
      } else if (processor.equals(""XML"")) {
        new PBImageXmlWriter(conf, out).visit(new RandomAccessFile(inputFile,
            ""r""));
{code}
The RandomAccessFile instance should be closed before the method returns."
Remove unnecessary log from method FSNamesystem.getCorruptFiles,HDFS-8861,namenode,,,,"The log in FSNamesystem.getCorruptFiles will print out too many messages mixed with other log entries, which makes whole log quite verbose, hard to understood and analyzed, especially in those cases where SuperuserPrivilege check and Operation check are not satisfied in frequent calls of listCorruptFileBlocks."
TestDatanodeBlockScanner#testBlockCorruptionRecoveryPolicy2 times out   ,HDFS-3660,,,,,Saw this on a recent jenkins run.
rename favoriteAndExcludedNodes to excludedNodes for clarity,HDFS-7973,,,,,"two reasons:
1. name {{favoriteAndExcludedNodes}} is easily confusing.
2. {{DatanodeStorageInfo[] chooseTarget(..)}} should like {{private Node chooseTarget(..)}} use name {{oldExcludedNodes}} to backup {{excludedNodes}}. We should keep the consistency."
favoredNodes should accept ip addr,HDFS-8700,,,,,"NameNode accepts two forms of FavoredNodes, {{ip:port}} and {{host:port}}

DFSClient#create(..) only uses {{host:port}}, if favoredNodes is created by 
{code}
new InetSocketAddress(ip, port)
{code}
DFSClient will attempt a reverse lookup locally to get {{host:port}}, instead of sending {{ip:port}} directly to NameNode.

It's not a problem in production. But it's a problem in MiniDFSCluster.
MiniDFSCluster use fake hostname ""host1.foo.com"" to start DataNodes.
DFSClient doesn't use StaticMapping. So if DFSClient do reverse lookup, ""127.0.0.1:50470"" becomes ""localhost:50470""."
Links resolving either from active/standby should be same (example clicking on datanodes from Standby),HDFS-5319,,,,,"click live nodes from standby namenode will throw exception ""Operation category READ is not supported in state standby"""
Some hdfs admin operations from client should have audit logs ,HDFS-8504,hdfs-client,,,,"Below  ""hdfs  dfsadmin xxx"" commands should have audit logs printed, because of those operations are helpful for administrator to check what  happened to hdfs service.
*hdfs dfsadmin commands*
{noformat}
hdfs dfsadmin -safemode enter
hdfs dfsadmin -safemode leave
hdfs dfsadmin -rollEdits
hdfs dfsadmin -refreshNodes
hdfs dfsadmin -refreshServiceAcl
hdfs dfsadmin -refreshUserToGroupsMappings
hdfs dfsadmin -refreshSuperUserGroupsConfiguration
hdfs dfsadmin -refreshCallQueue
hdfs dfsadmin -shutdownDatanode
{noformat}"
BlockPoolSliceScanner#getNewBlockScanTime does not handle numbers > 31 bits properly,HDFS-3488,,,,,"This code does not handle the case where period > 2**31 properly:

{code}
    long period = Math.min(scanPeriod, 
                           Math.max(blockMap.size(),1) * 600 * 1000L);
    int periodInt = Math.abs((int)period);
    return System.currentTimeMillis() - scanPeriod + 
        DFSUtil.getRandom().nextInt(periodInt);
{code}

So, for example, if period = 0x100000000, we'll map that to 0, and so forth."
Fix some issue in DFSInputstream,HDFS-4273,,,,,"Following issues in DFSInputStream are addressed in this jira:
1. read may not retry enough in some cases cause early failure
Assume the following call logic
{noformat} 
readWithStrategy()
  -> blockSeekTo()
  -> readBuffer()
     -> reader.doRead()
     -> seekToNewSource() add currentNode to deadnode, wish to get a different datanode
        -> blockSeekTo()
           -> chooseDataNode()
              -> block missing, clear deadNodes and pick the currentNode again
        seekToNewSource() return false
     readBuffer() re-throw the exception quit loop
readWithStrategy() got the exception,  and may fail the read call before tried MaxBlockAcquireFailures.
{noformat} 

2. In multi-threaded scenario(like hbase), DFSInputStream.failures has race condition, it is cleared to 0 when it is still used by other thread. So it is possible that  some read thread may never quit. Change failures to local variable solve this issue.

3. If local datanode is added to deadNodes, it will not be removed from deadNodes if DN is back alive. We need a way to remove local datanode from deadNodes when the local datanode is become live."
Add additional fields to the JMX output on NameNode,HDFS-2203,namenode,,,,"When accessing the JMX data via http (http://namenode:50070/jmx) there are a couple of useful fields missing from this bean:

""name"" : ""Hadoop:service=NameNode,name=NameNodeInfo"",
""modelerType"" : ""org.apache.hadoop.hdfs.server.namenode.FSNamesystem"",

Please add the number of blocks and the configured capacity. "
Add the current time to all the pages in the user interface,HDFS-274,,,,,"Adding current time to all of the pages, so that current time of the machine serving the page is displayed in the UI

As discussed in the hadoop-dev mailing list by Arkady Borkovsky :

""it would be so nice to add the CURRENT TIME to all the pages.
For naive users like myself, understanding universal time is very difficult.  So knowing what the cluster thinks about current time makes it so much easier to understand when a job has actually started or ended.... ""

"
Refactor the checking the minimum replication logic in BlockManager,HDFS-7905,namenode,,,,"This is some refactoring separated from the HDFS-7285 branch. In the current BlockManager, the ""checking the minimum replication"" code is repeated in several places. It will be better to wrap it into a single function. This also make the further extension (for Erasure Coded blocks) easier and cleaner."
Remove obsolete -ns options in in DFSHAAdmin.java,HDFS-7808,,,,,"After HDFS-7324 fix following piece of code become unused. It should be removed.
{code}
    int i = 0;
    String cmd = argv[i++];

    if (""-ns"".equals(cmd)) {
      if (i == argv.length) {
        errOut.println(""Missing nameservice ID"");
        printUsage(errOut);
        return -1;
      }
      nameserviceId = argv[i++];
      if (i >= argv.length) {
        errOut.println(""Missing command"");
        printUsage(errOut);
        return -1;
      }
      argv = Arrays.copyOfRange(argv, i, argv.length);
    }
{code}"
FSImage should specify which dirs are missing when refusing to come up,HDFS-979,namenode,,,,"When {{FSImage}} can't come up as either it has no data or edit dirs, it tells me this
{code}
java.io.IOException: All specified directories are not accessible or do not exist.
{code}
What it doesn't do is say which of the two attributes are missing. This would be beneficial to anyone trying to track down the problem. Also, I don't think the message is correct. It's bailing out because dataDirs.size() == 0 || editsDirs.size() == 0 , because a list is empty -not because the dirs aren't there, as there hasn't been any validation yet.

More useful would be
# Explicit mention of which attributes are null
# Declare that this is because they are not in the config"
DFSAdmin fetchImage command should initialize security credentials,HDFS-3349,hdfs-client,,,,The `hdfs dfsadmin -fetchImage' command should fetch the fsimage using the appropriate credentials if security is enabled.
TestBalancerWithSaslDataTransfer fails in trunk,HDFS-6946,,,,,"From build #1849 :
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer.testBalancer0Integrity

Error Message:
Cluster failed to reached expected values of totalSpace (current: 750, expected: 750), or usedSpace (current: 140, expected: 150), in more than 40000 msec.

Stack Trace:
java.util.concurrent.TimeoutException: Cluster failed to reached expected values of totalSpace (current: 750, expected: 750), or usedSpace (current: 140, expected: 150), in more than 40000 msec.
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.waitForHeartBeat(TestBalancer.java:253)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.runBalancer(TestBalancer.java:578)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:551)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:437)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.oneNodeTest(TestBalancer.java:645)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancer0Internal(TestBalancer.java:759)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer.testBalancer0Integrity(TestBalancerWithSaslDataTransfer.java:34)
{code}"
Build behind a proxy failing to fetch commons-daemon,HDFS-2381,build,,,,"The bit of Ant code in the HDFS project to D/L the commons-daemon binary is failing as it cannot reach archive.apache.org, even though Ant and Maven have their proxies setup. I will allege that M2 isn't passing proxy information down properly. "
"FSVolumeList#initializeReplicaMaps(..) not doing anything, it can be removed",HDFS-5903,datanode,,,,"{code}  void initializeReplicaMaps(ReplicaMap globalReplicaMap) throws IOException {
    for (FsVolumeImpl v : volumes) {
      v.getVolumeMap(globalReplicaMap);
    }
  }{code}

This method has been called at the time of initialization even before the blockpools are added. So its useless to call this method.

Anyway replica map will be updated for each of the blockpool during {{addBlockPool(..)}} by calling {{FSVolumesList#getAllVolumesMap(..)}}"
open and getFileInfo APIs treat paths inconsistently wrt protocol,HDFS-7895,,,,,"When open() is called with regular HDFS path, hdfs://blah/blah/blah, it appears to work.
However, getFileInfo doesn't
{noformat}
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.InvalidPathException): Invalid path name Invalid file name: hdfs://localhost:9000/apps/hive/warehouse/tpch_2.db/lineitem_orc/000001_0
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4128)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:838)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

        at org.apache.hadoop.ipc.Client.call(Client.java:1468)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1988)
{noformat}

1) this seems inconsistent.
2) not clear why the validation should reject what looks like a good HDFS path. At least, client code should clean this stuff up on the way.

[~prasanth_j] has the details, I just filed a bug so I could mention how buggy HDFS is to [~jingzhao] :)
"
TestHttpFSServer fails occasionally in trunk,HDFS-6177,,,,,"From https://builds.apache.org/job/Hadoop-hdfs-trunk/1716/consoleFull :
{code}
Running org.apache.hadoop.fs.http.server.TestHttpFSServer
Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.424 sec <<< FAILURE! - in org.apache.hadoop.fs.http.server.TestHttpFSServer
testDelegationTokenOperations(org.apache.hadoop.fs.http.server.TestHttpFSServer)  Time elapsed: 0.559 sec  <<< FAILURE!
java.lang.AssertionError: expected:<401> but was:<403>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.fs.http.server.TestHttpFSServer.testDelegationTokenOperations(TestHttpFSServer.java:352)
{code}"
TestCrcCorruption#testCorruptionDuringWrt sometimes fails in trunk,HDFS-6501,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1767/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt

Error Message:
test timed out after 50000 milliseconds

Stack Trace:
java.lang.Exception: test timed out after 50000 milliseconds
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:2024)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:2008)
        at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2107)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:98)
        at org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt(TestCrcCorruption.java:133)
{code}"
TestNamenodeCapacityReport fails intermittently,HDFS-6726,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1812/testReport/junit/org.apache.hadoop.hdfs.server.namenode/TestNamenodeCapacityReport/testXceiverCount/ :
{code}
java.io.IOException: Unable to close file because the last block does not have enough number of replicas.
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2141)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2109)
	at org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport.testXceiverCount(TestNamenodeCapacityReport.java:281)
{code}
There were multiple occurrences of 'Broken pipe', 'Connection reset by peer' and 'Premature EOF from inputStream' exceptions in test output"
TestDataNodeMetrics fails in trunk,HDFS-7220,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1896/testReport/junit/org.apache.hadoop.hdfs.server.datanode/TestDataNodeMetrics/testSendDataPacketMetrics/ :
{code}
java.lang.NoClassDefFoundError: org/apache/hadoop/util/IntrusiveCollection$IntrusiveIterator
	at org.apache.hadoop.util.IntrusiveCollection.iterator(IntrusiveCollection.java:213)
	at org.apache.hadoop.util.IntrusiveCollection.clear(IntrusiveCollection.java:368)
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.clearPendingCachingCommands(DatanodeManager.java:1590)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1262)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close(FSNamesystem.java:1590)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stopCommonServices(NameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:823)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1717)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1696)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics.testSendDataPacketMetrics(TestDataNodeMetrics.java:94)
{code}"
TestCacheDirectives#testExceedsCapacity sometimes fails in trunk,HDFS-7571,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1985/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity

Error Message:
Pending cached list of 127.0.0.1:47332 is not empty, [{blockId=1073741841, replication=1, mark=true}]

Stack Trace:
java.lang.AssertionError: Pending cached list of 127.0.0.1:47332 is not empty, [{blockId=1073741841, replication=1, mark=true}]
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1420)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1443)
{code}"
TestDFSUpgradeWithHA sometimes fails in trunk,HDFS-7289,,,,,"From trunk build #1912:
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testFinalizeFromSecondNameNodeWithJournalNodes

Error Message:
java.lang.RuntimeException: java.net.SocketTimeoutException: Read timed out

Stack Trace:
java.io.IOException: java.lang.RuntimeException: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:698)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:641)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1218)
        at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.doGetUrl(TransferFsImage.java:410)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:395)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadImageToStorage(TransferFsImage.java:114)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.doRun(BootstrapStandby.java:213)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.access$000(BootstrapStandby.java:69)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1.run(BootstrapStandby.java:107)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1.run(BootstrapStandby.java:103)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:414)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.run(BootstrapStandby.java:103)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.run(BootstrapStandby.java:315)
        at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testFinalizeFromSecondNameNodeWithJournalNodes(TestDFSUpgradeWithHA.java:493)
{code}"
TestLeaseRecovery2 sometimes fails in trunk,HDFS-7311,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1917/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecovery

Error Message:
Call From asf909.gq1.ygridcore.net/67.195.81.153 to localhost:55061 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

Stack Trace:
java.net.ConnectException: Call From asf909.gq1.ygridcore.net/67.195.81.153 to localhost:55061 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
        at org.apache.hadoop.ipc.Client.call(Client.java:1438)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy19.create(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:295)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
        at com.sun.proxy.$Proxy20.create(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1694)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1654)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1579)
        at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397)
        at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)
        at org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecovery(TestLeaseRecovery2.java:276)


FAILED:  org.apache.hadoop.hdfs.TestLeaseRecovery2.org.apache.hadoop.hdfs.TestLeaseRecovery2

Error Message:
Test resulted in an unexpected exit

Stack Trace:
java.lang.AssertionError: Test resulted in an unexpected exit
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1709)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1696)
        at org.apache.hadoop.hdfs.TestLeaseRecovery2.tearDown(TestLeaseRecovery2.java:105)
{code}"
TestEncryptionZonesWithKMS fails against Java 8,HDFS-7422,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/12/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testReadWriteUsingWebHdfs

Error Message:
Stream closed.

Stack Trace:
java.io.IOException: Stream closed.
        at sun.reflect.GeneratedConstructorAccessor58.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:385)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$600(WebHdfsFileSystem.java:91)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.shouldRetry(WebHdfsFileSystem.java:656)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:622)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1683)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener.connect(WebHdfsFileSystem.java:1204)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream.<init>(WebHdfsFileSystem.java:1261)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.open(WebHdfsFileSystem.java:1175)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
        at org.apache.hadoop.hdfs.DFSTestUtil.verifyFilesEqual(DFSTestUtil.java:1399)
        at org.apache.hadoop.hdfs.TestEncryptionZones.testReadWriteUsingWebHdfs(TestEncryptionZones.java:634)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: org.apache.hadoop.ipc.RemoteException: Stream closed.
        at org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:165)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:353)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:91)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:608)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1683)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener.connect(WebHdfsFileSystem.java:1204)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream.<init>(WebHdfsFileSystem.java:1261)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.open(WebHdfsFileSystem.java:1175)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
        at org.apache.hadoop.hdfs.DFSTestUtil.verifyFilesEqual(DFSTestUtil.java:1399)
        at org.apache.hadoop.hdfs.TestEncryptionZones.testReadWriteUsingWebHdfs(TestEncryptionZones.java:634)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}"
"If the NameNode has already been formatted, but a QuroumJournal has not, auto-format it on startup",HDFS-4173,journal-node,namenode,,,"If we have multiple edit log directories, and some of them are formatted, but others are not, we format the unformatted ones.  However, when we implemented QuorumJournalManager, we did not extend this behavior to it.  It makes sense to do this.

One use case is if you want to add a QuorumJournalManager URI ({{journal://}}) to an existing {{NameNode}}, without reformatting everything.  There is currently no easy way to do this, since {{namenode \-format}} will nuke everything, and there's no other way to format the {{JournalNodes}}."
Unable to change JAVA_HOME directory in hadoop-setup-conf.sh script.,HDFS-4063,scripts,tools,,,"The JAVA_HOME directory remains unchanged no matter what you enter when you run hadoop-setup-conf.sh to generate hadoop configurations. Please see below example:

*********************************************************
[root@hadoop-slave ~]# /sbin/hadoop-setup-conf.sh
Setup Hadoop Configuration

Where would you like to put config directory? (/etc/hadoop)
Where would you like to put log directory? (/var/log/hadoop)
Where would you like to put pid directory? (/var/run/hadoop)
What is the host of the namenode? (hadoop-slave)
Where would you like to put namenode data directory? (/var/lib/hadoop/hdfs/namenode)
Where would you like to put datanode data directory? (/var/lib/hadoop/hdfs/datanode)
What is the host of the jobtracker? (hadoop-slave)
Where would you like to put jobtracker/tasktracker data directory? (/var/lib/hadoop/mapred)
Where is JAVA_HOME directory? (/usr/java/default) *+/usr/lib/jvm/jre+*
Would you like to create directories/copy conf files to localhost? (Y/n)

Review your choices:

Config directory            : /etc/hadoop
Log directory               : /var/log/hadoop
PID directory               : /var/run/hadoop
Namenode host               : hadoop-slave
Namenode directory          : /var/lib/hadoop/hdfs/namenode
Datanode directory          : /var/lib/hadoop/hdfs/datanode
Jobtracker host             : hadoop-slave
Mapreduce directory         : /var/lib/hadoop/mapred
Task scheduler              : org.apache.hadoop.mapred.JobQueueTaskScheduler
JAVA_HOME directory         : *+/usr/java/default+*
Create dirs/copy conf files : y

Proceed with generate configuration? (y/N) n
User aborted setup, exiting...
*********************************************************

Resolution:
Amend line 509 in file /sbin/hadoop-setup-conf.sh

from:

JAVA_HOME=${USER_USER_JAVA_HOME:-$JAVA_HOME}

to:

JAVA_HOME=${USER_JAVA_HOME:-$JAVA_HOME}

will resolve this issue."
Support reading and writing sequencefile in libhdfs ,HDFS-924,,,,,"Some use case may need read and write sequencefile through libhdfs. 

We should provide the reading and writing api for sequencefile in libhdfs."
Include path in the XML output of oiv,HDFS-7428,tools,,,,"When generating the XML output using {noformat}hadoop oiv -p XML{noformat} the path of a file is not printed, just the file name. While the complete path can be derived by parsing INodeDirectorySection and INodeSection and their children, it would be convenient to have the ""absolute path"" present directly like {noformat}INodeSection->inode->path{noformat}"
Setting up of cluster using ssh - Scripts that help in minimising the cluster setup efforts,HDFS-1895,scripts,,,,"Sometimes when we have a large number of clusters we may have to specify the password of the different machines that we are using as slaves (datanodes).
 
If the cluster is very huge we may have to repeat this everytime.
So we would  to suggest a way to avoid this
 
1. Generate a SSH key from the name node machine
2. Read the entries from the conf/slaves file, for every entry add the key generated in step 1 to a file of slave machine.
3. Repeat the same for master file also.
 
when you execute step 1 it will prompt for the password.  This is only for the first time.
 
After that whenever you need to start the cluster then password need not be specified.
 
This scenario is valid when we are sure of the cluster that we will be maintaining and we are aware of the credentials of the machine.
 
This will help the cluster administrator.
 

"
Provide a 64bit Native library,HDFS-7038,build,,,,"As Hadoop will be running on nodes with more then 4GB memory, I do not understand why the build of libraries is done on 32 bit systems.

It will be great to have 64bit provided natively.

Regards
Gurmukh
An Hadoop Lover."
[HDFS-RAID] ExtFSInputStream#read wrapper does not preserve semantics,HDFS-2482,,,,,"The ExtFSInputStream#read wrapper has signed byte issues. No need to use a local byte buffer either, IMO."
[HDFS-RAID] DistributedRaidFileSystem cannot handle token delegation,HDFS-2483,,,,,"DistributedRaidFileSystem cannot handle token delegation, so it is impossible to specify it as fs.hdfs.impl in a secure configuration."
Atomicity of multi file operations,HDFS-6821,,,,,"Looking how HDFS updates the log files in case of chmod 鈥搑 or chown 鈥搑 operations. In these operations, HDFS name node seems to update each file separately; consequently the strace of the operation looks as follows.

append(edits)
fsync(edits)

append(edits)
fsync(edits)
-----------------------
append(edits)
fsync(edits)

append(edits)
fsync(edits)

If a crash happens in the middle of this operation (e.g. at the dashed line in the trace), the system will end up with part of the files updates with the new owner or permissions and part still with the old owner.

Isn鈥檛 it better to log the whole operations (chown -r) as one entry in the edit file?
"
dfs.journalnode.edits.dir should accept URI,HDFS-6091,journal-node,,,,"Using a URI in dfs.journalnode.edits.dir (such as file:///foo)  throws a ""Journal dir 'file:/foo' should be an absolute path'. "
Misc cleanup/logging improvements for branch-20-append,HDFS-1248,datanode,namenode,test,,"Last remaining bits of my append branch that didn't fit elsewhere in JIRA (just misc cleanup)
 - Slight cleanup to recoverFile() function in TFA4
 - Improve error messages on OP_READ_BLOCK
 - Some comment cleanup in FSNamesystem
 - Remove toInodeUnderConstruction (not used)
 - Add some checks for null blocks to avoid NPE
 - Only log ""inconsistent size"" warnings at WARN level for non-under-construction blocks.
 - Redundant addStoredBlock calls are also not worthy of WARN level
 - Add some extra information to a warning in ReplicationTargetChooser

This may need HDFS-1057 to be committed first to apply."
"Fedora jdk install creates circular symlinks, causes test-c++-libhdfs not to build",HDFS-901,build,,,,"installed hadoop; needed javac; installed jdk
export JAVA_HOME=/usr/java/jdk1.6.0_17
ant clean
ant
ant -Dcompile.c++=true -Dlibhdfs=true test-c++-libhdfs
-------------------------------------------------------------------------------
[exec] libtool: link: gcc -g -O2 -DOS_LINUX -DDSO_DLFCN -DCPU=\""i386\"" -m32 -I/usr/java/jdk1.6.0_17/include -I/usr/java/jdk1.6.0_17/include/linux -Wall -Wstrict-prototypes -m32 /usr/java/jdk1.6.0_17/jre1.6.0_17/lib/i386/server -Wl,-x -o hdfs_test hdfs_test.o  -L/usr/java/jdk1.6.0_17/jre/lib/i386/server /home/cook/Desktop/hadoop-0.20.1/build/c++/Linux-i386-32/lib/libhdfs.so -ljvm -ldl -lpthread -Wl,-rpath -Wl,/home/cook/Desktop/hadoop-0.20.1/build/c++/Linux-i386-32/lib -Wl,-rpath -Wl,/home/cook/Desktop/hadoop-0.20.1/build/c++/Linux-i386-32/lib
     [exec] /usr/java/jdk1.6.0_17/jre1.6.0_17/lib/i386/server: file not recognized: Is a directory
     [exec] collect2: ld returned 1 exit status
     [exec] make: *** [hdfs_test] Error 1

BUILD FAILED
note: insertion of /usr/java/jdk1.6.0_17/jre1.6.0_17/lib/i386/server is spurious
-------------------------------------------------------------------------------
tracked bug down as follows:
for some reason, install of jdk creates a symlink from within jdk directory to existing jre impl jre1.6.0_17 and a symlink from within jre directory to jdk.
I suspected the recursion likely messed up a ""find"" somewhere; deleted symlink in jdk then
ant clean
ant
ant -Dcompile.c++=true -Dlibhdfs=true test-c++-libhdfs
BUILD and TEST ran successfully then
just to be sure I put the symlink back; reran above; same failure"
supergroup permission ,HDFS-6761,,,,,"<hdfs default directory information>
Permission   Owner     Group           Size     Name
drwxr-wr-x      hdfs     supergroup      0        /user

I created 'testuser' account and added 'testuser' to the supergroup.
When i use 'testuser' account, I try to 'hadoop fs mkdir /user/data' command.

I thought when i run 'hadoop fs mkdir /user/data' command, this result would be shown 'permission denied'

but I can make 'data' directory.

Is this correct?


"
Work out the memory consumption of NN artifacts on a compressed pointer JVM,HDFS-559,namenode,,,,"Following up HADOOP-1687, it would be nice to know the size of datatypes in under the java16u14 JVM, which offers compressed pointers.

"
Use custom MAX_SIZE_TO_MOVE value  Balancer,HDFS-369,,,,,"Balancer will load ""fs.balancer.max.move.size"" for a custom value of MAX_SIZE_TO_MOVE."
Metrics on Secondary namenode's activity,HDFS-271,,,,,"To monitor secondary namenode's activiely, we currently rely on 'delete' metrics when it replays the edits.
(HADOOP-1495) 

Requesting for new metrics that shows secondary namenode's activity.
Maybe the size of the fsimage/edits being read and written."
Secondary Namenode: Limit number of retries when fsimage/edits transfer fails,HDFS-280,,,,,"When hitting HADOOP-3980, secondary namenode kept on pulling gigs of fsimage/edits every 10 minutes which slowed down the namenode significantly.    When namenode is down, I'd like the secondary namenode to keep on retrying to connect.  However, when pull/push of large files keep on failing, I'd like a upper limit on the number of retries.  Either shutdown or  sleep for _fs.checkpoint.period_ seconds.
"
Uncaught Exception in DataTransfer.run,HDFS-79,,,,,"Minor, but it would be nice if this exception is caught and logged.

I see in .out file of datanode, 

{noformat}
Exception in thread ""org.apache.hadoop.dfs.DataNode$DataTransfer@9d2805"" java.nio.channels.ClosedSelectorException
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:66)
        at sun.nio.ch.SelectorImpl.selectNow(SelectorImpl.java:88)
        at sun.nio.ch.Util.releaseTemporarySelector(Util.java:135)
        at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)
        at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:2604)
        at java.lang.Thread.run(Thread.java:619)
{noformat}"
Missing null check in FSImageSerialization#writePermissionStatus(),HDFS-6415,namenode,,,,"{code}
    PermissionStatus.write(out, inode.getUserName(), inode.getGroupName(), p);
{code}
getUserName() / getGroupName() may return null.
null check should be added for these two calls."
TestQuorumJournalManager#testChangeWritersLogsOutOfSync2 occasionally fails,HDFS-6083,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1695/testReport/junit/org.apache.hadoop.hdfs.qjournal.client/TestQuorumJournalManager/testChangeWritersLogsOutOfSync2/ :
{code}
Leaked thread: ""IPC Client (26533782) connection to /127.0.0.1:57898 from jenkins"" Id=590 RUNNABLE
 at java.lang.System.arraycopy(Native Method)
 at java.lang.ThreadGroup.remove(ThreadGroup.java:885)
 at java.lang.Thread.exit(Thread.java:672)
{code}
The following check should give more time for the threads to shutdown:
{code}
    // Should not leak clients between tests -- this can cause flaky tests.
    // (See HDFS-4643)
    GenericTestUtils.assertNoThreadsMatching("".*IPC Client.*"");
{code}"
Style Hadoop HDFS web ui's with Twitter's bootstrap.,HDFS-4670,,,,,A users' first experience of Apache Hadoop is often looking at the web ui.  This should give the user confidence that the project is usable and relatively current.
Create an option to specify a file path for OfflineImageViewer,HDFS-5975,tools,,,,"The output of OfflineImageViewer becomes quite large if an input fsimage is large. I propose '-filePath' option to make the output smaller.

The below command will output the {{ls -R}} of {{/user/root}}.
{code}
hdfs oiv -i input -o output -p Ls -filePath /user/root
{code}"
Interrupting the namenode thread triggers System.exit(),HDFS-8,,,,,"My service setup/teardown tests are managing to trigger system exits in the namenode, which seems overkill.

1. Interrupting the thread that is starting the namesystem up raises a java.nio.channels.ClosedByInterruptException.
2. This is caught in FSImage.rollFSImage, and handed off to processIOError
3. This triggers a call to Runtime.getRuntime().exit(-1); ""All storage directories are inaccessible."".

Stack trace to follow. Exiting the JVM is somewhat overkill; if someone has interrupted the thread is is (presumably) because they want to stop the namenode, which may not imply they want to kill the JVM at the same time. Certainly JUnit does not expect it. 

Some possibilities
 -ClosedByInterruptException get handled differently as some form of shutdown request
 -Calls to system exit are factored out into something that can have its behaviour changed by policy options to throw a RuntimeException instead. 
Hosting a Namenode in a security manager that blocks off System.exit() is the simplest workaround; this is fairly simple, but it means that what would be a straight exit does now get turned into an exception, so callers may be surprised by what happens."
Clean up the output of NameDistribution processor,HDFS-5867,tools,,,,"The output of 'hdfs oiv -i INPUT -o OUTPUT -p NameDistribution' is as follows:
{code}
Total unique file names 86
0 names are used by 0 files between 100000-13 times. Heap savings ~0 bytes.
0 names are used by 0 files between 10000-99999 times. Heap savings ~0 bytes.
0 names are used by 0 files between 1000-9999 times. Heap savings ~0 bytes.
0 names are used by 0 files between 100-999 times. Heap savings ~0 bytes.
1 names are used by 13 files between 10-99 times. Heap savings ~372 bytes.
4 names are used by 34 files between 5-9 times. Heap savings ~942 bytes.
2 names are used by 8 files 4 times. Heap savings ~192 bytes.
0 names are used by 0 files 3 times. Heap savings ~0 bytes.
7 names are used by 14 files 2 times. Heap savings ~222 bytes.

Total saved heap ~1728bytes.
{code}
'between 100000-13 times' should be 'over 99999 times' , or the line starting with '0 names' should not output."
port fuse-dfs existing autoconf to hadoop project's autoconf infrastructure,HDFS-431,fuse-dfs,,,,"Although fuse-dfs has its own autoconf macros and such, better to use one set of macros and in some places the macros could be improved.
"
namenode fails to run on ppc,HDFS-160,,,,,"Hadoop starts, but eats 100% CPU. Data- and Secondarynamenodes can not connect. No jobs were run, just trying to start the daemon. using bin/start-dfs.sh.

Using the same simple configuration on an x86-arch - also using Fedora 9 and gcj-1.5.0.0 - works perfectly."
LIBHDFS questions and performance suggestions,HDFS-5541,hdfs-client,,,,"Since libhdfs is a ""client"" interface"",  and esspecially because it is a ""C"" interface , it should be assumed that the code will be used accross many different platforms, and many different compilers.

1) The code should be cross platform ( no Linux extras )
2) The code should compile on standard c89 compilers, the
>>>  {least common denominator rule applies here} !! <<  

C  code with  ""c""   extension should follow the rules of the c standard  

All variables must be declared at the begining of scope , and no (//) comments allowed 

>> I just spent a week white-washing the code back to nornal C standards so that it could compile and build accross a wide range of platforms << 

Now on-to  performance questions 

1) If threads are not used why do a thread attach ( when threads are not used all the thread attach nonesense is a waste of time and a performance killer ) 

2) The JVM  init  code should not be imbedded within the context of every function call   .  The  JVM init code should be in a stand-alone  LIBINIT function that is only invoked once.   The JVM * and the JNI * should be global variables for use when no threads are utilized.  

3) When threads are utilized the attach fucntion can use the GLOBAL  jvm * created by the LIBINIT  { WHICH IS INVOKED ONLY ONCE } and thus safely outside the scope of any LOOP that is using the functions 

4) Hash Table and Locking  Why ?????
When threads are used the hash table locking is going to hurt perfromance .  Why not use thread local storage for the hash table,that way no locking is required either with or without threads.   
 
5) FINALLY Windows  Compatibility 

Do not use posix features if they cannot easilly be replaced on other platforms   !!"
Namenode safemode is on and is misleading on webpage,HDFS-5036,namenode,,,,"even though namenode is not in safemode , namenode webUI shows safemode is on.
when namenode safemode is get from command line it shows safe mode is off but on the it shows it is on, it is confusing to the users looking at the webUI."
"Retain old edits log, don't retain all minimum required logs",HDFS-4939,ha,namenode,,,"JNStorage.java

{code}

  private static void purgeMatching(File dir, List<Pattern> patterns,
      long minTxIdToKeep) throws IOException {

    for (File f : FileUtil.listFiles(dir)) {
      if (!f.isFile()) continue;

      for (Pattern p : patterns) {
        Matcher matcher = p.matcher(f.getName());
        if (matcher.matches()) {
          // This parsing will always succeed since the group(1) is
          // /\d+/ in the regex itself.
          long txid = Long.valueOf(matcher.group(1));
          if (txid < minTxIdToKeep) {
            LOG.info(""Purging no-longer needed file "" + txid);
            if (!f.delete()) {
              LOG.warn(""Unable to delete no-longer-needed data "" +
                  f);
            }
            break;
          }
        }
      }
    }
  }
{code}

Why break the for loop here? if so, only delete one file for each retain, am I right?
"
Support non-recursive create() in FileSystem API,HDFS-1269,hdfs-client,,,,"HDFS-617 added a non-recursive create() api to HDFS, however the FileSystem API was not changed.  This change is necessary for SequenceFile.Writer to support non-recursive creates."
SecondaryNamenode may report incorrect info host name,HDFS-62,,,,,"I have set up {{dfs.secondary.http.address}} like this:

{code}
<property>
  <name>dfs.secondary.http.address</name>
  <value>secondary.example.com:50090</value>
</property>
{code}

In my setup {{secondary.example.com}} resolves to an IP address (say, 192.168.0.10) which is not the same as the host's name (as returned by {{InetAddress.getLocalHost().getHostAddress()}}, say 192.168.0.1).

In this situation, edit log related transfers fail. From the namenode log:

{code}
2009-04-05 13:32:39,128 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.0.10
2009-04-05 13:32:39,168 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at java.net.Socket.connect(Socket.java:469)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:163)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:394)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:529)
        at sun.net.www.http.HttpClient.<init>(HttpClient.java:233)
        at sun.net.www.http.HttpClient.New(HttpClient.java:306)
        at sun.net.www.http.HttpClient.New(HttpClient.java:323)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:837)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:778)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:703)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1026)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:151)
        ...
{code}

From the secondary namenode log:

{code}
2009-04-05 13:42:39,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint: 
2009-04-05 13:42:39,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.FileNotFoundException: http://nn.example.com:50070/getimage?putimage=1&port=50090&machine=
192.168.0.1&token=-19:1243068779:0:1238929357000:1238929031783
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1288)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:151)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.putFSImage(SecondaryNameNode.java:294)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:333)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:239)
        at java.lang.Thread.run(Thread.java:619)
{code}"
Can not change replication factor of file during moving or deleting.,HDFS-4711,,,,,"I don't know is it a feature or a bug. 
According to hdfs dfs -help we can use key -D to set specific options for action;
When we copying or uploading file to hdfs we can override replication factor with -D dfs.replication=N. That's works well.
But it doesn't work for moving or removing(to trash) file.
Steps to reproduce:
Uploading file
hdfs dfs -put somefile /tmp/somefile
Copying with changing replication:
hdfs dfs -D dfs.replication=1 -mv /tmp/somefile /tmp/somefile2

hadoop version:
Hadoop 2.0.0-cdh4.1.2
"
port HDFS-1457 to branch-1.1,HDFS-4535,namenode,,,,port HDFS-1457 (configuration option to enable limiting the transfer rate used when sending the image and edits for checkpointing) to branch-1.1
"If there are multiple edit logs with the same fstime, Recovery Mode should load a different one than the normal loading process",HDFS-3947,,,,,"If there are multiple edit logs with the same {{fstime}}, Recovery Mode should load a different one than the normal loading process.  This will protect against many common kinds of corruption which affect only one edit log directory.  For example, most I/O errors would fall into this category.

This improvement is not necessary in branch-2 and later branches because in those branches, we have edit log failover (See HDFS-3049).  However, this is an extremely simple and useful thing we can do to make Recovery Mode more helpful in branch-1."
"Command : ""hadoop fs -ls / "" suggesting local directories/files instead hdfs directories/files",HDFS-4474,build,,,,"hadoop fs -ls / , suggesting local directories (when we press TAB key).
In earlier version, pressing TAB was not suggesting anything but in hadoop-2.0.0 (CDH4) is suggesting local filesystem directories.
 

hadoop@hadoop-VirtualBox:~/CDH4/hadoop-2.0.0-mr1-cdh4.1.3$ hadoop fs -ls /   {Pressing TAB}
bin/        dev/        initrd.img  lost+found/ opt/        run/        srv/        usr/        
boot/       etc/        lib/        media/      proc/       sbin/       sys/        var/        
cdrom/      home/       lib64/      mnt/        root/       selinux/    tmp/        vmlinuz    
"
Provide a way to disable browsing of files from the web UI,HDFS-3801,namenode,,,,"A few times we've had requests from users who wish to disable browsing of the filesystem in the web UI completely, while keeping other servlet functionality enabled (such as fsck, etc.). Right now, the cheap way to do this is by blocking out the DN web port (50075) from access by clients, but that also hampers HFTP transfers.

We should instead provide a toggle config for the JSPs to use and disallow browsing if the toggle's enabled. The config can be true by default, to not change the behavior."
EditLogFileInputStream: be more careful about closing streams when we're done with them.,HDFS-3371,,,,,"EditLogFileInputStream#EditLogFileInputStream should be more careful about closing streams when there is an exception thrown.  Also, EditLogFileInputStream#close should close all of the streams we opened in the constructor, not just one of them (although the file-backed one is probably the most important)."
File mode bits of some scripts in rpm package are incorrect,HDFS-4069,scripts,,,,"These scripts should have execute permission(755). It only happens to rpm package, deb package does not have this problem.

{noformat}-rw-r--r--. 1 root root  2143 Oct  4 22:12 /usr/sbin/slaves.sh
-rw-r--r--. 1 root root  1166 Oct  4 22:12 /usr/sbin/start-all.sh
-rw-r--r--. 1 root root  1065 Oct  4 22:12 /usr/sbin/start-balancer.sh
-rw-r--r--. 1 root root  1745 Oct  4 22:12 /usr/sbin/start-dfs.sh
-rw-r--r--. 1 root root  1145 Oct  4 22:12 /usr/sbin/start-jobhistoryserver.sh
-rw-r--r--. 1 root root  1259 Oct  4 22:12 /usr/sbin/start-mapred.sh
-rw-r--r--. 1 root root  1119 Oct  4 22:12 /usr/sbin/stop-all.sh
-rw-r--r--. 1 root root  1116 Oct  4 22:12 /usr/sbin/stop-balancer.sh
-rw-r--r--. 1 root root  1246 Oct  4 22:12 /usr/sbin/stop-dfs.sh
-rw-r--r--. 1 root root  1131 Oct  4 22:12 /usr/sbin/stop-jobhistoryserver.sh
-rw-r--r--. 1 root root  1168 Oct  4 22:12 /usr/sbin/stop-mapred.sh
-rw-r--r--. 1 root root  4210 Oct  4 22:12 /usr/sbin/update-hadoop-env.sh{noformat} "
BlockMap's corruptNodes count and CorruptReplicas map count is not matching.,HDFS-3162,namenode,,,,"Even after invalidating the block, continuosly below log is coming
 
Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1"
TestDFSUtil#testHANameNodesWithFederation failed because of misuse toString to generate hostname:port,HDFS-3748,federation,ha,test,,"In the test case TestDFSUtil#testHANameNodesWithFederation, it misused InetSocketAddress.toString() to generate hostname:port format data. It cause the following assert failure when run this test case.
assertEquals(NS1_NN1_HOST, map.get(""ns1"").get(""ns1-nn1"").toString());
assertEquals(NS1_NN2_HOST, map.get(""ns1"").get(""ns1-nn2"").toString());
assertEquals(NS2_NN1_HOST, map.get(""ns2"").get(""ns2-nn1"").toString());
assertEquals(NS2_NN2_HOST, map.get(""ns2"").get(""ns2-nn2"").toString());"
Supporting unzip and untar in hadoop shell,HDFS-3253,,,,,As of now hadoop command shell doesnot support unzipping or untaring files in HDFS. But API wise FileUtil already supports this functionality. Work of this jira will be adding untar and unzip functionality to FSShell. 
"webhdfs delete a path that does not exists returns a 200, we should return a 404",HDFS-2429,webhdfs,,,,"Request URI http://NN:50070/webhdfs/some_path_that_does_not_exists?op=DELETE
Request Method: DELETE
Status Line: HTTP/1.1 200 OK
Request Content: {""boolean"":false}"
DatanodeInfo should have a DatanodeID rather than extend it,HDFS-3237,,,,,"DatanodeInfo currently extends DatanodeID, the code would be more clear if it had a DatanodeID member instead, as DatanodeInfo is private within the server side and DatanodeID is passed to clients."
JIRA to redesign client side read path,HDFS-2033,,,,,This is a JIRA that came up based on comments on HADOOP-7316. This is to study the current design of the client side read path and redesign if necessary.
Some stacktraces are now too lengthy and sometimes no good,HDFS-3366,,,,,"This is a high-on-nitpick ticket for the benefit of troubleshooting.

This is partially related to all the PB-changes we've had. And also partially related to Java/JVMs.

Take a case of an AccessControlException, which is pretty common in HDFS permissions layer. We  now get, due to several more calls added at the RPC layer for PB (or maybe something else, if am mistaken):
{code}
Caused by: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=yarn, access=WRITE, inode=""/"":hdfs:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:205)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:186)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:135)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4204)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4175)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:2565)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2529)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:640)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42618)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:448)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1661)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1657)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1204)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1655)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:205)
	at $Proxy10.mkdirs(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
	at $Proxy10.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:430)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:1717)
	... 9 more
{code}

The ""9 more"" is what I was looking for, to identify the caller to debug on/find the exact directory. However it now gets eaten away cause just the mkdir-to-exception trace itself has grown quite a bit. Comparing this to 0.20, we have much fewer calls and that helps us see at least the real caller of mkdirs.

I'm actually not sure what causes Java to print ""... X more"" in these form of exception prints, but if thats controllable am all in favor of increasing its amount for HDFS (using new default java opts?). So that when an exception does occur, we don't get a nearly-unusable stacktrace."
Exclude second Ant JAR from classpath in hdfs builds,HDFS-798,build,,,,"I've no evidence that this is a problem, but I have known it to be in different projects:
{code}
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/Users/slo/Java/Apache/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/slo/.ivy2/cache/ant/ant/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
{code}
Somehow Ivy needs to be set up to skip pulling in an old version of Ant in the build -both paranamer-ant and jsp-2.1 declare a dependency on it. If both tools are only ever run under Ant, the ivy.xml file could exclude it, the build file just has to make sure that Ant's own classpath gets passed down."
"HDFS ignores group of a user when creating a file or a directory, and instead inherits",HDFS-3074,namenode,,,,"When creating a file or making a directory on HDFS, the namesystem calls pass {{null}} for the group name, thereby having the parent directory permissions inherited onto the file.

This is not how the Linux FS works at least.

For instance, if I have today a user 'foo' with default group 'foo', and I have my HDFS home dir created as ""foo:foo"" by the HDFS admin, all files I create under my directory too will have ""foo"" as group unless I chgrp them myself. This makes sense.

Now, if my admin were to change my local accounts' default/primary group to 'bar' (but did not change so on my homedir on HDFS, and I were to continue writing files to my home directory or any subdirectory that has 'foo' as group, all files still get created with group 'foo' - as if the NN has not realized the primary group of the mapped shell account has already changed.

On linux this is the opposite. My login session's current primary group is what determines the default group on my created files and directories, not the parent dir owner.

If the create and mkdirs call passed UGI's group info (UserGroupInformation.getCurrentUser().getGroupNames()[0] should give primary group?) along into their calls instead of a null in the PermissionsStatus object, perhaps this can be avoided.

Or should we leave this as-is, and instead state that if admins wish their default groups of users to change, they'd have to chgrp all the directories themselves?"
Merge NameNode roles into NodeType.,HDFS-2162,ha,namenode,,,"Currently Namenode has {{NamenodeRole}} with roles NAMENODE, BACKUP and CHECKPOINT. {{NodeType}} has node types NAME_NODE and DATA_NODE. Merge NamenodeRole into NodeType."
imeplement DFSClient on top of thriftfs - this may require DFSClient or DN protocol changes,HDFS-238,,,,,"Open up DFS Protocol to allow non-Hadoop DFS clients to implement reads/writes.  Obviously, the NN need not be changed because the thriftfs server will serve up the same metadata - ie it's a bridge to the NN.

This is useful because if we can do this in Java using more open APIs, we could do it in C++ or Python or Perl :)

Doing it in Java first makes sense because we already have the DFSClient - kind of a proof of concept.

"
StatusHttpServer is failing if trying to use hadoop as a standalone jar,HDFS-155,,,,,"I just recently tried to upgrade from hadoop .2 and I am getting an error during the startup of StatusHttpServer.  I did set my hadoop.log.dir, but there are still errros.  Here is my stack below.

java.io.FileNotFoundException: E:\projects\marzen\file:\E:\projects\marzen\web\WEB-INF\lib\hadoop.jar!\webapps\datanode
	at org.mortbay.jetty.servlet.WebApplicationContext.resolveWebApp(WebApplicationContext.java:266)
	at org.mortbay.jetty.servlet.WebApplicationContext.doStart(WebApplicationContext.java:449)
	at org.mortbay.util.Container.start(Container.java:72)
	at org.mortbay.http.HttpServer.doStart(HttpServer.java:753)
	at org.mortbay.util.Container.start(Container.java:72)
	at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:177)
	at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:167)
	at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1069)
	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1003)
	...rest of stack removed...

The root of the issues seems to be in StatusHttpServer.getWebAppsPath().  Its possible that I am using hadoop differently than most people (as a standalone jar in another web app), but I'm not sure how this function could ever work if you were just using hadoop packaged as a standalone jar.  Its trying to get the webapps path using this line: 

URL url = StatusHttpServer.class.getClassLoader().getResource(""webapps"");

Which is returning the following path.  This is a path within the hadoop jar:   file:\E:\projects\marzen\web\WEB-INF\lib\hadoop.jar!\webapps

It then tries to return the canonical path of that wich returns this path:  E:\projects\marzen\file:\E:\projects\marzen\web\WEB-INF\lib\hadoop.jar!\webapps

I don't even think that is a valid path, and the Jetty server certainly doesn't like it.  Is there something I'm doing wrong?  Is it possible nobody else is using hadoop as a standalone jar?  Is this a windows specific bug?  






"
HDFS jira for changes related to HADOOP-7970,HDFS-2783,ha,,,,Create a separate hdfs patch for HDFS changes related to Hadoop-7970 to avoid test failures.
"When all data directory volumes pulled out in DataNode, Its better to shutdown.",HDFS-1863,datanode,,,,"When we pulled out all the data directory volumes in DataNode, 
 it is not shuttingdown. Because of this NameNode is keep selecting this DN also for write.
But datanode is saying 'No available volumes' and throwing exception.

Instead of this, we can shutdown the DataNode."
DataNode.setDataNode() considered dangerous,HDFS-973,datanode,,,,"I don't have any plans to address this, but it seems to me that having the DataNode save a reference to itself in its constructor by way of {{DataNode.setDataNode(this)}} is hazardous. 

# The reference could be used before the constructor has finished, especially when subclasses are involved
# Callers may assume the DN is actually live
# If startup fails, the DN tries to shut down, but the reference hangs around. Dangerous as well as leaking a reference
# The reference gets retained forever
# It's a singleton that will get confused if >1 DN gets instantiated in-VM

The likely way these problems will surface are in race conditions that are more likely the more cores you have on the machine -production rather than development. This is why it is dangerous.

As part of the service lifecycle patch, I could have this reference only set when the service gets started, set it to null when stopped (and the reference==this). But really the singleton should be removed altogether, somehow. There are methods in DataNode, DataStorage, FSDataset and the namenode that do this, and they should somehow get a reference to any in-VM DN in a cleaner way. For example, servlets can have it set as servlet context."
Handling of deprecated dfs.info.bindAddress and dfs.info.port,HDFS-36,,,,,"When checkpointing is triggered in Secondary name node, Secondary name node throws exception while it tries to connect to Namenode's http server in the following two cases:

1) In hadoop-site.xml, if you put only dfs.http.address but not dfs.info.bindAddress and dfs.info.port (Connection Refused Exception)

2) In hadoop-site.xml, if you put only dfs.info.bindAddress and dfs.info.port but not dfs.http.address (SecondaryNameNode.getServerAddress line 148 throws exception since newAddrPort is null)

Temporary Solution: If you put dfs.http.address, dfs.info.bindAddress, and dfs.info.port, then SecondaryNameNode successfully fetches the image and log from Namenode."
One of the DFSClient::create functions ignores parameter,HDFS-497,hdfs-client,,,,"DFSClient::create(String src, boolean overwrite, Progressable progress) ignores progress parameter"
"null pointer exception while accessing secondaryname web interface (servlet dfshealth.jsp should not be served from the
secondary Namenode)",HDFS-131,,,,,"when I go to the secondary namenode HTTP (dfs.secondary.http.address) in
my browser I see something like this:

        HTTP ERROR: 500
        init
        RequestURI=/dfshealth.jsp
        Powered by Jetty://

And in secondary's log I find these lines:

2008-04-02 11:26:25,357 WARN /: /dfshealth.jsp:
java.lang.NullPointerException
        at org.apache.hadoop.dfs.dfshealth_jsp.<init>(dfshealth_jsp.java:21)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:539)
        at java.lang.Class.newInstance0(Class.java:373)
        at java.lang.Class.newInstance(Class.java:326)
        at org.mortbay.jetty.servlet.Holder.newInstance(Holder.java:199)
        at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:326)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:405)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)



Responses from core-user@hadoop.apache.org:
 
------------
""dhruba Borthakur"":
The secondary Namenode uses the HTTP interface to pull the fsimage from
the primary. Similarly, the primary Namenode uses the
dfs.secondary.http.address to pull the checkpointed-fsimage back from
the secondary to the primary. So, the definition of
dfs.secondary.http.address is needed.

However, the servlet dfshealth.jsp should not be served from the
secondary Namenode. This servet should be setup in such a way that only
the primary Namenode invokes this servlet.
--------------
 Konstantin Shvachko:
We do not have any secondary nn web interface as of today.
The http server is used for transferring data between the primary and the secondary.
I don't see we can display anything useful on the secondary web UI except for the
current status, config values, and the last checkpoint date/time.
--------------"
"HTTP ERROR: 500 when using ""Go back to dir listing"" link in NameNode web interface",HDFS-71,,,,,"I'm running Hadoop 0.13.0 on Windows XP.

Steps to reproduce HTTP 500 error:

1. Go to http://localhost:50070/dfshealth.jsp
2. Click ""Browse the filesystem""
3. Click on any file
4. Click ""Go back to dir listing""

If I do this, I get the following error:

HTTP ERROR: 500

java.io.IOException: Cannot open filename \
	at org.apache.hadoop.dfs.NameNode.open(NameNode.java:263)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)

RequestURI=/browseDirectory.jsp"
"fsimage,fstime not replicated when edit file is empty.",HDFS-41,,,,,"Added a second directory to the dfs.name.dir  by 
1) stop dfs
2) restart namenode so that all the edit gets committed to fsimage (edit file becomes empty)
3) stop namenode
4) add second directory to dfs.name.dir in hadoop-site.xml
5) format the new directory
6) restart the dfs

Somehow, edit file was replicated but not the fsimage and fstime.
"
"in FSNamesystem.registerDatanode, dnAddress should be resolved (rarely occured)",HDFS-6,,,,,"In FSNamesystem.java registerDatanode(), if the datanode address cannot be
got from the RPC Server, it will use that from the datanode report:

    String dnAddress = Server.getRemoteAddress();
    if (dnAddress == null) {
      // Mostly called inside an RPC.
      // But if not, use address passed by the data-node.
      dnAddress = nodeReg.getHost();
    }      

The getHost() may return the hostname or address, while the Server.getRemoteAddress() 
will return the IP address, which is the dnAddress should be. Thus I think the it should be

    if (dnAddress == null) {
      // Mostly called inside an RPC.
      // But if not, use address passed by the data-node.
      dnAddress = InetAddress.getByName(nodeReg.getHost()).getHostAddress();
    }      

I know it should not be called in most situation, but I indeed use that, and I suppose the 
dnAddress should be an IP address.

"
fsck -files -blocks -locations is a little slow,HDFS-263,,,,,"fsck on one subdirectory. 

about 50,000 files (50,000 blocks) 

fsck  /user/aaa            3 seconds
fsck /user/aaa -files   30 seconds
fsck /user/aaa -files -blocks -locations 90 seconds. 

It depends on the network, but could it be a little faster?"
combine sequence read and position read,HDFS-2201,,,,,It might be beneficial (especially from maintenance pov) to combine the logic of sequence and position read calls in DFSInputStream.java.
Remove -genclusterid from NameNode startup options,HDFS-1941,namenode,,,,"Currently, namenode -genclusterid is a helper utility to generate unique clusterid. This option is useless once namenode -format automatically generates the clusterid."
Programmatically start hdfs processes with JMX port open ,HDFS-1874,datanode,namenode,,,Federation web console makes JMX calls to each name node in order to collect statistics from the name nodes.  This requires name node processes to have JMX port open for communication.  We propose a programmatic way to start name node processes with JMX enabled.
TestFileContextResolveAfs will fail with default test.build.data property.,HDFS-2008,,,,,
dynamically add/subtract dfs.name.dir directories,HDFS-248,,,,,It would be very beneficial to be able to add and subtract dfs.name.dir entries on the fly.  This would be used when one drive fails such that another location could be used to keep image and edits redundancy.
dfs.data.dir syntax needs revamping: multiple percentages and weights,HDFS-284,,,,,"Currently, all filesystems listed in the dfs.data.dir are treated the same with respected to the space reservation percentages.  This makes sense on homogeneous, dedicated machines, but breaks badly on heterogeneous ones and creates a bit of a support nightmare. 

In a grid with multiple disk sizes, the admin is either leaving space unallocated or is required to slice up the disk.  In addition, if Hadoop isn't the only application running, there may be unexpected collisions. In order to work around this limitation, the administrator must specifically partition up filesystem space such that the reservation 'make sense' for all of the configured file systems.   For example, if someone has 2 small file systems and 2 big ones on a single machine, due to various requirements (such as the OS being mirrored, systems were built from spare parts, server consolidation, whatever).   Reserving 10% might make sense on the small file systems  (say 7G) but 10% may leave a lot more space than desired free on the big ones (say 50G).  

Instead, Hadoop should support a more robust syntax for directory layout.  Ideally, an admin should be able to specify the directory location, the amount of space reserved (in either a percentage or a raw size syntax) for HDFS, as well as a weighting such that some file systems may be preferred over others.  In the example above, the two larger file systems would likely be preferred over the two smaller ones.  Additionally, the reservation on the larger file system might be changed such that it matches the 7G on the smaller file system.

Doing so would allow for much more complex configuration scenarios without having to shuffle a lot of things around at the operating system level."
Make it harder to accidentally close a shared DFSClient,HDFS-925,hdfs-client,,,,"Every so often I get stack traces telling me that DFSClient is closed, usually in {{org.apache.hadoop.hdfs.DFSClient.checkOpen() }} . The root cause of this is usually that one thread has closed a shared fsclient while another thread still has a reference to it. If the other thread then asks for a new client it will get one -and the cache repopulated- but if has one already, then I get to see a stack trace. 

It's effectively a race condition between clients in different threads. "
dfs does not support -rmdir,HDFS-639,,,,,"Given we have a mkdir, we should have a rmdir.  Using rmr is not a reasonable substitute when you only want to delete empty directories."
add fuse-dfs to src/contrib/build.xml test target,HDFS-414,,,,,"since contrib/build.xml test target now specifically includes contrib projects rather than all, fuse-dfs needs to be added.

Note that fuse-dfs' test target is gated on -Dfusedfs=1 and -Dlibhdfs=1, so just running ant test-contrib will not actually trigger it being run.
"
Startup sanity data directory check in main loop.,HDFS-164,,,,,"In the overall scheme of things this is probably a nit, but in the run() method of DataXceiveServer in DataNode.java the method ""data.checkDataDir()"" is called right after the socket accept. data.checkDataDir() is a sanity check that makes sure the data directory is setup properly. It is a good sanity check to do when the server is started, but it seems like a bit of a waste to do it after every socket accept. If something happens that causes the sanity check to fail, you would end up finding out about it during the processing of the request anyway."
"Data node should shutdown when a ""critical"" error is returned by the name node",HDFS-70,,,,,"Currently data node does not distinguish between critical and non critical exceptions.
Any exception is treated as a signal to sleep and then try again. See
org.apache.hadoop.dfs.DataNode.run()
This is happening because RPC always throws the same RemoteException.
In some cases (like UnregisteredDatanodeException, IncorrectVersionException) the data 
node should shutdown rather than retry.
This logic naturally belongs to the 
org.apache.hadoop.dfs.DataNode.offerService()
but can be reasonably implemented (without examining the RemoteException.className 
field) after HADOOP-266 (2) is fixed."
JspHelper should be a singleton,HDFS-266,,,,,"Servlets creating JspHelper instances initialize a set of statics and use the empty instance as a handle to class methods. Instead, JspHelper should either be a singleton with instance fields or its methods should also be static."
Improvements to Hadoop Thrift bindings,HDFS-417,,,,,"I have made the following changes to hadoopfs.thrift:

#  Added namespaces for Python, Perl and C++.

# Renamed parameters and struct members to camelCase versions to keep them consistent (in particular FileStatus{blockReplication,blockSize} vs FileStatus.{block_replication,blocksize}).

# Renamed ThriftHadoopFileSystem to FileSystem. From the perspective of a Perl/Python/C++ user, 1) it is already clear that we're using Thrift, and 2) the fact that we're dealing with Hadoop is already explicit in the namespace.  The usage of generated code is more compact and (in my opinion) clearer:
{quote}
        *Perl*:
        use HadoopFS;

        my $client = HadoopFS::FileSystemClient->new(..);

         _instead of:_

        my $client = HadoopFS::ThriftHadoopFileSystemClient->new(..);

        *Python*:

        from hadoopfs import FileSystem

        client = FileSystem.Client(..)

        _instead of_

        from hadoopfs import ThriftHadoopFileSystem

        client = ThriftHadoopFileSystem.Client(..)

        (See also the attached diff [^scripts_hdfs_py.diff] for the
         new version of 'scripts/hdfs.py').

        *C++*:

        hadoopfs::FileSystemClient client(..);

         _instead of_:

        hadoopfs::ThriftHadoopFileSystemClient client(..);
{quote}

# Renamed ThriftHandle to FileHandle: As in 3, it is clear that we're dealing with a Thrift object, and its purpose (to act as a handle for file operations) is clearer.

# Renamed ThriftIOException to IOException, to keep it simpler, and consistent with MalformedInputException.

# Added explicit version tags to fields of ThriftHandle/FileHandle, Pathname, MalformedInputException and ThriftIOException/IOException, to improve compatibility of existing clients with future versions of the interface which might add new fields to those objects (like stack traces for the exception types, for instance).

Those changes are reflected in the attachment [^hadoopfs_thrift.diff].

Changes in generated Java, Python, Perl and C++ code are also attached in [^gen.diff]. They were generated by a Thrift checkout from trunk
([http://svn.apache.org/repos/asf/incubator/thrift/trunk/]) as of revision
719697, plus the following Perl-related patches:

* [https://issues.apache.org/jira/browse/THRIFT-190]
* [https://issues.apache.org/jira/browse/THRIFT-193]
* [https://issues.apache.org/jira/browse/THRIFT-199]

The Thrift jar file [^libthrift.jar] built from that Thrift checkout is also attached, since it's needed to run the Java Thrift server.

I have also added a new target to src/contrib/thriftfs/build.xml to build the Java bindings needed for org.apache.hadoop.thriftfs.HadoopThriftServer.java (see attachment [^build_xml.diff] and modified HadoopThriftServer.java to make use of the new bindings (see attachment [^HadoopThriftServer_java.diff]).

The jar file [^lib/hadoopthriftapi.jar] is also included, although it can be regenerated from the stuff under 'gen-java' and the new 'compile-gen' Ant target.

The whole changeset is also included as [^all.diff]."
Namenode GUI does not show actual memory usage,HDFS-307,,,,,"In the namenode GUI, the showed memory usage is not the actual memory usage. Instead, it is the memory currently allocated to Java (Runtime.totalMemory()). That's why we see most of the time, the memory usage is 100%. This is a concern for operation, since this is the only page we can monitor the memory usage of a name node. Showing 100% makes the wrong impression that there is a memory leak in name node, and the name node needs to be restarted.

The current value should show would be Runtime.totalMemory() - Runtime.freeMemory()."
Remove .eclipse.templates directory,HDFS-1710,build,,,,The {{.eclipse.templates}} directory can be removed.  It contains only a README.txt file.
Use readlink to get absolute paths in the scripts ,HDFS-1569,scripts,,,,HDFS side of HADOOP-7089.
Manual tool to test sync against a real cluster,HDFS-1246,test,,,,"Contributing a tool I've built that writes data against a real cluster, calling sync as fast as it can, and then kill -9s the writer and verifies the data can be recovered."
HAR files used for RAID parity need to have configurable partfile size,HDFS-1175,contrib/raid,,,,"RAID parity files are merged into HAR archives periodically. This is required to reduce the number of files that the NameNode has to track. The number of files present in a HAR archive depends on the size of HAR part files - higher the size, lower the number of files.
The size of HAR part files is configurable through the setting har.partfile.size, but that is a global setting. This task introduces a new setting specific to raid.har.partfile.size, that is used in-turn to set har.partfile.size
"
NPE in datanode.handshake(),HDFS-165,,,,,It appears possible to raise an NPE in DataNode.handshake() if the startup protocol gets interrupted or fails in some manner
Low Latency distributed reads,HDFS-516,,,,,"I created a method for low latency random reads using NIO on the server side and simulated OS paging with LRU caching and lookahead on the client side.  Some applications could include lucene searching (term->doc and doc->offset mappings are likely to be in local cache, thus much faster than nutch's current FsDirectory impl and binary search through record files (bytes at 1/2, 1/4, 1/8 marks are likely to be cached)"
NullPointerException when reading deleted file,HDFS-50,,,,,"hdfs://AAA:9999/distcp/destdir/Trash/0803050600/data/part-00018
: java.lang.NullPointerException
  at org.apache.hadoop.dfs.DFSClient$DFSInputStream.getBlockAt(DFSClient.java:919)
  at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:992)
  at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1112)
  at java.io.DataInputStream.read(DataInputStream.java:83)
  at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.copy(CopyFiles.java:303)
  at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:364)
  at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:219)
  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
  at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1804)

(Line number for CopyFiles.java is a little off since I'm using my modified version)"
namenode and datanode are not starting,HDFS-14217,datanode,namenode,,,"I am new to hadoop ecosystem. I am setting up a cluster with 4 nodes, one master and 3 slave nodes. I have done the ground work of updating all xml files in hadoop/etc/hadoop folder. I saved the slaves file. I formatted the namenode. I then started the cluster with command, sbin/start-dfs.sh. I get to see the lines:

'starting namenode on localhost.... starting datanode on slave1...starting datanode on slave2...starting datanode on slave3... starting secondary namenode..... But when I run jps command in the terminal of masternode, I see only Jps and Secondary Namenode. When I run jps command on the slave nodes, I see only Jps, no datanode running on the slaves. What should I write in my hdfs-site.xml? Currently I set the path for namenode and datanode like... <value>file:/home/user/hadoop_store/hdfs/namenode<\value> and similarly for datanode as well. In the log file for namenode, I see the line saying ""Inconsistent state: storage directory doesn't exist or not accessible for the path /home/user/hadoop_store/hdfs/namenode."
minor change in HDFS Erasure Coding document,HDFS-11607,hdfs,,,,"The ""Redundant Array of Inexpensive Disks "" in HDFSErasureCoding.md is not appropriate. I think it  should be ""Redundant Arrays of Inexpensive Disks "".
This word means multiple groups of disks, and I have also check this term in papers searched from IEEEXplore

http://ieeexplore.ieee.org/search/searchresult.jsp?newsearch=true&queryText=Redundant%20Arrays%20of%20Inexpensive%20Disks

Since it appears in HDFS Erasure Coding page, a hdfs new feature, it should be accurately described.
"
RequestHedgingInvocationHandler can't be cast to org.apache.hadoop.ipc.RpcInvocationHandler,HDFS-9836,ha,,,,"RequestHedgingInvocationHandler cannot be cast to org.apache.hadoop.ipc.RpcInvocationHandler

Reproduce steps:
1: Set client failover provider as RequestHedgingProxyProvider.
<property>
    <name>dfs.client.failover.proxy.provider.[nameservice]</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider</value>
  </property>

2: run hdfs fsck / will get following exceptions.
Exception in thread ""main"" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler cannot be cast to org.apache.hadoop.ipc.RpcInvocationHandler
        at org.apache.hadoop.ipc.RPC.getConnectionIdForProxy(RPC.java:613)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.getConnectionId(RetryInvocationHandler.java:281)
        at org.apache.hadoop.ipc.RPC.getConnectionIdForProxy(RPC.java:615)
        at org.apache.hadoop.ipc.RPC.getServerAddress(RPC.java:598)
        at org.apache.hadoop.hdfs.HAUtil.getAddressOfActive(HAUtil.java:380)
        at org.apache.hadoop.hdfs.tools.DFSck.getCurrentNamenodeAddress(DFSck.java:248)
        at org.apache.hadoop.hdfs.tools.DFSck.doWork(DFSck.java:255)
        at org.apache.hadoop.hdfs.tools.DFSck.access$000(DFSck.java:72)
        at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:148)
        at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:145)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.hdfs.tools.DFSck.run(DFSck.java:144)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.hdfs.tools.DFSck.main(DFSck.java:360)
"
RoundRobinVolumeChoosingPolicy,HDFS-9492,namenode,,,,"This is some general clean-up for: RoundRobinVolumeChoosingPolicy

I have also updated and expanded the unit tests a bit.

There is one error message being generated that I changed.  I felt the previous Exception message was not that helpful and therefore it was possible to trim it down. If the exception message must be enhanced, the entire list of ""volumes"" should be included."
HDFS,HDFS-13216,,,,,
Fix log format in StripedBlockReconstructor,HDFS-12065,erasure-coding,,,,"The {{LOG}} is using wrong signature in {{StripedBlockReconstructor}}, and results to the following message without the stack:

{code}
Failed to reconstruct striped block: BP-1026491657-172.31.114.203-1498498077419:blk_-9223372036854759232_5065
java.lang.NullPointerException
{code}"
QJM client tests require a bit more time in some environments,HDFS-4394,namenode,test,,,"I see hard timeouts of TestQuorumJournalManager and TestQJMWithFaults locally on a dual core laptop. Frequent jstacking shows some minor delay in IPv4 address<->local hostname resolution. These tests start up new ""daemons"" frequently enough for this to be an issue.

With the patch applied, these tests pass. 

I've made a few tries at improving the behavior of the resolver: I've of course insured the 'hosts' file does not have entries which interfere, checked nsswitch.conf sanity, as well as experimented with toggling the java.net.preferIPv4 system property, finally reconfigured dnsmasq to serve local forward and reverse entries, still need the patch. Perhaps I've overlooked another option?"
chmod 777 the .snapshot directory does not error that modification on RO snapshot is disallowed,HDFS-4981,snapshots,,,,"Snapshots currently are RO, so it's expected that when someone tries to modify the .snapshot directory s/he is denied.

However, if the user tries to chmod 777 the .snapshot directory, the operation does not error. The user should be alerted that modifications are not allowed, even if this operation didn't actually change anything.

Using other modes will trigger the error, though.

{code}
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chmod 777 /user/schu/test_dir_1/.snapshot/
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chmod 755 /user/schu/test_dir_1/.snapshot/
chmod: changing permissions of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chmod 435 /user/schu/test_dir_1/.snapshot/
chmod: changing permissions of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chown hdfs /user/schu/test_dir_1/.snapshot/
chown: changing ownership of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chown schu /user/schu/test_dir_1/.snapshot/
chown: changing ownership of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ 
{code}"
A small improvement of SerialNumberManager$SerialNumberMap.get,HDFS-3805,,,,,"Instead of use containsKey() + get() to get the value from the HashMap, we can do get() once and check if the result is null"
merge BlockPlacementPolicyWithNodeGroup with default policy,HDFS-8390,,,,,"We saw requirements for adding new policies.( HDFS-7613, HDFS-7892, HDFS-8131, maybe  HDFS-4894, HDFS-7068 in the future). Every policy need to support NodeGroup.

Assume we have N policies:
{noformat}
BlockPlacementPolicyDefault
BlockPlacementPolicyCustomized1 extends BlockPlacementPolicyDefault
BlockPlacementPolicyCustomized2 extends BlockPlacementPolicyDefault
BlockPlacementPolicyCustomized3 extends BlockPlacementPolicyDefault
{noformat}
We need to implements another N policies:
{noformat}
BlockPlacementPolicyyWithNodeGroup
BlockPlacementPolicyCustomized1WithNodeGroup extends BlockPlacementPolicyWithNodeGroup
BlockPlacementPolicyCustomized2WithNodeGroup extends BlockPlacementPolicyWithNodeGroup
BlockPlacementPolicyCustomized3WithNodeGroup extends BlockPlacementPolicyWithNodeGroup
{noformat}
We had better merge nodeGroup awareness into default policy. So every new policy only need to extends BlockPlacementPolicyDefault."
"Add a main method to HdfsConfiguration, for debug purposes",HDFS-3621,hdfs-client,,,,"Just like Configuration has a main() func that dumps XML out for debug purposes, we should have a similar function under the HdfsConfiguration class that does the same. This is useful in testing out app classpath setups at times."
Add testcases for -n option of FSshell cat,HDFS-2530,test,,,,Add test cases for HADOOP-7795.
Deep learn about hadoop,HDFS-7631,,,,,I want to learn more about hadoop code. If there are any books that can help me.
Some of property descriptions are not given(hdfs-default.xml),HDFS-2892,namenode,,,,"Hi..I taken 23.0 release form http://hadoop.apache.org/common/releases.html#11+Nov%2C+2011%3A+release+0.23.0+available

I just gone through all properties provided in the hdfs-default.xml..Some of the property description not mentioned..It's better to give description of property and usage(how to configure ) and Only MapReduce related jars only provided..Please check following two configurations


 *No Description*

{noformat}
<property>
  <name>dfs.datanode.https.address</name>
  <value>0.0.0.0:50475</value>
</property>

<property>
  <name>dfs.namenode.https-address</name>
  <value>0.0.0.0:50470</value>
</property>
{noformat}


 Better to mention example usage (what to configure...format(syntax))in desc,here I did not get what default mean whether this name of n/w interface or something else

 <property>
  <name>dfs.datanode.dns.interface</name>
  <value>default</value>
  <description>The name of the Network Interface from which a data node should 
  report its IP address.
  </description>
 </property>


The following property is commented..If it is not supported better to remove.

<property>
   <name>dfs.cluster.administrators</name>
   <value>ACL for the admins</value>
   <description>This configuration is used to control who can access the
                default servlets in the namenode, etc.
   </description>
</property>




 Small clarification for following property..if some value configured this then NN will be safe mode upto this much time..
May I know usage of the following property...
<property>
  <name>dfs.blockreport.initialDelay</name>  <value>0</value>
  <description>Delay for first block report in seconds.</description>
</property>
"
[JDK8] azurenative test cases fail builds,HDFS-7297,test,,,,java.util.Base64 conflicts with com.microsoft.windowsazure.storage.core.Base64 in Azure unit tests.
Fix 1 space misalignment in FileSystem class ,HDFS-6669,,,,,"This is simple cleanup on FileSystem class to align 1 space char misalignment to help better code updates in IDE.

No code functionality change."
Missing '\n' in the output of 'hdfs oiv --help',HDFS-5864,tools,,,,"In OfflineImageViewer.java, 

{code}
    ""  * NameDistribution: This processor analyzes the file names\n"" +
    ""    in the image and prints total number of file names and how frequently"" +
    ""    file names are reused.\n"" +
{code}

should be

{code}
    ""  * NameDistribution: This processor analyzes the file names\n"" +
    ""    in the image and prints total number of file names and how frequently\n"" +
    ""    file names are reused.\n"" +
{code}"
GSetByHashMap breaks contract of GSet,HDFS-5764,,,,,"The contract of GSet says it is ensured to throw NullPointerException if a given argument is null for many methods, but GSetByHashMap doesn't. I think just writing non-null preconditions for GSet are required.
"
"fine tune ""Access token verification failed"" error msg in datanode log",HDFS-4881,datanode,,,,"I'd like to issue this ticket is due to we suffered a datanode access token verification failure issue recently. The client is HBase who is accessing the local datanode via DFSClient. The details log snippets as follows...
*regionserver log*
{code}
...
[2013-05-24 08:33:37,553][regionserver8120-compactions-1369288874174][INFO ][org.apache.hadoop.hbase.regionserver.Store]: Started compaction of 1 file(s) in cf=ho, hasReferences=true, into hdfs://sjdc-s-hdd-001.sjdc.ispn.trendmicro.com:8020/user/SPN-hbase/spn.guidcensus.ho/f99c6fb26f488034bf0e6ddd7a647ba4/.tmp, seqid=3, totalSize=4.2g
[2013-05-24 08:33:37,554][regionserver8120-compactions-1369288874174][INFO ][org.apache.hadoop.hdfs.DFSClient]: Access token was invalid when connecting to /10.31.6.49:1004 : org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error for OP_READ_BLOCK, self=/10.31.6.49:36530, remote=/10.31.6.49:1004, for file /user/SPN-hbase/spn.guidcensus.ho/a565dd142933e3abf9bec33d59210d1b/ho/c5b37b9dd8801275c8fb160c0fb32ce5c48b56f4, for block 4549293737579979499_205814042
...
{code}

*datanode log*
{code}
...
[2013-05-24 08:33:37,554][DataXceiver for client /10.31.6.49:36530 [Waiting for operation #1]][ERROR][org.apache.hadoop.hdfs.server.datanode.DataNode]: DatanodeRegistration(10.31.6.49:1004, storageID=DS-1953102179-10.31.6.49-1004-       1342490559943, infoPort=1006, ipcPort=50020):DataXceiver
java.io.IOException: Access token verification failed, for client /10.31.6.49:36530 for OP_READ_BLOCK for block blk_4549293737579979499_205814042
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:252)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:175)
...
{code}

After trace o.a.h.hdfs.security.token.block.BlockTokenSecretManager.java, I found that there are more further details error description written in code.
*o.a.h.hdfs.security.token.block.BlockTokenSecretManager.java*
{code}
public void checkAccess(BlockTokenIdentifier id, String userId, Block block,
      AccessMode mode) throws InvalidToken {
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Checking access for user="" + userId + "", block="" + block
          + "", access mode="" + mode + "" using "" + id.toString());
    }
    if (userId != null && !userId.equals(id.getUserId())) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't belong to user "" + userId);
    }
    if (id.getBlockId() != block.getBlockId()) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't apply to block "" + block);
    }
    if (isExpired(id.getExpiryDate())) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" is expired."");
    }
    if (!id.getAccessModes().contains(mode)) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't have "" + mode + "" permission"");
    }
  }
{code}

But actually, this InvalidTokenException will not be handled further (but caught), so I can not trace what kind of this access block token verification is...
*o.a.h.hdfs.server.datanode.DataXceiver.java*
{code}
...
if (datanode.isBlockTokenEnabled) {
      try {
        datanode.blockTokenSecretManager.checkAccess(accessToken, null, block,
            BlockTokenSecretManager.AccessMode.READ);
      } catch (InvalidToken e) {
        // the e object not handled further...
        try {
          out.writeShort(DataTransferProtocol.OP_STATUS_ERROR_ACCESS_TOKEN);
          out.flush();
          throw new IOException(""Access token verification failed, for client ""
              + remoteAddress + "" for OP_READ_BLOCK for block "" + block); 
        } finally {
          IOUtils.closeStream(out);
        }   
      }   
    }
...
{code}
"
Remove redundant SuppressWarning annotations in WebHdfsFileSystem,HDFS-5179,,,,,"It seems that the annotations of SuppressWarning in this file are no longer needed.

Remove them to eliminate the warnings from the compiler."
misleading comment in CommonConfigurationKeysPublic,HDFS-3994,hdfs-client,,,,"{{CommonConfigurationKeysPublic}} contains a potentially misleading comment:

{code}
/** 
 * This class contains constants for configuration keys used
 * in the common code.
 *
 * It includes all publicly documented configuration keys. In general
 * this class should not be used directly (use CommonConfigurationKeys
 * instead)
 */
{code}

This comment suggests that the user use {{CommonConfigurationKeys}}, despite the fact that that class is {{InterfaceAudience.private}} whereas {{CommonConfigurationKeysPublic}} is {{InterfaceAudience.public}}.  Perhaps this should be rephrased."
Add testcases for -n option of FSshell -tail ,HDFS-2279,test,,,,Add a few test cases for HADOOP-7546.
Replace hardcoded strings with the already defined config keys in DataNode.java ,HDFS-3855,datanode,,,,Replace hardcoded strings with the already defined config keys in DataNode.java 
DNA_SHUTDOWN command is never sent by the NN,HDFS-2987,datanode,,,,"The DataNode has a code path to handle a DNA_SHUTDOWN command, but in fact this command has never been sent by the NN (it was introduced by HADOOP-641 in 0.8.0!)"
review my networking,HDFS-3327,,,,,all my communication
Missing license headers in branch-20-append,HDFS-1266,,,,,"We appear to have some files without license headers, we should do a quick pass through and fix them."
thriftfs creates jar file named $\{version\}-thriftfs.jar,HDFS-416,,,,,"I am building thrift-fs off of 0.17.2. I accomplished this by downloading the trunk moving it into the 0.17.2.1 source and then running the build process. 
I made the append method throw illegal argument exception. 

After  the file that was produced is named 'hadoop-0.17.2.1/build/contrib/thriftfs/hadoop-${version}-thriftfs.jar'. (The version was not inserted) on my filesystems. This may not be a bug. this may be a result up the mismatched way I built 0.19 and 0.17 components together. I wanted to make sure the component compiled against the version I am running."
Fake jira for illustrating workflow (sorry),HDFS-1916,documentation,,,,"The namenode explodes when it eats too much.
Steps to reproduce: a) eat too much. b) explode"
Repeat commonly viewed statistics at the end of dfsadmin -report,HDFS-1545,tools,,,,"Safemode status, datanode's available, and capacity are the most commonly viewed statistics when running dfsadmin -report. They rightly appear first, followed by the statistics of individual datanodes. 

However with a large cluster, these commonly viewed statistics become obscured by the length of the individual datanode statistics. It is a tad ridiculous to have to have to grep or scroll to the top after a long list of nodes. For convenience, these commonly viewed statistics should be repeated at the end of the list."
